{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "L2 reg-n\n",
      "********** replication  0  **********\n",
      "msnbc\n",
      "(3689, 40) (3689, 40) (737, 40) (737, 40)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tGRU bidir(False), task: msnbc\n",
      "    hid: \t\t\t50,\n",
      "    h_hid: \t\t\t100\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t3689/0, SEQ_LEN: 40\n",
      "Logged Successfully: \n",
      "0 10000000000.0 2.8903298\n",
      "Logged Successfully: \n",
      "2018-05-17 18:59:54epoch=0; Loss Pred=2.8901; Val Loss=2.8903; Val Acc=0.0724; Loss Att={'forw': '1.0264'}; Train Acc=0.072; Test Acc=0.0707; Entropy={'forw': '7.0835'}; Entropy_Test=\n",
      "\n",
      "0 2.8903298 2.7956798\n",
      "Logged Successfully: \n",
      "2018-05-17 18:59:56epoch=1; Loss Pred=2.7979; Val Loss=2.7957; Val Acc=0.0974; Loss Att={'forw': '1.0168'}; Train Acc=0.096; Test Acc=0.0952; Entropy={'forw': '7.0285'}; Entropy_Test=\n",
      "\n",
      "0 2.7956798 2.792905\n",
      "Logged Successfully: \n",
      "2018-05-17 18:59:58epoch=2; Loss Pred=2.7936; Val Loss=2.7929; Val Acc=0.0974; Loss Att={'forw': '1.0036'}; Train Acc=0.096; Test Acc=0.0958; Entropy={'forw': '7.1068'}; Entropy_Test=\n",
      "\n",
      "0 2.792905 2.7010572\n",
      "Logged Successfully: \n",
      "2018-05-17 18:59:59epoch=3; Loss Pred=2.7035; Val Loss=2.7011; Val Acc=0.1289; Loss Att={'forw': '0.9870'}; Train Acc=0.128; Test Acc=0.1286; Entropy={'forw': '7.2275'}; Entropy_Test=\n",
      "\n",
      "0 2.7010572 2.6904266\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:01epoch=4; Loss Pred=2.6952; Val Loss=2.6904; Val Acc=0.1353; Loss Att={'forw': '0.9718'}; Train Acc=0.133; Test Acc=0.1329; Entropy={'forw': '7.3466'}; Entropy_Test=\n",
      "\n",
      "0 2.6904266 2.5900784\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:03epoch=5; Loss Pred=2.5962; Val Loss=2.5901; Val Acc=0.2216; Loss Att={'forw': '0.9350'}; Train Acc=0.213; Test Acc=0.2124; Entropy={'forw': '7.5008'}; Entropy_Test=\n",
      "\n",
      "0 2.5900784 2.5771046\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:05epoch=6; Loss Pred=2.5862; Val Loss=2.5771; Val Acc=0.2143; Loss Att={'forw': '0.9108'}; Train Acc=0.203; Test Acc=0.2024; Entropy={'forw': '7.5466'}; Entropy_Test=\n",
      "\n",
      "0 2.5771046 2.5015924\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:06epoch=7; Loss Pred=2.5147; Val Loss=2.5016; Val Acc=0.2118; Loss Att={'forw': '0.8574'}; Train Acc=0.207; Test Acc=0.2044; Entropy={'forw': '7.3593'}; Entropy_Test=\n",
      "\n",
      "0 2.5015924 2.5017567\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:08epoch=8; Loss Pred=2.5165; Val Loss=2.5018; Val Acc=0.2065; Loss Att={'forw': '0.8415'}; Train Acc=0.200; Test Acc=0.2011; Entropy={'forw': '6.9348'}; Entropy_Test=\n",
      "\n",
      "1 2.5015924 2.4641957\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:09epoch=9; Loss Pred=2.4814; Val Loss=2.4642; Val Acc=0.2146; Loss Att={'forw': '0.8259'}; Train Acc=0.210; Test Acc=0.2083; Entropy={'forw': '6.6224'}; Entropy_Test=\n",
      "\n",
      "0 2.4641957 2.465743\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:11epoch=10; Loss Pred=2.4803; Val Loss=2.4657; Val Acc=0.2151; Loss Att={'forw': '0.8139'}; Train Acc=0.210; Test Acc=0.2108; Entropy={'forw': '6.7221'}; Entropy_Test=\n",
      "\n",
      "1 2.4641957 2.4190168\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:12epoch=11; Loss Pred=2.4336; Val Loss=2.4190; Val Acc=0.2488; Loss Att={'forw': '0.8271'}; Train Acc=0.244; Test Acc=0.2424; Entropy={'forw': '6.9795'}; Entropy_Test=\n",
      "\n",
      "0 2.4190168 2.411761\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:14epoch=12; Loss Pred=2.4266; Val Loss=2.4118; Val Acc=0.2609; Loss Att={'forw': '0.8243'}; Train Acc=0.256; Test Acc=0.2534; Entropy={'forw': '7.2464'}; Entropy_Test=\n",
      "\n",
      "0 2.411761 2.3508298\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:16epoch=13; Loss Pred=2.3623; Val Loss=2.3508; Val Acc=0.2985; Loss Att={'forw': '0.8535'}; Train Acc=0.294; Test Acc=0.2877; Entropy={'forw': '7.2296'}; Entropy_Test=\n",
      "\n",
      "0 2.3508298 2.345495\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:17epoch=14; Loss Pred=2.3570; Val Loss=2.3455; Val Acc=0.3086; Loss Att={'forw': '0.8551'}; Train Acc=0.300; Test Acc=0.2949; Entropy={'forw': '7.4625'}; Entropy_Test=\n",
      "\n",
      "0 2.345495 2.2865531\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:19epoch=15; Loss Pred=2.2951; Val Loss=2.2866; Val Acc=0.3450; Loss Att={'forw': '0.8751'}; Train Acc=0.336; Test Acc=0.3317; Entropy={'forw': '7.8489'}; Entropy_Test=\n",
      "\n",
      "0 2.2865531 2.2823608\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:21epoch=16; Loss Pred=2.2925; Val Loss=2.2824; Val Acc=0.3423; Loss Att={'forw': '0.8728'}; Train Acc=0.338; Test Acc=0.3312; Entropy={'forw': '7.9362'}; Entropy_Test=\n",
      "\n",
      "0 2.2823608 2.218064\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:22epoch=17; Loss Pred=2.2257; Val Loss=2.2181; Val Acc=0.3840; Loss Att={'forw': '0.8765'}; Train Acc=0.379; Test Acc=0.3756; Entropy={'forw': '8.2117'}; Entropy_Test=\n",
      "\n",
      "0 2.218064 2.212694\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:24epoch=18; Loss Pred=2.2209; Val Loss=2.2127; Val Acc=0.3883; Loss Att={'forw': '0.8694'}; Train Acc=0.378; Test Acc=0.3755; Entropy={'forw': '8.3116'}; Entropy_Test=\n",
      "\n",
      "0 2.212694 2.172731\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:26epoch=19; Loss Pred=2.1832; Val Loss=2.1727; Val Acc=0.3953; Loss Att={'forw': '0.8583'}; Train Acc=0.384; Test Acc=0.3792; Entropy={'forw': '8.2568'}; Entropy_Test=\n",
      "\n",
      "0 2.172731 2.1871111\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:27epoch=20; Loss Pred=2.1974; Val Loss=2.1871; Val Acc=0.3680; Loss Att={'forw': '0.8437'}; Train Acc=0.362; Test Acc=0.3569; Entropy={'forw': '8.2406'}; Entropy_Test=\n",
      "\n",
      "1 2.172731 2.1607387\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:29epoch=21; Loss Pred=2.1686; Val Loss=2.1607; Val Acc=0.3785; Loss Att={'forw': '0.8396'}; Train Acc=0.374; Test Acc=0.3715; Entropy={'forw': '8.3357'}; Entropy_Test=\n",
      "\n",
      "0 2.1607387 2.1877928\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:30epoch=22; Loss Pred=2.1971; Val Loss=2.1878; Val Acc=0.3567; Loss Att={'forw': '0.8264'}; Train Acc=0.354; Test Acc=0.3512; Entropy={'forw': '8.2156'}; Entropy_Test=\n",
      "\n",
      "1 2.1607387 2.1399417\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:32epoch=23; Loss Pred=2.1468; Val Loss=2.1399; Val Acc=0.3912; Loss Att={'forw': '0.8312'}; Train Acc=0.386; Test Acc=0.3845; Entropy={'forw': '8.3536'}; Entropy_Test=\n",
      "\n",
      "0 2.1399417 2.1573248\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:33epoch=24; Loss Pred=2.1653; Val Loss=2.1573; Val Acc=0.3846; Loss Att={'forw': '0.8238'}; Train Acc=0.380; Test Acc=0.3769; Entropy={'forw': '8.2708'}; Entropy_Test=\n",
      "\n",
      "1 2.1399417 2.050583\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:35epoch=25; Loss Pred=2.0613; Val Loss=2.0506; Val Acc=0.4349; Loss Att={'forw': '0.8348'}; Train Acc=0.425; Test Acc=0.4242; Entropy={'forw': '8.3558'}; Entropy_Test=\n",
      "\n",
      "0 2.050583 2.0516973\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:36epoch=26; Loss Pred=2.0612; Val Loss=2.0517; Val Acc=0.4344; Loss Att={'forw': '0.8293'}; Train Acc=0.422; Test Acc=0.4189; Entropy={'forw': '8.3705'}; Entropy_Test=\n",
      "\n",
      "1 2.050583 1.9895747\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:38epoch=27; Loss Pred=2.0028; Val Loss=1.9896; Val Acc=0.4467; Loss Att={'forw': '0.8343'}; Train Acc=0.435; Test Acc=0.4270; Entropy={'forw': '8.1557'}; Entropy_Test=\n",
      "\n",
      "0 1.9895747 1.9908593\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:39epoch=28; Loss Pred=2.0034; Val Loss=1.9909; Val Acc=0.4438; Loss Att={'forw': '0.8280'}; Train Acc=0.430; Test Acc=0.4257; Entropy={'forw': '8.1426'}; Entropy_Test=\n",
      "\n",
      "1 1.9895747 1.9634762\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:41epoch=29; Loss Pred=1.9781; Val Loss=1.9635; Val Acc=0.4416; Loss Att={'forw': '0.8248'}; Train Acc=0.429; Test Acc=0.4256; Entropy={'forw': '7.9059'}; Entropy_Test=\n",
      "\n",
      "0 1.9634762 1.9688189\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:42epoch=30; Loss Pred=1.9842; Val Loss=1.9688; Val Acc=0.4377; Loss Att={'forw': '0.8182'}; Train Acc=0.422; Test Acc=0.4164; Entropy={'forw': '7.8964'}; Entropy_Test=\n",
      "\n",
      "1 1.9634762 1.9424378\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:44epoch=31; Loss Pred=1.9564; Val Loss=1.9424; Val Acc=0.4514; Loss Att={'forw': '0.8131'}; Train Acc=0.443; Test Acc=0.4362; Entropy={'forw': '7.8230'}; Entropy_Test=\n",
      "\n",
      "0 1.9424378 1.9489902\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:45epoch=32; Loss Pred=1.9637; Val Loss=1.9490; Val Acc=0.4526; Loss Att={'forw': '0.8046'}; Train Acc=0.440; Test Acc=0.4329; Entropy={'forw': '7.8846'}; Entropy_Test=\n",
      "\n",
      "1 1.9424378 1.9080912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Successfully: \n",
      "2018-05-17 19:00:47epoch=33; Loss Pred=1.9218; Val Loss=1.9081; Val Acc=0.4875; Loss Att={'forw': '0.7977'}; Train Acc=0.480; Test Acc=0.4743; Entropy={'forw': '7.9465'}; Entropy_Test=\n",
      "\n",
      "0 1.9080912 1.9108592\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:49epoch=34; Loss Pred=1.9272; Val Loss=1.9109; Val Acc=0.4925; Loss Att={'forw': '0.7889'}; Train Acc=0.484; Test Acc=0.4755; Entropy={'forw': '7.9289'}; Entropy_Test=\n",
      "\n",
      "1 1.9080912 1.8530815\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:51epoch=35; Loss Pred=1.8686; Val Loss=1.8531; Val Acc=0.5290; Loss Att={'forw': '0.7830'}; Train Acc=0.526; Test Acc=0.5180; Entropy={'forw': '8.1270'}; Entropy_Test=\n",
      "\n",
      "0 1.8530815 1.8574303\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:52epoch=36; Loss Pred=1.8696; Val Loss=1.8574; Val Acc=0.5345; Loss Att={'forw': '0.7737'}; Train Acc=0.532; Test Acc=0.5239; Entropy={'forw': '8.1343'}; Entropy_Test=\n",
      "\n",
      "1 1.8530815 1.7968547\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:54epoch=37; Loss Pred=1.8075; Val Loss=1.7969; Val Acc=0.5648; Loss Att={'forw': '0.7679'}; Train Acc=0.561; Test Acc=0.5535; Entropy={'forw': '8.2888'}; Entropy_Test=\n",
      "\n",
      "0 1.7968547 1.7988997\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:56epoch=38; Loss Pred=1.8085; Val Loss=1.7989; Val Acc=0.5646; Loss Att={'forw': '0.7592'}; Train Acc=0.562; Test Acc=0.5566; Entropy={'forw': '8.3375'}; Entropy_Test=\n",
      "\n",
      "1 1.7968547 1.7565655\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:57epoch=39; Loss Pred=1.7654; Val Loss=1.7566; Val Acc=0.5713; Loss Att={'forw': '0.7523'}; Train Acc=0.571; Test Acc=0.5627; Entropy={'forw': '8.4185'}; Entropy_Test=\n",
      "\n",
      "0 1.7565655 1.7656089\n",
      "Logged Successfully: \n",
      "2018-05-17 19:00:59epoch=40; Loss Pred=1.7725; Val Loss=1.7656; Val Acc=0.5697; Loss Att={'forw': '0.7425'}; Train Acc=0.569; Test Acc=0.5638; Entropy={'forw': '8.4665'}; Entropy_Test=\n",
      "\n",
      "1 1.7565655 1.7413535\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:00epoch=41; Loss Pred=1.7464; Val Loss=1.7414; Val Acc=0.5674; Loss Att={'forw': '0.7322'}; Train Acc=0.566; Test Acc=0.5597; Entropy={'forw': '8.5380'}; Entropy_Test=\n",
      "\n",
      "0 1.7413535 1.756567\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:02epoch=42; Loss Pred=1.7609; Val Loss=1.7566; Val Acc=0.5620; Loss Att={'forw': '0.7225'}; Train Acc=0.560; Test Acc=0.5513; Entropy={'forw': '8.5887'}; Entropy_Test=\n",
      "\n",
      "1 1.7413535 1.6999061\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:03epoch=43; Loss Pred=1.7046; Val Loss=1.6999; Val Acc=0.5728; Loss Att={'forw': '0.7146'}; Train Acc=0.569; Test Acc=0.5608; Entropy={'forw': '8.5867'}; Entropy_Test=\n",
      "\n",
      "0 1.6999061 1.7070314\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:05epoch=44; Loss Pred=1.7145; Val Loss=1.7070; Val Acc=0.5704; Loss Att={'forw': '0.7062'}; Train Acc=0.566; Test Acc=0.5569; Entropy={'forw': '8.5556'}; Entropy_Test=\n",
      "\n",
      "1 1.6999061 1.6376715\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:06epoch=45; Loss Pred=1.6420; Val Loss=1.6377; Val Acc=0.5844; Loss Att={'forw': '0.6987'}; Train Acc=0.581; Test Acc=0.5752; Entropy={'forw': '8.4936'}; Entropy_Test=\n",
      "\n",
      "0 1.6376715 1.6362482\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:08epoch=46; Loss Pred=1.6414; Val Loss=1.6362; Val Acc=0.5858; Loss Att={'forw': '0.6906'}; Train Acc=0.582; Test Acc=0.5726; Entropy={'forw': '8.5386'}; Entropy_Test=\n",
      "\n",
      "0 1.6362482 1.5866166\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:10epoch=47; Loss Pred=1.5889; Val Loss=1.5866; Val Acc=0.5889; Loss Att={'forw': '0.6816'}; Train Acc=0.588; Test Acc=0.5811; Entropy={'forw': '8.4355'}; Entropy_Test=\n",
      "\n",
      "0 1.5866166 1.582893\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:11epoch=48; Loss Pred=1.5882; Val Loss=1.5829; Val Acc=0.5910; Loss Att={'forw': '0.6730'}; Train Acc=0.589; Test Acc=0.5817; Entropy={'forw': '8.4629'}; Entropy_Test=\n",
      "\n",
      "0 1.582893 1.551532\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:13epoch=49; Loss Pred=1.5561; Val Loss=1.5515; Val Acc=0.5955; Loss Att={'forw': '0.6631'}; Train Acc=0.593; Test Acc=0.5889; Entropy={'forw': '8.4566'}; Entropy_Test=\n",
      "\n",
      "0 1.551532 1.5544485\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:14epoch=50; Loss Pred=1.5574; Val Loss=1.5544; Val Acc=0.5968; Loss Att={'forw': '0.6534'}; Train Acc=0.594; Test Acc=0.5902; Entropy={'forw': '8.4527'}; Entropy_Test=\n",
      "\n",
      "1 1.551532 1.5286151\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:16epoch=51; Loss Pred=1.5342; Val Loss=1.5286; Val Acc=0.6002; Loss Att={'forw': '0.6427'}; Train Acc=0.599; Test Acc=0.5945; Entropy={'forw': '8.4887'}; Entropy_Test=\n",
      "\n",
      "0 1.5286151 1.5324502\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:18epoch=52; Loss Pred=1.5373; Val Loss=1.5325; Val Acc=0.6013; Loss Att={'forw': '0.6328'}; Train Acc=0.598; Test Acc=0.5931; Entropy={'forw': '8.4961'}; Entropy_Test=\n",
      "\n",
      "1 1.5286151 1.5132443\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:20epoch=53; Loss Pred=1.5196; Val Loss=1.5132; Val Acc=0.6049; Loss Att={'forw': '0.6227'}; Train Acc=0.602; Test Acc=0.5986; Entropy={'forw': '8.5627'}; Entropy_Test=\n",
      "\n",
      "0 1.5132443 1.5183879\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:21epoch=54; Loss Pred=1.5240; Val Loss=1.5184; Val Acc=0.6049; Loss Att={'forw': '0.6119'}; Train Acc=0.601; Test Acc=0.6007; Entropy={'forw': '8.5932'}; Entropy_Test=\n",
      "\n",
      "1 1.5132443 1.4983262\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:23epoch=55; Loss Pred=1.5062; Val Loss=1.4983; Val Acc=0.6094; Loss Att={'forw': '0.6019'}; Train Acc=0.607; Test Acc=0.6018; Entropy={'forw': '8.6520'}; Entropy_Test=\n",
      "\n",
      "0 1.4983262 1.5004436\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:24epoch=56; Loss Pred=1.5100; Val Loss=1.5004; Val Acc=0.6093; Loss Att={'forw': '0.5933'}; Train Acc=0.606; Test Acc=0.6021; Entropy={'forw': '8.6555'}; Entropy_Test=\n",
      "\n",
      "1 1.4983262 1.4844328\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:26epoch=57; Loss Pred=1.4899; Val Loss=1.4844; Val Acc=0.6152; Loss Att={'forw': '0.5836'}; Train Acc=0.611; Test Acc=0.6056; Entropy={'forw': '8.6664'}; Entropy_Test=\n",
      "\n",
      "0 1.4844328 1.488679\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:28epoch=58; Loss Pred=1.4923; Val Loss=1.4887; Val Acc=0.6126; Loss Att={'forw': '0.5742'}; Train Acc=0.611; Test Acc=0.6054; Entropy={'forw': '8.6408'}; Entropy_Test=\n",
      "\n",
      "1 1.4844328 1.4636066\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:29epoch=59; Loss Pred=1.4691; Val Loss=1.4636; Val Acc=0.6207; Loss Att={'forw': '0.5640'}; Train Acc=0.616; Test Acc=0.6109; Entropy={'forw': '8.5727'}; Entropy_Test=\n",
      "\n",
      "0 1.4636066 1.4644343\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:31epoch=60; Loss Pred=1.4709; Val Loss=1.4644; Val Acc=0.6156; Loss Att={'forw': '0.5546'}; Train Acc=0.616; Test Acc=0.6108; Entropy={'forw': '8.6280'}; Entropy_Test=\n",
      "\n",
      "1 1.4636066 1.4440998\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:32epoch=61; Loss Pred=1.4487; Val Loss=1.4441; Val Acc=0.6231; Loss Att={'forw': '0.5448'}; Train Acc=0.621; Test Acc=0.6183; Entropy={'forw': '8.5397'}; Entropy_Test=\n",
      "\n",
      "0 1.4440998 1.4441255\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:34epoch=62; Loss Pred=1.4490; Val Loss=1.4441; Val Acc=0.6272; Loss Att={'forw': '0.5344'}; Train Acc=0.621; Test Acc=0.6182; Entropy={'forw': '8.4924'}; Entropy_Test=\n",
      "\n",
      "1 1.4440998 1.4248728\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:35epoch=63; Loss Pred=1.4331; Val Loss=1.4249; Val Acc=0.6290; Loss Att={'forw': '0.5257'}; Train Acc=0.627; Test Acc=0.6243; Entropy={'forw': '8.4845'}; Entropy_Test=\n",
      "\n",
      "0 1.4248728 1.4250386\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:37epoch=64; Loss Pred=1.4332; Val Loss=1.4250; Val Acc=0.6297; Loss Att={'forw': '0.5158'}; Train Acc=0.626; Test Acc=0.6231; Entropy={'forw': '8.5028'}; Entropy_Test=\n",
      "\n",
      "1 1.4248728 1.4165549\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:39epoch=65; Loss Pred=1.4194; Val Loss=1.4166; Val Acc=0.6358; Loss Att={'forw': '0.5067'}; Train Acc=0.630; Test Acc=0.6278; Entropy={'forw': '8.4507'}; Entropy_Test=\n",
      "\n",
      "0 1.4165549 1.4153078\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:40epoch=66; Loss Pred=1.4209; Val Loss=1.4153; Val Acc=0.6342; Loss Att={'forw': '0.4969'}; Train Acc=0.630; Test Acc=0.6262; Entropy={'forw': '8.4466'}; Entropy_Test=\n",
      "\n",
      "0 1.4153078 1.4059638\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:42epoch=67; Loss Pred=1.4087; Val Loss=1.4060; Val Acc=0.6377; Loss Att={'forw': '0.4884'}; Train Acc=0.634; Test Acc=0.6315; Entropy={'forw': '8.3939'}; Entropy_Test=\n",
      "\n",
      "0 1.4059638 1.4031008\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:43epoch=68; Loss Pred=1.4093; Val Loss=1.4031; Val Acc=0.6369; Loss Att={'forw': '0.4791'}; Train Acc=0.634; Test Acc=0.6274; Entropy={'forw': '8.3685'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.4031008 1.3969809\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:45epoch=69; Loss Pred=1.3987; Val Loss=1.3970; Val Acc=0.6407; Loss Att={'forw': '0.4698'}; Train Acc=0.637; Test Acc=0.6350; Entropy={'forw': '8.3148'}; Entropy_Test=\n",
      "\n",
      "0 1.3969809 1.398012\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:47epoch=70; Loss Pred=1.4005; Val Loss=1.3980; Val Acc=0.6399; Loss Att={'forw': '0.4614'}; Train Acc=0.638; Test Acc=0.6337; Entropy={'forw': '8.2819'}; Entropy_Test=\n",
      "\n",
      "1 1.3969809 1.3826414\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:48epoch=71; Loss Pred=1.3906; Val Loss=1.3826; Val Acc=0.6418; Loss Att={'forw': '0.4525'}; Train Acc=0.640; Test Acc=0.6355; Entropy={'forw': '8.2980'}; Entropy_Test=\n",
      "\n",
      "0 1.3826414 1.3904243\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:50epoch=72; Loss Pred=1.3929; Val Loss=1.3904; Val Acc=0.6424; Loss Att={'forw': '0.4444'}; Train Acc=0.640; Test Acc=0.6359; Entropy={'forw': '8.2929'}; Entropy_Test=\n",
      "\n",
      "1 1.3826414 1.3803707\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:51epoch=73; Loss Pred=1.3844; Val Loss=1.3804; Val Acc=0.6441; Loss Att={'forw': '0.4356'}; Train Acc=0.642; Test Acc=0.6377; Entropy={'forw': '8.2378'}; Entropy_Test=\n",
      "\n",
      "0 1.3803707 1.380311\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:53epoch=74; Loss Pred=1.3867; Val Loss=1.3803; Val Acc=0.6430; Loss Att={'forw': '0.4274'}; Train Acc=0.642; Test Acc=0.6356; Entropy={'forw': '8.2507'}; Entropy_Test=\n",
      "\n",
      "0 1.380311 1.3729056\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:55epoch=75; Loss Pred=1.3773; Val Loss=1.3729; Val Acc=0.6446; Loss Att={'forw': '0.4192'}; Train Acc=0.644; Test Acc=0.6387; Entropy={'forw': '8.1413'}; Entropy_Test=\n",
      "\n",
      "0 1.3729056 1.3755302\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:56epoch=76; Loss Pred=1.3805; Val Loss=1.3755; Val Acc=0.6436; Loss Att={'forw': '0.4108'}; Train Acc=0.644; Test Acc=0.6371; Entropy={'forw': '8.1503'}; Entropy_Test=\n",
      "\n",
      "1 1.3729056 1.3677737\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:58epoch=77; Loss Pred=1.3708; Val Loss=1.3678; Val Acc=0.6462; Loss Att={'forw': '0.4030'}; Train Acc=0.646; Test Acc=0.6429; Entropy={'forw': '8.0901'}; Entropy_Test=\n",
      "\n",
      "0 1.3677737 1.3685675\n",
      "Logged Successfully: \n",
      "2018-05-17 19:01:59epoch=78; Loss Pred=1.3737; Val Loss=1.3686; Val Acc=0.6466; Loss Att={'forw': '0.3950'}; Train Acc=0.647; Test Acc=0.6434; Entropy={'forw': '8.0574'}; Entropy_Test=\n",
      "\n",
      "1 1.3677737 1.3623123\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:01epoch=79; Loss Pred=1.3647; Val Loss=1.3623; Val Acc=0.6476; Loss Att={'forw': '0.3874'}; Train Acc=0.648; Test Acc=0.6445; Entropy={'forw': '8.0796'}; Entropy_Test=\n",
      "\n",
      "0 1.3623123 1.3602985\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:03epoch=80; Loss Pred=1.3669; Val Loss=1.3603; Val Acc=0.6468; Loss Att={'forw': '0.3801'}; Train Acc=0.648; Test Acc=0.6448; Entropy={'forw': '8.0773'}; Entropy_Test=\n",
      "\n",
      "0 1.3602985 1.3563187\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:04epoch=81; Loss Pred=1.3585; Val Loss=1.3563; Val Acc=0.6492; Loss Att={'forw': '0.3725'}; Train Acc=0.649; Test Acc=0.6449; Entropy={'forw': '7.9913'}; Entropy_Test=\n",
      "\n",
      "0 1.3563187 1.359471\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:06epoch=82; Loss Pred=1.3609; Val Loss=1.3595; Val Acc=0.6496; Loss Att={'forw': '0.3654'}; Train Acc=0.649; Test Acc=0.6451; Entropy={'forw': '7.9129'}; Entropy_Test=\n",
      "\n",
      "1 1.3563187 1.3477603\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:08epoch=83; Loss Pred=1.3524; Val Loss=1.3478; Val Acc=0.6508; Loss Att={'forw': '0.3578'}; Train Acc=0.650; Test Acc=0.6471; Entropy={'forw': '7.8924'}; Entropy_Test=\n",
      "\n",
      "0 1.3477603 1.3510731\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:09epoch=84; Loss Pred=1.3536; Val Loss=1.3511; Val Acc=0.6514; Loss Att={'forw': '0.3513'}; Train Acc=0.651; Test Acc=0.6474; Entropy={'forw': '7.8718'}; Entropy_Test=\n",
      "\n",
      "1 1.3477603 1.3461934\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:11epoch=85; Loss Pred=1.3476; Val Loss=1.3462; Val Acc=0.6503; Loss Att={'forw': '0.3450'}; Train Acc=0.651; Test Acc=0.6451; Entropy={'forw': '7.8912'}; Entropy_Test=\n",
      "\n",
      "0 1.3461934 1.3461667\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:12epoch=86; Loss Pred=1.3478; Val Loss=1.3462; Val Acc=0.6514; Loss Att={'forw': '0.3388'}; Train Acc=0.651; Test Acc=0.6483; Entropy={'forw': '7.9088'}; Entropy_Test=\n",
      "\n",
      "0 1.3461667 1.3399687\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:14epoch=87; Loss Pred=1.3418; Val Loss=1.3400; Val Acc=0.6505; Loss Att={'forw': '0.3319'}; Train Acc=0.652; Test Acc=0.6461; Entropy={'forw': '7.8896'}; Entropy_Test=\n",
      "\n",
      "0 1.3399687 1.3384997\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:16epoch=88; Loss Pred=1.3427; Val Loss=1.3385; Val Acc=0.6515; Loss Att={'forw': '0.3265'}; Train Acc=0.651; Test Acc=0.6466; Entropy={'forw': '7.9437'}; Entropy_Test=\n",
      "\n",
      "0 1.3384997 1.3363384\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:17epoch=89; Loss Pred=1.3369; Val Loss=1.3363; Val Acc=0.6513; Loss Att={'forw': '0.3205'}; Train Acc=0.652; Test Acc=0.6486; Entropy={'forw': '7.9004'}; Entropy_Test=\n",
      "\n",
      "0 1.3363384 1.3387597\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:19epoch=90; Loss Pred=1.3386; Val Loss=1.3388; Val Acc=0.6528; Loss Att={'forw': '0.3144'}; Train Acc=0.652; Test Acc=0.6491; Entropy={'forw': '7.8385'}; Entropy_Test=\n",
      "\n",
      "1 1.3363384 1.3322861\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:20epoch=91; Loss Pred=1.3328; Val Loss=1.3323; Val Acc=0.6518; Loss Att={'forw': '0.3106'}; Train Acc=0.652; Test Acc=0.6491; Entropy={'forw': '7.8051'}; Entropy_Test=\n",
      "\n",
      "0 1.3322861 1.3331236\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:22epoch=92; Loss Pred=1.3343; Val Loss=1.3331; Val Acc=0.6529; Loss Att={'forw': '0.3042'}; Train Acc=0.653; Test Acc=0.6473; Entropy={'forw': '7.8487'}; Entropy_Test=\n",
      "\n",
      "1 1.3322861 1.3265388\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:24epoch=93; Loss Pred=1.3272; Val Loss=1.3265; Val Acc=0.6531; Loss Att={'forw': '0.2993'}; Train Acc=0.653; Test Acc=0.6493; Entropy={'forw': '7.8576'}; Entropy_Test=\n",
      "\n",
      "0 1.3265388 1.3259044\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:25epoch=94; Loss Pred=1.3308; Val Loss=1.3259; Val Acc=0.6522; Loss Att={'forw': '0.2945'}; Train Acc=0.653; Test Acc=0.6506; Entropy={'forw': '7.8563'}; Entropy_Test=\n",
      "\n",
      "0 1.3259044 1.3234688\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:27epoch=95; Loss Pred=1.3243; Val Loss=1.3235; Val Acc=0.6549; Loss Att={'forw': '0.2903'}; Train Acc=0.654; Test Acc=0.6478; Entropy={'forw': '7.7928'}; Entropy_Test=\n",
      "\n",
      "0 1.3234688 1.3237748\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:28epoch=96; Loss Pred=1.3246; Val Loss=1.3238; Val Acc=0.6535; Loss Att={'forw': '0.2859'}; Train Acc=0.654; Test Acc=0.6487; Entropy={'forw': '7.7744'}; Entropy_Test=\n",
      "\n",
      "1 1.3234688 1.3129781\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:30epoch=97; Loss Pred=1.3184; Val Loss=1.3130; Val Acc=0.6546; Loss Att={'forw': '0.2815'}; Train Acc=0.654; Test Acc=0.6519; Entropy={'forw': '7.7904'}; Entropy_Test=\n",
      "\n",
      "0 1.3129781 1.3193315\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:31epoch=98; Loss Pred=1.3196; Val Loss=1.3193; Val Acc=0.6530; Loss Att={'forw': '0.2780'}; Train Acc=0.654; Test Acc=0.6509; Entropy={'forw': '7.7427'}; Entropy_Test=\n",
      "\n",
      "1 1.3129781 1.3118684\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:33epoch=99; Loss Pred=1.3124; Val Loss=1.3119; Val Acc=0.6542; Loss Att={'forw': '0.2741'}; Train Acc=0.655; Test Acc=0.6495; Entropy={'forw': '7.7336'}; Entropy_Test=\n",
      "\n",
      "0 1.3118684 1.3143595\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:34epoch=100; Loss Pred=1.3128; Val Loss=1.3144; Val Acc=0.6539; Loss Att={'forw': '0.2699'}; Train Acc=0.654; Test Acc=0.6509; Entropy={'forw': '7.7775'}; Entropy_Test=\n",
      "\n",
      "1 1.3118684 1.3051354\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:36epoch=101; Loss Pred=1.3081; Val Loss=1.3051; Val Acc=0.6555; Loss Att={'forw': '0.2670'}; Train Acc=0.656; Test Acc=0.6506; Entropy={'forw': '7.7770'}; Entropy_Test=\n",
      "\n",
      "0 1.3051354 1.3102857\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:38epoch=102; Loss Pred=1.3087; Val Loss=1.3103; Val Acc=0.6554; Loss Att={'forw': '0.2635'}; Train Acc=0.655; Test Acc=0.6509; Entropy={'forw': '7.7877'}; Entropy_Test=\n",
      "\n",
      "1 1.3051354 1.3031877\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:39epoch=103; Loss Pred=1.3038; Val Loss=1.3032; Val Acc=0.6562; Loss Att={'forw': '0.2606'}; Train Acc=0.656; Test Acc=0.6524; Entropy={'forw': '7.7448'}; Entropy_Test=\n",
      "\n",
      "0 1.3031877 1.3023099\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:41epoch=104; Loss Pred=1.3034; Val Loss=1.3023; Val Acc=0.6549; Loss Att={'forw': '0.2571'}; Train Acc=0.656; Test Acc=0.6503; Entropy={'forw': '7.7703'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3023099 1.2978954\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:42epoch=105; Loss Pred=1.3006; Val Loss=1.2979; Val Acc=0.6567; Loss Att={'forw': '0.2552'}; Train Acc=0.657; Test Acc=0.6545; Entropy={'forw': '7.8425'}; Entropy_Test=\n",
      "\n",
      "0 1.2978954 1.3003764\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:44epoch=106; Loss Pred=1.3011; Val Loss=1.3004; Val Acc=0.6554; Loss Att={'forw': '0.2519'}; Train Acc=0.657; Test Acc=0.6515; Entropy={'forw': '7.8256'}; Entropy_Test=\n",
      "\n",
      "1 1.2978954 1.2971768\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:46epoch=107; Loss Pred=1.2971; Val Loss=1.2972; Val Acc=0.6569; Loss Att={'forw': '0.2493'}; Train Acc=0.658; Test Acc=0.6491; Entropy={'forw': '7.7605'}; Entropy_Test=\n",
      "\n",
      "0 1.2971768 1.2975742\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:47epoch=108; Loss Pred=1.2960; Val Loss=1.2976; Val Acc=0.6569; Loss Att={'forw': '0.2467'}; Train Acc=0.656; Test Acc=0.6504; Entropy={'forw': '7.7305'}; Entropy_Test=\n",
      "\n",
      "1 1.2971768 1.2951092\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:49epoch=109; Loss Pred=1.2948; Val Loss=1.2951; Val Acc=0.6580; Loss Att={'forw': '0.2449'}; Train Acc=0.657; Test Acc=0.6546; Entropy={'forw': '7.7723'}; Entropy_Test=\n",
      "\n",
      "0 1.2951092 1.2945868\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:50epoch=110; Loss Pred=1.2934; Val Loss=1.2946; Val Acc=0.6559; Loss Att={'forw': '0.2420'}; Train Acc=0.657; Test Acc=0.6504; Entropy={'forw': '7.7247'}; Entropy_Test=\n",
      "\n",
      "0 1.2945868 1.2889447\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:52epoch=111; Loss Pred=1.2900; Val Loss=1.2889; Val Acc=0.6573; Loss Att={'forw': '0.2409'}; Train Acc=0.657; Test Acc=0.6518; Entropy={'forw': '7.7709'}; Entropy_Test=\n",
      "\n",
      "0 1.2889447 1.2903845\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:53epoch=112; Loss Pred=1.2916; Val Loss=1.2904; Val Acc=0.6572; Loss Att={'forw': '0.2383'}; Train Acc=0.657; Test Acc=0.6544; Entropy={'forw': '7.7763'}; Entropy_Test=\n",
      "\n",
      "1 1.2889447 1.2888476\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:55epoch=113; Loss Pred=1.2876; Val Loss=1.2888; Val Acc=0.6578; Loss Att={'forw': '0.2372'}; Train Acc=0.657; Test Acc=0.6547; Entropy={'forw': '7.7930'}; Entropy_Test=\n",
      "\n",
      "0 1.2888476 1.2922598\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:57epoch=114; Loss Pred=1.2872; Val Loss=1.2923; Val Acc=0.6569; Loss Att={'forw': '0.2349'}; Train Acc=0.657; Test Acc=0.6546; Entropy={'forw': '7.7820'}; Entropy_Test=\n",
      "\n",
      "1 1.2888476 1.2847773\n",
      "Logged Successfully: \n",
      "2018-05-17 19:02:58epoch=115; Loss Pred=1.2860; Val Loss=1.2848; Val Acc=0.6575; Loss Att={'forw': '0.2334'}; Train Acc=0.658; Test Acc=0.6547; Entropy={'forw': '7.7882'}; Entropy_Test=\n",
      "\n",
      "0 1.2847773 1.285528\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:00epoch=116; Loss Pred=1.2847; Val Loss=1.2855; Val Acc=0.6591; Loss Att={'forw': '0.2320'}; Train Acc=0.658; Test Acc=0.6556; Entropy={'forw': '7.7715'}; Entropy_Test=\n",
      "\n",
      "1 1.2847773 1.2814207\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:01epoch=117; Loss Pred=1.2809; Val Loss=1.2814; Val Acc=0.6582; Loss Att={'forw': '0.2303'}; Train Acc=0.658; Test Acc=0.6564; Entropy={'forw': '7.7635'}; Entropy_Test=\n",
      "\n",
      "0 1.2814207 1.2832394\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:03epoch=118; Loss Pred=1.2820; Val Loss=1.2832; Val Acc=0.6567; Loss Att={'forw': '0.2287'}; Train Acc=0.659; Test Acc=0.6545; Entropy={'forw': '7.7871'}; Entropy_Test=\n",
      "\n",
      "1 1.2814207 1.283486\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:04epoch=119; Loss Pred=1.2798; Val Loss=1.2835; Val Acc=0.6583; Loss Att={'forw': '0.2275'}; Train Acc=0.659; Test Acc=0.6548; Entropy={'forw': '7.7935'}; Entropy_Test=\n",
      "\n",
      "2 1.2814207 1.2826005\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:06epoch=120; Loss Pred=1.2793; Val Loss=1.2826; Val Acc=0.6572; Loss Att={'forw': '0.2268'}; Train Acc=0.660; Test Acc=0.6549; Entropy={'forw': '7.7758'}; Entropy_Test=\n",
      "\n",
      "3 1.2814207 1.2762733\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:07epoch=121; Loss Pred=1.2759; Val Loss=1.2763; Val Acc=0.6588; Loss Att={'forw': '0.2254'}; Train Acc=0.659; Test Acc=0.6555; Entropy={'forw': '7.7967'}; Entropy_Test=\n",
      "\n",
      "0 1.2762733 1.2788876\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:09epoch=122; Loss Pred=1.2772; Val Loss=1.2789; Val Acc=0.6568; Loss Att={'forw': '0.2242'}; Train Acc=0.660; Test Acc=0.6561; Entropy={'forw': '7.8024'}; Entropy_Test=\n",
      "\n",
      "1 1.2762733 1.2761531\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:10epoch=123; Loss Pred=1.2754; Val Loss=1.2762; Val Acc=0.6597; Loss Att={'forw': '0.2232'}; Train Acc=0.660; Test Acc=0.6561; Entropy={'forw': '7.8032'}; Entropy_Test=\n",
      "\n",
      "0 1.2761531 1.2779357\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:12epoch=124; Loss Pred=1.2742; Val Loss=1.2779; Val Acc=0.6580; Loss Att={'forw': '0.2223'}; Train Acc=0.661; Test Acc=0.6558; Entropy={'forw': '7.8037'}; Entropy_Test=\n",
      "\n",
      "1 1.2761531 1.2739766\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:14epoch=125; Loss Pred=1.2738; Val Loss=1.2740; Val Acc=0.6594; Loss Att={'forw': '0.2218'}; Train Acc=0.660; Test Acc=0.6555; Entropy={'forw': '7.8359'}; Entropy_Test=\n",
      "\n",
      "0 1.2739766 1.2755505\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:15epoch=126; Loss Pred=1.2723; Val Loss=1.2756; Val Acc=0.6569; Loss Att={'forw': '0.2205'}; Train Acc=0.660; Test Acc=0.6573; Entropy={'forw': '7.8489'}; Entropy_Test=\n",
      "\n",
      "1 1.2739766 1.2750229\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:17epoch=127; Loss Pred=1.2715; Val Loss=1.2750; Val Acc=0.6598; Loss Att={'forw': '0.2202'}; Train Acc=0.661; Test Acc=0.6573; Entropy={'forw': '7.8255'}; Entropy_Test=\n",
      "\n",
      "2 1.2739766 1.2745003\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:18epoch=128; Loss Pred=1.2708; Val Loss=1.2745; Val Acc=0.6586; Loss Att={'forw': '0.2193'}; Train Acc=0.660; Test Acc=0.6569; Entropy={'forw': '7.7781'}; Entropy_Test=\n",
      "\n",
      "3 1.2739766 1.2704583\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:20epoch=129; Loss Pred=1.2678; Val Loss=1.2705; Val Acc=0.6599; Loss Att={'forw': '0.2185'}; Train Acc=0.660; Test Acc=0.6586; Entropy={'forw': '7.7982'}; Entropy_Test=\n",
      "\n",
      "0 1.2704583 1.2712389\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:22epoch=130; Loss Pred=1.2676; Val Loss=1.2712; Val Acc=0.6603; Loss Att={'forw': '0.2178'}; Train Acc=0.661; Test Acc=0.6543; Entropy={'forw': '7.8279'}; Entropy_Test=\n",
      "\n",
      "1 1.2704583 1.2691511\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:23epoch=131; Loss Pred=1.2676; Val Loss=1.2692; Val Acc=0.6606; Loss Att={'forw': '0.2176'}; Train Acc=0.661; Test Acc=0.6586; Entropy={'forw': '7.7562'}; Entropy_Test=\n",
      "\n",
      "0 1.2691511 1.2678462\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:25epoch=132; Loss Pred=1.2671; Val Loss=1.2678; Val Acc=0.6607; Loss Att={'forw': '0.2162'}; Train Acc=0.662; Test Acc=0.6584; Entropy={'forw': '7.7671'}; Entropy_Test=\n",
      "\n",
      "0 1.2678462 1.2676128\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:27epoch=133; Loss Pred=1.2642; Val Loss=1.2676; Val Acc=0.6587; Loss Att={'forw': '0.2163'}; Train Acc=0.662; Test Acc=0.6574; Entropy={'forw': '7.8356'}; Entropy_Test=\n",
      "\n",
      "0 1.2676128 1.2658132\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:28epoch=134; Loss Pred=1.2644; Val Loss=1.2658; Val Acc=0.6619; Loss Att={'forw': '0.2154'}; Train Acc=0.662; Test Acc=0.6598; Entropy={'forw': '7.8001'}; Entropy_Test=\n",
      "\n",
      "0 1.2658132 1.2650948\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:30epoch=135; Loss Pred=1.2634; Val Loss=1.2651; Val Acc=0.6618; Loss Att={'forw': '0.2150'}; Train Acc=0.663; Test Acc=0.6599; Entropy={'forw': '7.7154'}; Entropy_Test=\n",
      "\n",
      "0 1.2650948 1.2646337\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:32epoch=136; Loss Pred=1.2635; Val Loss=1.2646; Val Acc=0.6623; Loss Att={'forw': '0.2138'}; Train Acc=0.664; Test Acc=0.6595; Entropy={'forw': '7.7173'}; Entropy_Test=\n",
      "\n",
      "0 1.2646337 1.2672712\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:33epoch=137; Loss Pred=1.2619; Val Loss=1.2673; Val Acc=0.6625; Loss Att={'forw': '0.2137'}; Train Acc=0.664; Test Acc=0.6589; Entropy={'forw': '7.7057'}; Entropy_Test=\n",
      "\n",
      "1 1.2646337 1.2648178\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:35epoch=138; Loss Pred=1.2612; Val Loss=1.2648; Val Acc=0.6634; Loss Att={'forw': '0.2132'}; Train Acc=0.664; Test Acc=0.6617; Entropy={'forw': '7.7213'}; Entropy_Test=\n",
      "\n",
      "2 1.2646337 1.2620379\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:37epoch=139; Loss Pred=1.2599; Val Loss=1.2620; Val Acc=0.6638; Loss Att={'forw': '0.2129'}; Train Acc=0.665; Test Acc=0.6628; Entropy={'forw': '7.7004'}; Entropy_Test=\n",
      "\n",
      "0 1.2620379 1.2630012\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:38epoch=140; Loss Pred=1.2592; Val Loss=1.2630; Val Acc=0.6625; Loss Att={'forw': '0.2126'}; Train Acc=0.665; Test Acc=0.6613; Entropy={'forw': '7.7268'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.2620379 1.2584991\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:40epoch=141; Loss Pred=1.2573; Val Loss=1.2585; Val Acc=0.6644; Loss Att={'forw': '0.2120'}; Train Acc=0.667; Test Acc=0.6639; Entropy={'forw': '7.7114'}; Entropy_Test=\n",
      "\n",
      "0 1.2584991 1.261543\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:41epoch=142; Loss Pred=1.2599; Val Loss=1.2615; Val Acc=0.6643; Loss Att={'forw': '0.2119'}; Train Acc=0.665; Test Acc=0.6629; Entropy={'forw': '7.7078'}; Entropy_Test=\n",
      "\n",
      "1 1.2584991 1.2576172\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:43epoch=143; Loss Pred=1.2570; Val Loss=1.2576; Val Acc=0.6657; Loss Att={'forw': '0.2117'}; Train Acc=0.668; Test Acc=0.6645; Entropy={'forw': '7.6970'}; Entropy_Test=\n",
      "\n",
      "0 1.2576172 1.2579716\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:45epoch=144; Loss Pred=1.2566; Val Loss=1.2580; Val Acc=0.6659; Loss Att={'forw': '0.2116'}; Train Acc=0.667; Test Acc=0.6629; Entropy={'forw': '7.7081'}; Entropy_Test=\n",
      "\n",
      "1 1.2576172 1.2611715\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:46epoch=145; Loss Pred=1.2558; Val Loss=1.2612; Val Acc=0.6672; Loss Att={'forw': '0.2112'}; Train Acc=0.668; Test Acc=0.6646; Entropy={'forw': '7.7251'}; Entropy_Test=\n",
      "\n",
      "2 1.2576172 1.2607296\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:48epoch=146; Loss Pred=1.2552; Val Loss=1.2607; Val Acc=0.6655; Loss Att={'forw': '0.2115'}; Train Acc=0.668; Test Acc=0.6643; Entropy={'forw': '7.7097'}; Entropy_Test=\n",
      "\n",
      "3 1.2576172 1.2581631\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:50epoch=147; Loss Pred=1.2542; Val Loss=1.2582; Val Acc=0.6676; Loss Att={'forw': '0.2107'}; Train Acc=0.669; Test Acc=0.6651; Entropy={'forw': '7.7370'}; Entropy_Test=\n",
      "\n",
      "4 1.2576172 1.2571218\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:51epoch=148; Loss Pred=1.2544; Val Loss=1.2571; Val Acc=0.6677; Loss Att={'forw': '0.2107'}; Train Acc=0.668; Test Acc=0.6656; Entropy={'forw': '7.7236'}; Entropy_Test=\n",
      "\n",
      "0 1.2571218 1.2525713\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:53epoch=149; Loss Pred=1.2529; Val Loss=1.2526; Val Acc=0.6668; Loss Att={'forw': '0.2106'}; Train Acc=0.669; Test Acc=0.6642; Entropy={'forw': '7.7294'}; Entropy_Test=\n",
      "\n",
      "0 1.2525713 1.2543547\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:55epoch=150; Loss Pred=1.2532; Val Loss=1.2544; Val Acc=0.6669; Loss Att={'forw': '0.2099'}; Train Acc=0.670; Test Acc=0.6659; Entropy={'forw': '7.7055'}; Entropy_Test=\n",
      "\n",
      "1 1.2525713 1.2553476\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:56epoch=151; Loss Pred=1.2541; Val Loss=1.2553; Val Acc=0.6685; Loss Att={'forw': '0.2101'}; Train Acc=0.670; Test Acc=0.6639; Entropy={'forw': '7.7718'}; Entropy_Test=\n",
      "\n",
      "2 1.2525713 1.2566367\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:58epoch=152; Loss Pred=1.2523; Val Loss=1.2566; Val Acc=0.6679; Loss Att={'forw': '0.2103'}; Train Acc=0.670; Test Acc=0.6657; Entropy={'forw': '7.7523'}; Entropy_Test=\n",
      "\n",
      "3 1.2525713 1.2512307\n",
      "Logged Successfully: \n",
      "2018-05-17 19:03:59epoch=153; Loss Pred=1.2495; Val Loss=1.2512; Val Acc=0.6683; Loss Att={'forw': '0.2100'}; Train Acc=0.671; Test Acc=0.6661; Entropy={'forw': '7.7837'}; Entropy_Test=\n",
      "\n",
      "0 1.2512307 1.2538589\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:01epoch=154; Loss Pred=1.2504; Val Loss=1.2539; Val Acc=0.6685; Loss Att={'forw': '0.2093'}; Train Acc=0.670; Test Acc=0.6638; Entropy={'forw': '7.7453'}; Entropy_Test=\n",
      "\n",
      "1 1.2512307 1.253369\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:02epoch=155; Loss Pred=1.2474; Val Loss=1.2534; Val Acc=0.6683; Loss Att={'forw': '0.2091'}; Train Acc=0.670; Test Acc=0.6683; Entropy={'forw': '7.7749'}; Entropy_Test=\n",
      "\n",
      "2 1.2512307 1.2527882\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:04epoch=156; Loss Pred=1.2485; Val Loss=1.2528; Val Acc=0.6674; Loss Att={'forw': '0.2096'}; Train Acc=0.670; Test Acc=0.6663; Entropy={'forw': '7.7893'}; Entropy_Test=\n",
      "\n",
      "3 1.2512307 1.249008\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:06epoch=157; Loss Pred=1.2479; Val Loss=1.2490; Val Acc=0.6679; Loss Att={'forw': '0.2090'}; Train Acc=0.670; Test Acc=0.6652; Entropy={'forw': '7.7675'}; Entropy_Test=\n",
      "\n",
      "0 1.249008 1.2450544\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:07epoch=158; Loss Pred=1.2469; Val Loss=1.2451; Val Acc=0.6675; Loss Att={'forw': '0.2092'}; Train Acc=0.670; Test Acc=0.6651; Entropy={'forw': '7.7869'}; Entropy_Test=\n",
      "\n",
      "0 1.2450544 1.250038\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:09epoch=159; Loss Pred=1.2486; Val Loss=1.2500; Val Acc=0.6665; Loss Att={'forw': '0.2090'}; Train Acc=0.671; Test Acc=0.6672; Entropy={'forw': '7.7618'}; Entropy_Test=\n",
      "\n",
      "1 1.2450544 1.2503489\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:10epoch=160; Loss Pred=1.2465; Val Loss=1.2503; Val Acc=0.6686; Loss Att={'forw': '0.2090'}; Train Acc=0.671; Test Acc=0.6666; Entropy={'forw': '7.7839'}; Entropy_Test=\n",
      "\n",
      "2 1.2450544 1.2496626\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:12epoch=161; Loss Pred=1.2472; Val Loss=1.2497; Val Acc=0.6672; Loss Att={'forw': '0.2091'}; Train Acc=0.670; Test Acc=0.6665; Entropy={'forw': '7.8266'}; Entropy_Test=\n",
      "\n",
      "3 1.2450544 1.2453148\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:13epoch=162; Loss Pred=1.2455; Val Loss=1.2453; Val Acc=0.6665; Loss Att={'forw': '0.2087'}; Train Acc=0.671; Test Acc=0.6673; Entropy={'forw': '7.8525'}; Entropy_Test=\n",
      "\n",
      "4 1.2450544 1.2474664\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:15epoch=163; Loss Pred=1.2438; Val Loss=1.2475; Val Acc=0.6655; Loss Att={'forw': '0.2087'}; Train Acc=0.671; Test Acc=0.6664; Entropy={'forw': '7.8540'}; Entropy_Test=\n",
      "\n",
      "5 1.2450544 1.2483832\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:16epoch=164; Loss Pred=1.2459; Val Loss=1.2484; Val Acc=0.6673; Loss Att={'forw': '0.2086'}; Train Acc=0.671; Test Acc=0.6664; Entropy={'forw': '7.8637'}; Entropy_Test=\n",
      "\n",
      "6 1.2450544 1.2439272\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:18epoch=165; Loss Pred=1.2443; Val Loss=1.2439; Val Acc=0.6676; Loss Att={'forw': '0.2086'}; Train Acc=0.671; Test Acc=0.6667; Entropy={'forw': '7.8492'}; Entropy_Test=\n",
      "\n",
      "0 1.2439272 1.247525\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:19epoch=166; Loss Pred=1.2434; Val Loss=1.2475; Val Acc=0.6688; Loss Att={'forw': '0.2088'}; Train Acc=0.671; Test Acc=0.6652; Entropy={'forw': '7.8303'}; Entropy_Test=\n",
      "\n",
      "1 1.2439272 1.2429543\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:21epoch=167; Loss Pred=1.2421; Val Loss=1.2430; Val Acc=0.6674; Loss Att={'forw': '0.2087'}; Train Acc=0.670; Test Acc=0.6655; Entropy={'forw': '7.8512'}; Entropy_Test=\n",
      "\n",
      "0 1.2429543 1.2463803\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:23epoch=168; Loss Pred=1.2423; Val Loss=1.2464; Val Acc=0.6690; Loss Att={'forw': '0.2083'}; Train Acc=0.671; Test Acc=0.6666; Entropy={'forw': '7.8681'}; Entropy_Test=\n",
      "\n",
      "1 1.2429543 1.2440923\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:24epoch=169; Loss Pred=1.2404; Val Loss=1.2441; Val Acc=0.6670; Loss Att={'forw': '0.2084'}; Train Acc=0.671; Test Acc=0.6674; Entropy={'forw': '7.8585'}; Entropy_Test=\n",
      "\n",
      "2 1.2429543 1.2422706\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:26epoch=170; Loss Pred=1.2407; Val Loss=1.2423; Val Acc=0.6683; Loss Att={'forw': '0.2089'}; Train Acc=0.672; Test Acc=0.6680; Entropy={'forw': '7.8190'}; Entropy_Test=\n",
      "\n",
      "0 1.2422706 1.2438107\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:28epoch=171; Loss Pred=1.2395; Val Loss=1.2438; Val Acc=0.6687; Loss Att={'forw': '0.2084'}; Train Acc=0.671; Test Acc=0.6669; Entropy={'forw': '7.8173'}; Entropy_Test=\n",
      "\n",
      "1 1.2422706 1.2458247\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:29epoch=172; Loss Pred=1.2390; Val Loss=1.2458; Val Acc=0.6682; Loss Att={'forw': '0.2083'}; Train Acc=0.671; Test Acc=0.6651; Entropy={'forw': '7.8219'}; Entropy_Test=\n",
      "\n",
      "2 1.2422706 1.2453022\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:31epoch=173; Loss Pred=1.2405; Val Loss=1.2453; Val Acc=0.6685; Loss Att={'forw': '0.2081'}; Train Acc=0.670; Test Acc=0.6644; Entropy={'forw': '7.8604'}; Entropy_Test=\n",
      "\n",
      "3 1.2422706 1.2450185\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:32epoch=174; Loss Pred=1.2383; Val Loss=1.2450; Val Acc=0.6679; Loss Att={'forw': '0.2081'}; Train Acc=0.671; Test Acc=0.6663; Entropy={'forw': '7.9030'}; Entropy_Test=\n",
      "\n",
      "4 1.2422706 1.2418728\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:34epoch=175; Loss Pred=1.2387; Val Loss=1.2419; Val Acc=0.6674; Loss Att={'forw': '0.2084'}; Train Acc=0.671; Test Acc=0.6645; Entropy={'forw': '7.9427'}; Entropy_Test=\n",
      "\n",
      "0 1.2418728 1.2452122\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:35epoch=176; Loss Pred=1.2376; Val Loss=1.2452; Val Acc=0.6665; Loss Att={'forw': '0.2080'}; Train Acc=0.672; Test Acc=0.6665; Entropy={'forw': '7.9370'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.2418728 1.2399603\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:37epoch=177; Loss Pred=1.2369; Val Loss=1.2400; Val Acc=0.6680; Loss Att={'forw': '0.2078'}; Train Acc=0.671; Test Acc=0.6668; Entropy={'forw': '7.9490'}; Entropy_Test=\n",
      "\n",
      "0 1.2399603 1.2416017\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:38epoch=178; Loss Pred=1.2360; Val Loss=1.2416; Val Acc=0.6682; Loss Att={'forw': '0.2074'}; Train Acc=0.672; Test Acc=0.6659; Entropy={'forw': '7.9532'}; Entropy_Test=\n",
      "\n",
      "1 1.2399603 1.2416692\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:40epoch=179; Loss Pred=1.2357; Val Loss=1.2417; Val Acc=0.6682; Loss Att={'forw': '0.2079'}; Train Acc=0.671; Test Acc=0.6675; Entropy={'forw': '7.9628'}; Entropy_Test=\n",
      "\n",
      "2 1.2399603 1.2428632\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:41epoch=180; Loss Pred=1.2355; Val Loss=1.2429; Val Acc=0.6684; Loss Att={'forw': '0.2081'}; Train Acc=0.672; Test Acc=0.6671; Entropy={'forw': '7.9352'}; Entropy_Test=\n",
      "\n",
      "3 1.2399603 1.2382977\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:43epoch=181; Loss Pred=1.2347; Val Loss=1.2383; Val Acc=0.6678; Loss Att={'forw': '0.2076'}; Train Acc=0.672; Test Acc=0.6672; Entropy={'forw': '7.9751'}; Entropy_Test=\n",
      "\n",
      "0 1.2382977 1.2425717\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:44epoch=182; Loss Pred=1.2348; Val Loss=1.2426; Val Acc=0.6675; Loss Att={'forw': '0.2078'}; Train Acc=0.672; Test Acc=0.6656; Entropy={'forw': '7.9638'}; Entropy_Test=\n",
      "\n",
      "1 1.2382977 1.2438496\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:46epoch=183; Loss Pred=1.2365; Val Loss=1.2438; Val Acc=0.6682; Loss Att={'forw': '0.2076'}; Train Acc=0.671; Test Acc=0.6664; Entropy={'forw': '7.9803'}; Entropy_Test=\n",
      "\n",
      "2 1.2382977 1.2407469\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:47epoch=184; Loss Pred=1.2345; Val Loss=1.2407; Val Acc=0.6688; Loss Att={'forw': '0.2073'}; Train Acc=0.672; Test Acc=0.6662; Entropy={'forw': '7.9628'}; Entropy_Test=\n",
      "\n",
      "3 1.2382977 1.2375984\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:49epoch=185; Loss Pred=1.2332; Val Loss=1.2376; Val Acc=0.6672; Loss Att={'forw': '0.2075'}; Train Acc=0.671; Test Acc=0.6667; Entropy={'forw': '7.9686'}; Entropy_Test=\n",
      "\n",
      "0 1.2375984 1.2405905\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:51epoch=186; Loss Pred=1.2337; Val Loss=1.2406; Val Acc=0.6687; Loss Att={'forw': '0.2068'}; Train Acc=0.671; Test Acc=0.6664; Entropy={'forw': '7.9847'}; Entropy_Test=\n",
      "\n",
      "1 1.2375984 1.2363776\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:52epoch=187; Loss Pred=1.2339; Val Loss=1.2364; Val Acc=0.6678; Loss Att={'forw': '0.2079'}; Train Acc=0.672; Test Acc=0.6667; Entropy={'forw': '8.0138'}; Entropy_Test=\n",
      "\n",
      "0 1.2363776 1.2364597\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:54epoch=188; Loss Pred=1.2328; Val Loss=1.2365; Val Acc=0.6688; Loss Att={'forw': '0.2074'}; Train Acc=0.671; Test Acc=0.6676; Entropy={'forw': '7.9800'}; Entropy_Test=\n",
      "\n",
      "1 1.2363776 1.239009\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:55epoch=189; Loss Pred=1.2323; Val Loss=1.2390; Val Acc=0.6672; Loss Att={'forw': '0.2072'}; Train Acc=0.671; Test Acc=0.6686; Entropy={'forw': '7.9581'}; Entropy_Test=\n",
      "\n",
      "2 1.2363776 1.2388754\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:57epoch=190; Loss Pred=1.2329; Val Loss=1.2389; Val Acc=0.6693; Loss Att={'forw': '0.2069'}; Train Acc=0.671; Test Acc=0.6667; Entropy={'forw': '7.9366'}; Entropy_Test=\n",
      "\n",
      "3 1.2363776 1.2339302\n",
      "Logged Successfully: \n",
      "2018-05-17 19:04:59epoch=191; Loss Pred=1.2324; Val Loss=1.2339; Val Acc=0.6676; Loss Att={'forw': '0.2076'}; Train Acc=0.672; Test Acc=0.6667; Entropy={'forw': '7.9567'}; Entropy_Test=\n",
      "\n",
      "0 1.2339302 1.2376577\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:00epoch=192; Loss Pred=1.2322; Val Loss=1.2377; Val Acc=0.6672; Loss Att={'forw': '0.2074'}; Train Acc=0.672; Test Acc=0.6664; Entropy={'forw': '7.9861'}; Entropy_Test=\n",
      "\n",
      "1 1.2339302 1.2334687\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:02epoch=193; Loss Pred=1.2316; Val Loss=1.2335; Val Acc=0.6688; Loss Att={'forw': '0.2075'}; Train Acc=0.672; Test Acc=0.6673; Entropy={'forw': '8.0174'}; Entropy_Test=\n",
      "\n",
      "0 1.2334687 1.2334677\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:03epoch=194; Loss Pred=1.2324; Val Loss=1.2335; Val Acc=0.6685; Loss Att={'forw': '0.2072'}; Train Acc=0.670; Test Acc=0.6658; Entropy={'forw': '8.0031'}; Entropy_Test=\n",
      "\n",
      "0 1.2334677 1.2327399\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:05epoch=195; Loss Pred=1.2294; Val Loss=1.2327; Val Acc=0.6686; Loss Att={'forw': '0.2074'}; Train Acc=0.672; Test Acc=0.6689; Entropy={'forw': '7.9907'}; Entropy_Test=\n",
      "\n",
      "0 1.2327399 1.2347103\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:06epoch=196; Loss Pred=1.2291; Val Loss=1.2347; Val Acc=0.6683; Loss Att={'forw': '0.2068'}; Train Acc=0.671; Test Acc=0.6649; Entropy={'forw': '7.9884'}; Entropy_Test=\n",
      "\n",
      "1 1.2327399 1.2340229\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:08epoch=197; Loss Pred=1.2288; Val Loss=1.2340; Val Acc=0.6680; Loss Att={'forw': '0.2077'}; Train Acc=0.672; Test Acc=0.6677; Entropy={'forw': '8.0400'}; Entropy_Test=\n",
      "\n",
      "2 1.2327399 1.2350123\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:09epoch=198; Loss Pred=1.2285; Val Loss=1.2350; Val Acc=0.6700; Loss Att={'forw': '0.2070'}; Train Acc=0.672; Test Acc=0.6682; Entropy={'forw': '8.0410'}; Entropy_Test=\n",
      "\n",
      "3 1.2327399 1.2302208\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:11epoch=199; Loss Pred=1.2289; Val Loss=1.2302; Val Acc=0.6680; Loss Att={'forw': '0.2070'}; Train Acc=0.672; Test Acc=0.6658; Entropy={'forw': '8.0582'}; Entropy_Test=\n",
      "\n",
      "0 1.2302208 1.2290055\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:13epoch=200; Loss Pred=1.2280; Val Loss=1.2290; Val Acc=0.6674; Loss Att={'forw': '0.2072'}; Train Acc=0.672; Test Acc=0.6670; Entropy={'forw': '8.0128'}; Entropy_Test=\n",
      "\n",
      "0 1.2290055 1.2292643\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:14epoch=201; Loss Pred=1.2277; Val Loss=1.2293; Val Acc=0.6677; Loss Att={'forw': '0.2077'}; Train Acc=0.671; Test Acc=0.6689; Entropy={'forw': '8.0155'}; Entropy_Test=\n",
      "\n",
      "1 1.2290055 1.2307266\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:16epoch=202; Loss Pred=1.2272; Val Loss=1.2307; Val Acc=0.6673; Loss Att={'forw': '0.2067'}; Train Acc=0.671; Test Acc=0.6689; Entropy={'forw': '8.0139'}; Entropy_Test=\n",
      "\n",
      "2 1.2290055 1.2330611\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:17epoch=203; Loss Pred=1.2259; Val Loss=1.2331; Val Acc=0.6668; Loss Att={'forw': '0.2074'}; Train Acc=0.671; Test Acc=0.6667; Entropy={'forw': '8.0314'}; Entropy_Test=\n",
      "\n",
      "3 1.2290055 1.234216\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:19epoch=204; Loss Pred=1.2273; Val Loss=1.2342; Val Acc=0.6669; Loss Att={'forw': '0.2071'}; Train Acc=0.672; Test Acc=0.6689; Entropy={'forw': '8.0040'}; Entropy_Test=\n",
      "\n",
      "4 1.2290055 1.2314522\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:20epoch=205; Loss Pred=1.2271; Val Loss=1.2315; Val Acc=0.6687; Loss Att={'forw': '0.2067'}; Train Acc=0.671; Test Acc=0.6674; Entropy={'forw': '8.0486'}; Entropy_Test=\n",
      "\n",
      "5 1.2290055 1.2285422\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:22epoch=206; Loss Pred=1.2267; Val Loss=1.2285; Val Acc=0.6679; Loss Att={'forw': '0.2073'}; Train Acc=0.672; Test Acc=0.6671; Entropy={'forw': '8.0287'}; Entropy_Test=\n",
      "\n",
      "0 1.2285422 1.2292252\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:23epoch=207; Loss Pred=1.2251; Val Loss=1.2292; Val Acc=0.6702; Loss Att={'forw': '0.2076'}; Train Acc=0.672; Test Acc=0.6672; Entropy={'forw': '8.0400'}; Entropy_Test=\n",
      "\n",
      "1 1.2285422 1.2328554\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:25epoch=208; Loss Pred=1.2265; Val Loss=1.2329; Val Acc=0.6688; Loss Att={'forw': '0.2066'}; Train Acc=0.672; Test Acc=0.6665; Entropy={'forw': '8.0472'}; Entropy_Test=\n",
      "\n",
      "2 1.2285422 1.228574\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:27epoch=209; Loss Pred=1.2262; Val Loss=1.2286; Val Acc=0.6687; Loss Att={'forw': '0.2064'}; Train Acc=0.672; Test Acc=0.6685; Entropy={'forw': '8.0603'}; Entropy_Test=\n",
      "\n",
      "3 1.2285422 1.2295713\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:28epoch=210; Loss Pred=1.2272; Val Loss=1.2296; Val Acc=0.6709; Loss Att={'forw': '0.2070'}; Train Acc=0.672; Test Acc=0.6697; Entropy={'forw': '8.0452'}; Entropy_Test=\n",
      "\n",
      "4 1.2285422 1.2265942\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:30epoch=211; Loss Pred=1.2246; Val Loss=1.2266; Val Acc=0.6706; Loss Att={'forw': '0.2069'}; Train Acc=0.672; Test Acc=0.6658; Entropy={'forw': '8.0808'}; Entropy_Test=\n",
      "\n",
      "0 1.2265942 1.2327647\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:31epoch=212; Loss Pred=1.2260; Val Loss=1.2328; Val Acc=0.6675; Loss Att={'forw': '0.2064'}; Train Acc=0.671; Test Acc=0.6686; Entropy={'forw': '8.1238'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.2265942 1.2301618\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:33epoch=213; Loss Pred=1.2247; Val Loss=1.2302; Val Acc=0.6686; Loss Att={'forw': '0.2067'}; Train Acc=0.673; Test Acc=0.6671; Entropy={'forw': '8.0874'}; Entropy_Test=\n",
      "\n",
      "2 1.2265942 1.2291988\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:34epoch=214; Loss Pred=1.2237; Val Loss=1.2292; Val Acc=0.6687; Loss Att={'forw': '0.2064'}; Train Acc=0.672; Test Acc=0.6681; Entropy={'forw': '8.0456'}; Entropy_Test=\n",
      "\n",
      "3 1.2265942 1.2286267\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:36epoch=215; Loss Pred=1.2243; Val Loss=1.2286; Val Acc=0.6680; Loss Att={'forw': '0.2062'}; Train Acc=0.672; Test Acc=0.6686; Entropy={'forw': '8.0627'}; Entropy_Test=\n",
      "\n",
      "4 1.2265942 1.226015\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:38epoch=216; Loss Pred=1.2247; Val Loss=1.2260; Val Acc=0.6682; Loss Att={'forw': '0.2058'}; Train Acc=0.672; Test Acc=0.6669; Entropy={'forw': '8.0500'}; Entropy_Test=\n",
      "\n",
      "0 1.226015 1.2249581\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:39epoch=217; Loss Pred=1.2245; Val Loss=1.2250; Val Acc=0.6682; Loss Att={'forw': '0.2057'}; Train Acc=0.672; Test Acc=0.6685; Entropy={'forw': '8.0265'}; Entropy_Test=\n",
      "\n",
      "0 1.2249581 1.2254947\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:41epoch=218; Loss Pred=1.2227; Val Loss=1.2255; Val Acc=0.6669; Loss Att={'forw': '0.2056'}; Train Acc=0.672; Test Acc=0.6674; Entropy={'forw': '8.0359'}; Entropy_Test=\n",
      "\n",
      "1 1.2249581 1.228031\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:42epoch=219; Loss Pred=1.2238; Val Loss=1.2280; Val Acc=0.6698; Loss Att={'forw': '0.2061'}; Train Acc=0.673; Test Acc=0.6664; Entropy={'forw': '8.0038'}; Entropy_Test=\n",
      "\n",
      "2 1.2249581 1.2260271\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:44epoch=220; Loss Pred=1.2244; Val Loss=1.2260; Val Acc=0.6663; Loss Att={'forw': '0.2055'}; Train Acc=0.672; Test Acc=0.6688; Entropy={'forw': '7.9739'}; Entropy_Test=\n",
      "\n",
      "3 1.2249581 1.2258087\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:45epoch=221; Loss Pred=1.2212; Val Loss=1.2258; Val Acc=0.6686; Loss Att={'forw': '0.2055'}; Train Acc=0.672; Test Acc=0.6707; Entropy={'forw': '8.0027'}; Entropy_Test=\n",
      "\n",
      "4 1.2249581 1.2269142\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:47epoch=222; Loss Pred=1.2223; Val Loss=1.2269; Val Acc=0.6687; Loss Att={'forw': '0.2049'}; Train Acc=0.673; Test Acc=0.6669; Entropy={'forw': '7.9989'}; Entropy_Test=\n",
      "\n",
      "5 1.2249581 1.2258568\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:48epoch=223; Loss Pred=1.2218; Val Loss=1.2259; Val Acc=0.6706; Loss Att={'forw': '0.2050'}; Train Acc=0.673; Test Acc=0.6694; Entropy={'forw': '7.9935'}; Entropy_Test=\n",
      "\n",
      "6 1.2249581 1.2239116\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:50epoch=224; Loss Pred=1.2215; Val Loss=1.2239; Val Acc=0.6694; Loss Att={'forw': '0.2052'}; Train Acc=0.673; Test Acc=0.6690; Entropy={'forw': '8.0118'}; Entropy_Test=\n",
      "\n",
      "0 1.2239116 1.2241323\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:51epoch=225; Loss Pred=1.2213; Val Loss=1.2241; Val Acc=0.6692; Loss Att={'forw': '0.2052'}; Train Acc=0.672; Test Acc=0.6664; Entropy={'forw': '7.9896'}; Entropy_Test=\n",
      "\n",
      "1 1.2239116 1.2262563\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:53epoch=226; Loss Pred=1.2216; Val Loss=1.2263; Val Acc=0.6694; Loss Att={'forw': '0.2052'}; Train Acc=0.672; Test Acc=0.6676; Entropy={'forw': '8.0245'}; Entropy_Test=\n",
      "\n",
      "2 1.2239116 1.2251973\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:54epoch=227; Loss Pred=1.2228; Val Loss=1.2252; Val Acc=0.6704; Loss Att={'forw': '0.2045'}; Train Acc=0.672; Test Acc=0.6650; Entropy={'forw': '8.0242'}; Entropy_Test=\n",
      "\n",
      "3 1.2239116 1.2265819\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:56epoch=228; Loss Pred=1.2219; Val Loss=1.2266; Val Acc=0.6689; Loss Att={'forw': '0.2046'}; Train Acc=0.671; Test Acc=0.6686; Entropy={'forw': '8.0025'}; Entropy_Test=\n",
      "\n",
      "4 1.2239116 1.2210954\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:58epoch=229; Loss Pred=1.2213; Val Loss=1.2211; Val Acc=0.6710; Loss Att={'forw': '0.2049'}; Train Acc=0.672; Test Acc=0.6688; Entropy={'forw': '7.9980'}; Entropy_Test=\n",
      "\n",
      "0 1.2210954 1.2243686\n",
      "Logged Successfully: \n",
      "2018-05-17 19:05:59epoch=230; Loss Pred=1.2199; Val Loss=1.2244; Val Acc=0.6694; Loss Att={'forw': '0.2043'}; Train Acc=0.672; Test Acc=0.6663; Entropy={'forw': '8.0259'}; Entropy_Test=\n",
      "\n",
      "1 1.2210954 1.223155\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:01epoch=231; Loss Pred=1.2204; Val Loss=1.2232; Val Acc=0.6679; Loss Att={'forw': '0.2041'}; Train Acc=0.673; Test Acc=0.6675; Entropy={'forw': '8.0290'}; Entropy_Test=\n",
      "\n",
      "2 1.2210954 1.2249048\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:02epoch=232; Loss Pred=1.2206; Val Loss=1.2249; Val Acc=0.6704; Loss Att={'forw': '0.2038'}; Train Acc=0.672; Test Acc=0.6675; Entropy={'forw': '7.9796'}; Entropy_Test=\n",
      "\n",
      "3 1.2210954 1.2263122\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:04epoch=233; Loss Pred=1.2193; Val Loss=1.2263; Val Acc=0.6668; Loss Att={'forw': '0.2038'}; Train Acc=0.672; Test Acc=0.6668; Entropy={'forw': '8.0088'}; Entropy_Test=\n",
      "\n",
      "4 1.2210954 1.2241482\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:05epoch=234; Loss Pred=1.2215; Val Loss=1.2241; Val Acc=0.6692; Loss Att={'forw': '0.2036'}; Train Acc=0.673; Test Acc=0.6672; Entropy={'forw': '8.0172'}; Entropy_Test=\n",
      "\n",
      "5 1.2210954 1.2237021\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:07epoch=235; Loss Pred=1.2220; Val Loss=1.2237; Val Acc=0.6683; Loss Att={'forw': '0.2036'}; Train Acc=0.672; Test Acc=0.6666; Entropy={'forw': '8.0022'}; Entropy_Test=\n",
      "\n",
      "6 1.2210954 1.2244024\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:08epoch=236; Loss Pred=1.2215; Val Loss=1.2244; Val Acc=0.6687; Loss Att={'forw': '0.2035'}; Train Acc=0.672; Test Acc=0.6673; Entropy={'forw': '8.0182'}; Entropy_Test=\n",
      "\n",
      "7 1.2210954 1.226446\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:10epoch=237; Loss Pred=1.2210; Val Loss=1.2264; Val Acc=0.6669; Loss Att={'forw': '0.2034'}; Train Acc=0.672; Test Acc=0.6674; Entropy={'forw': '8.0637'}; Entropy_Test=\n",
      "\n",
      "8 1.2210954 1.2239273\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:11epoch=238; Loss Pred=1.2199; Val Loss=1.2239; Val Acc=0.6678; Loss Att={'forw': '0.2029'}; Train Acc=0.673; Test Acc=0.6705; Entropy={'forw': '8.0573'}; Entropy_Test=\n",
      "\n",
      "9 1.2210954 1.2194419\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:13epoch=239; Loss Pred=1.2207; Val Loss=1.2194; Val Acc=0.6702; Loss Att={'forw': '0.2031'}; Train Acc=0.673; Test Acc=0.6688; Entropy={'forw': '8.0641'}; Entropy_Test=\n",
      "\n",
      "0 1.2194419 1.2239134\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:14epoch=240; Loss Pred=1.2198; Val Loss=1.2239; Val Acc=0.6715; Loss Att={'forw': '0.2028'}; Train Acc=0.672; Test Acc=0.6664; Entropy={'forw': '8.0862'}; Entropy_Test=\n",
      "\n",
      "1 1.2194419 1.2222589\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:16epoch=241; Loss Pred=1.2198; Val Loss=1.2223; Val Acc=0.6688; Loss Att={'forw': '0.2034'}; Train Acc=0.672; Test Acc=0.6685; Entropy={'forw': '8.1217'}; Entropy_Test=\n",
      "\n",
      "2 1.2194419 1.222866\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:18epoch=242; Loss Pred=1.2226; Val Loss=1.2229; Val Acc=0.6714; Loss Att={'forw': '0.2028'}; Train Acc=0.672; Test Acc=0.6666; Entropy={'forw': '8.1408'}; Entropy_Test=\n",
      "\n",
      "3 1.2194419 1.2210464\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:19epoch=243; Loss Pred=1.2204; Val Loss=1.2210; Val Acc=0.6697; Loss Att={'forw': '0.2026'}; Train Acc=0.672; Test Acc=0.6664; Entropy={'forw': '8.1296'}; Entropy_Test=\n",
      "\n",
      "4 1.2194419 1.2237118\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:21epoch=244; Loss Pred=1.2209; Val Loss=1.2237; Val Acc=0.6714; Loss Att={'forw': '0.2022'}; Train Acc=0.672; Test Acc=0.6672; Entropy={'forw': '8.1481'}; Entropy_Test=\n",
      "\n",
      "5 1.2194419 1.2226517\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:22epoch=245; Loss Pred=1.2183; Val Loss=1.2227; Val Acc=0.6720; Loss Att={'forw': '0.2021'}; Train Acc=0.674; Test Acc=0.6694; Entropy={'forw': '8.1205'}; Entropy_Test=\n",
      "\n",
      "6 1.2194419 1.2186439\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:24epoch=246; Loss Pred=1.2183; Val Loss=1.2186; Val Acc=0.6703; Loss Att={'forw': '0.2016'}; Train Acc=0.672; Test Acc=0.6683; Entropy={'forw': '8.1196'}; Entropy_Test=\n",
      "\n",
      "0 1.2186439 1.2211198\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:26epoch=247; Loss Pred=1.2184; Val Loss=1.2211; Val Acc=0.6689; Loss Att={'forw': '0.2017'}; Train Acc=0.674; Test Acc=0.6672; Entropy={'forw': '8.1137'}; Entropy_Test=\n",
      "\n",
      "1 1.2186439 1.2224354\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:27epoch=248; Loss Pred=1.2204; Val Loss=1.2224; Val Acc=0.6694; Loss Att={'forw': '0.2010'}; Train Acc=0.673; Test Acc=0.6695; Entropy={'forw': '8.0528'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1.2186439 1.2201103\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:29epoch=249; Loss Pred=1.2213; Val Loss=1.2201; Val Acc=0.6682; Loss Att={'forw': '0.2001'}; Train Acc=0.674; Test Acc=0.6683; Entropy={'forw': '8.0491'}; Entropy_Test=\n",
      "\n",
      "3 1.2186439 1.2242802\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:30epoch=250; Loss Pred=1.2220; Val Loss=1.2243; Val Acc=0.6665; Loss Att={'forw': '0.2005'}; Train Acc=0.673; Test Acc=0.6682; Entropy={'forw': '8.0151'}; Entropy_Test=\n",
      "\n",
      "4 1.2186439 1.2249353\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:32epoch=251; Loss Pred=1.2211; Val Loss=1.2249; Val Acc=0.6678; Loss Att={'forw': '0.2005'}; Train Acc=0.673; Test Acc=0.6670; Entropy={'forw': '7.9976'}; Entropy_Test=\n",
      "\n",
      "5 1.2186439 1.2261243\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:33epoch=252; Loss Pred=1.2229; Val Loss=1.2261; Val Acc=0.6664; Loss Att={'forw': '0.1993'}; Train Acc=0.674; Test Acc=0.6668; Entropy={'forw': '7.9470'}; Entropy_Test=\n",
      "\n",
      "6 1.2186439 1.2195371\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:35epoch=253; Loss Pred=1.2190; Val Loss=1.2195; Val Acc=0.6696; Loss Att={'forw': '0.1991'}; Train Acc=0.674; Test Acc=0.6670; Entropy={'forw': '7.9715'}; Entropy_Test=\n",
      "\n",
      "7 1.2186439 1.2230258\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:36epoch=254; Loss Pred=1.2191; Val Loss=1.2230; Val Acc=0.6681; Loss Att={'forw': '0.1988'}; Train Acc=0.674; Test Acc=0.6688; Entropy={'forw': '7.9914'}; Entropy_Test=\n",
      "\n",
      "8 1.2186439 1.2177163\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:38epoch=255; Loss Pred=1.2183; Val Loss=1.2177; Val Acc=0.6688; Loss Att={'forw': '0.1989'}; Train Acc=0.674; Test Acc=0.6689; Entropy={'forw': '8.0258'}; Entropy_Test=\n",
      "\n",
      "0 1.2177163 1.2181678\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:40epoch=256; Loss Pred=1.2192; Val Loss=1.2182; Val Acc=0.6696; Loss Att={'forw': '0.1987'}; Train Acc=0.673; Test Acc=0.6696; Entropy={'forw': '8.0403'}; Entropy_Test=\n",
      "\n",
      "1 1.2177163 1.2237331\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:41epoch=257; Loss Pred=1.2230; Val Loss=1.2237; Val Acc=0.6691; Loss Att={'forw': '0.1983'}; Train Acc=0.672; Test Acc=0.6671; Entropy={'forw': '8.1056'}; Entropy_Test=\n",
      "\n",
      "2 1.2177163 1.2282928\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:43epoch=258; Loss Pred=1.2258; Val Loss=1.2283; Val Acc=0.6686; Loss Att={'forw': '0.1981'}; Train Acc=0.671; Test Acc=0.6652; Entropy={'forw': '8.1202'}; Entropy_Test=\n",
      "\n",
      "3 1.2177163 1.2230415\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:44epoch=259; Loss Pred=1.2232; Val Loss=1.2230; Val Acc=0.6682; Loss Att={'forw': '0.1977'}; Train Acc=0.672; Test Acc=0.6659; Entropy={'forw': '8.0968'}; Entropy_Test=\n",
      "\n",
      "4 1.2177163 1.2247955\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:46epoch=260; Loss Pred=1.2226; Val Loss=1.2248; Val Acc=0.6674; Loss Att={'forw': '0.1971'}; Train Acc=0.672; Test Acc=0.6658; Entropy={'forw': '8.0445'}; Entropy_Test=\n",
      "\n",
      "5 1.2177163 1.2200099\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:47epoch=261; Loss Pred=1.2179; Val Loss=1.2200; Val Acc=0.6699; Loss Att={'forw': '0.1974'}; Train Acc=0.674; Test Acc=0.6682; Entropy={'forw': '8.0023'}; Entropy_Test=\n",
      "\n",
      "6 1.2177163 1.2166743\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:49epoch=262; Loss Pred=1.2163; Val Loss=1.2167; Val Acc=0.6697; Loss Att={'forw': '0.1968'}; Train Acc=0.673; Test Acc=0.6669; Entropy={'forw': '8.0281'}; Entropy_Test=\n",
      "\n",
      "0 1.2166743 1.217759\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:50epoch=263; Loss Pred=1.2170; Val Loss=1.2178; Val Acc=0.6684; Loss Att={'forw': '0.1972'}; Train Acc=0.673; Test Acc=0.6669; Entropy={'forw': '8.0146'}; Entropy_Test=\n",
      "\n",
      "1 1.2166743 1.2240722\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:52epoch=264; Loss Pred=1.2199; Val Loss=1.2241; Val Acc=0.6664; Loss Att={'forw': '0.1966'}; Train Acc=0.673; Test Acc=0.6692; Entropy={'forw': '8.0083'}; Entropy_Test=\n",
      "\n",
      "2 1.2166743 1.2255199\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:53epoch=265; Loss Pred=1.2238; Val Loss=1.2255; Val Acc=0.6669; Loss Att={'forw': '0.1963'}; Train Acc=0.673; Test Acc=0.6680; Entropy={'forw': '8.0342'}; Entropy_Test=\n",
      "\n",
      "3 1.2166743 1.2296695\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:55epoch=266; Loss Pred=1.2264; Val Loss=1.2297; Val Acc=0.6656; Loss Att={'forw': '0.1962'}; Train Acc=0.672; Test Acc=0.6677; Entropy={'forw': '8.0198'}; Entropy_Test=\n",
      "\n",
      "4 1.2166743 1.2262664\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:57epoch=267; Loss Pred=1.2215; Val Loss=1.2263; Val Acc=0.6664; Loss Att={'forw': '0.1963'}; Train Acc=0.672; Test Acc=0.6669; Entropy={'forw': '8.0698'}; Entropy_Test=\n",
      "\n",
      "5 1.2166743 1.2258629\n",
      "Logged Successfully: \n",
      "2018-05-17 19:06:58epoch=268; Loss Pred=1.2243; Val Loss=1.2259; Val Acc=0.6682; Loss Att={'forw': '0.1959'}; Train Acc=0.673; Test Acc=0.6660; Entropy={'forw': '8.0552'}; Entropy_Test=\n",
      "\n",
      "6 1.2166743 1.2190044\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:00epoch=269; Loss Pred=1.2181; Val Loss=1.2190; Val Acc=0.6687; Loss Att={'forw': '0.1957'}; Train Acc=0.674; Test Acc=0.6678; Entropy={'forw': '8.0600'}; Entropy_Test=\n",
      "\n",
      "7 1.2166743 1.2195351\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:01epoch=270; Loss Pred=1.2179; Val Loss=1.2195; Val Acc=0.6708; Loss Att={'forw': '0.1963'}; Train Acc=0.673; Test Acc=0.6700; Entropy={'forw': '8.1355'}; Entropy_Test=\n",
      "\n",
      "8 1.2166743 1.2235243\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:03epoch=271; Loss Pred=1.2175; Val Loss=1.2235; Val Acc=0.6694; Loss Att={'forw': '0.1956'}; Train Acc=0.673; Test Acc=0.6681; Entropy={'forw': '8.1824'}; Entropy_Test=\n",
      "\n",
      "9 1.2166743 1.2282753\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:04epoch=272; Loss Pred=1.2231; Val Loss=1.2283; Val Acc=0.6696; Loss Att={'forw': '0.1958'}; Train Acc=0.671; Test Acc=0.6648; Entropy={'forw': '8.1698'}; Entropy_Test=\n",
      "\n",
      "10 1.2166743 1.2357545\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:06epoch=273; Loss Pred=1.2297; Val Loss=1.2358; Val Acc=0.6685; Loss Att={'forw': '0.1960'}; Train Acc=0.669; Test Acc=0.6616; Entropy={'forw': '8.2120'}; Entropy_Test=\n",
      "\n",
      "11 1.2166743 1.236181\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:07epoch=274; Loss Pred=1.2335; Val Loss=1.2362; Val Acc=0.6671; Loss Att={'forw': '0.1951'}; Train Acc=0.669; Test Acc=0.6636; Entropy={'forw': '8.2491'}; Entropy_Test=\n",
      "\n",
      "12 1.2166743 1.2219156\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:09epoch=275; Loss Pred=1.2203; Val Loss=1.2219; Val Acc=0.6717; Loss Att={'forw': '0.1949'}; Train Acc=0.672; Test Acc=0.6662; Entropy={'forw': '8.1610'}; Entropy_Test=\n",
      "\n",
      "13 1.2166743 1.2207913\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:10epoch=276; Loss Pred=1.2178; Val Loss=1.2208; Val Acc=0.6691; Loss Att={'forw': '0.1946'}; Train Acc=0.672; Test Acc=0.6684; Entropy={'forw': '8.1717'}; Entropy_Test=\n",
      "\n",
      "14 1.2166743 1.2223206\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:12epoch=277; Loss Pred=1.2166; Val Loss=1.2223; Val Acc=0.6698; Loss Att={'forw': '0.1948'}; Train Acc=0.674; Test Acc=0.6694; Entropy={'forw': '8.0877'}; Entropy_Test=\n",
      "\n",
      "15 1.2166743 1.2235866\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:13epoch=278; Loss Pred=1.2180; Val Loss=1.2236; Val Acc=0.6685; Loss Att={'forw': '0.1939'}; Train Acc=0.673; Test Acc=0.6695; Entropy={'forw': '8.0968'}; Entropy_Test=\n",
      "\n",
      "16 1.2166743 1.2301811\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:15epoch=279; Loss Pred=1.2234; Val Loss=1.2302; Val Acc=0.6666; Loss Att={'forw': '0.1937'}; Train Acc=0.672; Test Acc=0.6670; Entropy={'forw': '8.0125'}; Entropy_Test=\n",
      "\n",
      "17 1.2166743 1.2314144\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:17epoch=280; Loss Pred=1.2242; Val Loss=1.2314; Val Acc=0.6677; Loss Att={'forw': '0.1930'}; Train Acc=0.671; Test Acc=0.6664; Entropy={'forw': '7.9842'}; Entropy_Test=\n",
      "\n",
      "18 1.2166743 1.2345915\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:18epoch=281; Loss Pred=1.2267; Val Loss=1.2346; Val Acc=0.6634; Loss Att={'forw': '0.1920'}; Train Acc=0.671; Test Acc=0.6632; Entropy={'forw': '7.9310'}; Entropy_Test=\n",
      "\n",
      "19 1.2166743 1.2357293\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:20epoch=282; Loss Pred=1.2283; Val Loss=1.2357; Val Acc=0.6640; Loss Att={'forw': '0.1915'}; Train Acc=0.670; Test Acc=0.6666; Entropy={'forw': '7.7938'}; Entropy_Test=\n",
      "\n",
      "20 1.2166743 1.2323653\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 284\n",
      "Optimization Finished!\n",
      "Saved Results Successfully\n",
      "L2 reg-n\n",
      "********** replication  0  **********\n",
      "msnbc\n",
      "(3689, 40) (3689, 40) (737, 40) (737, 40)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tGRU bidir(False), task: msnbc\n",
      "    hid: \t\t\t50,\n",
      "    h_hid: \t\t\t100\n",
      "    n_attractor_iterations: \t0,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t3689/0, SEQ_LEN: 40\n",
      "Logged Successfully: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000000000.0 2.8820734\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:25epoch=0; Loss Pred=2.8867; Val Loss=2.8821; Val Acc=0.1198; Loss Att={'forw': '1.0000'}; Train Acc=0.119; Test Acc=0.1211; Entropy={'forw': '6.6938'}; Entropy_Test=\n",
      "\n",
      "0 2.8820734 2.815235\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:26epoch=1; Loss Pred=2.8199; Val Loss=2.8152; Val Acc=0.1828; Loss Att={'forw': '1.0000'}; Train Acc=0.178; Test Acc=0.1770; Entropy={'forw': '6.7012'}; Entropy_Test=\n",
      "\n",
      "0 2.815235 2.8151965\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:27epoch=2; Loss Pred=2.8200; Val Loss=2.8152; Val Acc=0.1824; Loss Att={'forw': '1.0000'}; Train Acc=0.178; Test Acc=0.1784; Entropy={'forw': '6.7012'}; Entropy_Test=\n",
      "\n",
      "0 2.8151965 2.748392\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:28epoch=3; Loss Pred=2.7538; Val Loss=2.7484; Val Acc=0.2184; Loss Att={'forw': '1.0000'}; Train Acc=0.212; Test Acc=0.2095; Entropy={'forw': '6.7567'}; Entropy_Test=\n",
      "\n",
      "0 2.748392 2.749419\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:29epoch=4; Loss Pred=2.7542; Val Loss=2.7494; Val Acc=0.2143; Loss Att={'forw': '1.0000'}; Train Acc=0.211; Test Acc=0.2111; Entropy={'forw': '6.7567'}; Entropy_Test=\n",
      "\n",
      "1 2.748392 2.6792328\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:30epoch=5; Loss Pred=2.6851; Val Loss=2.6792; Val Acc=0.2410; Loss Att={'forw': '1.0000'}; Train Acc=0.237; Test Acc=0.2370; Entropy={'forw': '7.0152'}; Entropy_Test=\n",
      "\n",
      "0 2.6792328 2.6795475\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:31epoch=6; Loss Pred=2.6853; Val Loss=2.6795; Val Acc=0.2403; Loss Att={'forw': '1.0000'}; Train Acc=0.237; Test Acc=0.2358; Entropy={'forw': '7.0152'}; Entropy_Test=\n",
      "\n",
      "1 2.6792328 2.6039808\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:31epoch=7; Loss Pred=2.6096; Val Loss=2.6040; Val Acc=0.2615; Loss Att={'forw': '1.0000'}; Train Acc=0.261; Test Acc=0.2570; Entropy={'forw': '7.2072'}; Entropy_Test=\n",
      "\n",
      "0 2.6039808 2.6030645\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:32epoch=8; Loss Pred=2.6098; Val Loss=2.6031; Val Acc=0.2666; Loss Att={'forw': '1.0000'}; Train Acc=0.262; Test Acc=0.2581; Entropy={'forw': '7.2072'}; Entropy_Test=\n",
      "\n",
      "0 2.6030645 2.5163805\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:33epoch=9; Loss Pred=2.5248; Val Loss=2.5164; Val Acc=0.2899; Loss Att={'forw': '1.0000'}; Train Acc=0.285; Test Acc=0.2814; Entropy={'forw': '7.4060'}; Entropy_Test=\n",
      "\n",
      "0 2.5163805 2.5173452\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:34epoch=10; Loss Pred=2.5246; Val Loss=2.5173; Val Acc=0.2876; Loss Att={'forw': '1.0000'}; Train Acc=0.283; Test Acc=0.2802; Entropy={'forw': '7.4060'}; Entropy_Test=\n",
      "\n",
      "1 2.5163805 2.4198987\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:35epoch=11; Loss Pred=2.4303; Val Loss=2.4199; Val Acc=0.2982; Loss Att={'forw': '1.0000'}; Train Acc=0.288; Test Acc=0.2856; Entropy={'forw': '7.7135'}; Entropy_Test=\n",
      "\n",
      "0 2.4198987 2.4196796\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:36epoch=12; Loss Pred=2.4303; Val Loss=2.4197; Val Acc=0.2936; Loss Att={'forw': '1.0000'}; Train Acc=0.288; Test Acc=0.2850; Entropy={'forw': '7.7135'}; Entropy_Test=\n",
      "\n",
      "0 2.4196796 2.3257143\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:37epoch=13; Loss Pred=2.3381; Val Loss=2.3257; Val Acc=0.2828; Loss Att={'forw': '1.0000'}; Train Acc=0.276; Test Acc=0.2710; Entropy={'forw': '8.1741'}; Entropy_Test=\n",
      "\n",
      "0 2.3257143 2.325199\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:37epoch=14; Loss Pred=2.3382; Val Loss=2.3252; Val Acc=0.2830; Loss Att={'forw': '1.0000'}; Train Acc=0.277; Test Acc=0.2710; Entropy={'forw': '8.1741'}; Entropy_Test=\n",
      "\n",
      "0 2.325199 2.2759156\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:38epoch=15; Loss Pred=2.2938; Val Loss=2.2759; Val Acc=0.2810; Loss Att={'forw': '1.0000'}; Train Acc=0.273; Test Acc=0.2713; Entropy={'forw': '8.3253'}; Entropy_Test=\n",
      "\n",
      "0 2.2759156 2.280466\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:39epoch=16; Loss Pred=2.2944; Val Loss=2.2805; Val Acc=0.2805; Loss Att={'forw': '1.0000'}; Train Acc=0.272; Test Acc=0.2680; Entropy={'forw': '8.3253'}; Entropy_Test=\n",
      "\n",
      "1 2.2759156 2.208564\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:40epoch=17; Loss Pred=2.2244; Val Loss=2.2086; Val Acc=0.3170; Loss Att={'forw': '1.0000'}; Train Acc=0.308; Test Acc=0.3056; Entropy={'forw': '8.3861'}; Entropy_Test=\n",
      "\n",
      "0 2.208564 2.2075832\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:41epoch=18; Loss Pred=2.2243; Val Loss=2.2076; Val Acc=0.3182; Loss Att={'forw': '1.0000'}; Train Acc=0.308; Test Acc=0.3061; Entropy={'forw': '8.3861'}; Entropy_Test=\n",
      "\n",
      "0 2.2075832 2.1168308\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:41epoch=19; Loss Pred=2.1319; Val Loss=2.1168; Val Acc=0.4012; Loss Att={'forw': '1.0000'}; Train Acc=0.387; Test Acc=0.3839; Entropy={'forw': '8.5880'}; Entropy_Test=\n",
      "\n",
      "0 2.1168308 2.1178322\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:42epoch=20; Loss Pred=2.1324; Val Loss=2.1178; Val Acc=0.3993; Loss Att={'forw': '1.0000'}; Train Acc=0.386; Test Acc=0.3827; Entropy={'forw': '8.5880'}; Entropy_Test=\n",
      "\n",
      "1 2.1168308 2.0436819\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:43epoch=21; Loss Pred=2.0577; Val Loss=2.0437; Val Acc=0.4723; Loss Att={'forw': '1.0000'}; Train Acc=0.462; Test Acc=0.4567; Entropy={'forw': '8.7161'}; Entropy_Test=\n",
      "\n",
      "0 2.0436819 2.0420406\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:44epoch=22; Loss Pred=2.0574; Val Loss=2.0420; Val Acc=0.4702; Loss Att={'forw': '1.0000'}; Train Acc=0.463; Test Acc=0.4571; Entropy={'forw': '8.7161'}; Entropy_Test=\n",
      "\n",
      "0 2.0420406 1.9850198\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:45epoch=23; Loss Pred=1.9983; Val Loss=1.9850; Val Acc=0.5085; Loss Att={'forw': '1.0000'}; Train Acc=0.499; Test Acc=0.4925; Entropy={'forw': '8.7128'}; Entropy_Test=\n",
      "\n",
      "0 1.9850198 1.9853286\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:46epoch=24; Loss Pred=1.9971; Val Loss=1.9853; Val Acc=0.5076; Loss Att={'forw': '1.0000'}; Train Acc=0.501; Test Acc=0.4935; Entropy={'forw': '8.7128'}; Entropy_Test=\n",
      "\n",
      "1 1.9850198 1.9260336\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:47epoch=25; Loss Pred=1.9416; Val Loss=1.9260; Val Acc=0.5316; Loss Att={'forw': '1.0000'}; Train Acc=0.522; Test Acc=0.5212; Entropy={'forw': '8.6252'}; Entropy_Test=\n",
      "\n",
      "0 1.9260336 1.9265188\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:48epoch=26; Loss Pred=1.9411; Val Loss=1.9265; Val Acc=0.5322; Loss Att={'forw': '1.0000'}; Train Acc=0.524; Test Acc=0.5179; Entropy={'forw': '8.6252'}; Entropy_Test=\n",
      "\n",
      "1 1.9260336 1.8650441\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:48epoch=27; Loss Pred=1.8796; Val Loss=1.8650; Val Acc=0.5526; Loss Att={'forw': '1.0000'}; Train Acc=0.544; Test Acc=0.5367; Entropy={'forw': '8.6564'}; Entropy_Test=\n",
      "\n",
      "0 1.8650441 1.8657223\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:49epoch=28; Loss Pred=1.8799; Val Loss=1.8657; Val Acc=0.5518; Loss Att={'forw': '1.0000'}; Train Acc=0.544; Test Acc=0.5381; Entropy={'forw': '8.6564'}; Entropy_Test=\n",
      "\n",
      "1 1.8650441 1.7958342\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:50epoch=29; Loss Pred=1.8128; Val Loss=1.7958; Val Acc=0.5736; Loss Att={'forw': '1.0000'}; Train Acc=0.566; Test Acc=0.5610; Entropy={'forw': '8.6901'}; Entropy_Test=\n",
      "\n",
      "0 1.7958342 1.7959479\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:51epoch=30; Loss Pred=1.8134; Val Loss=1.7959; Val Acc=0.5721; Loss Att={'forw': '1.0000'}; Train Acc=0.566; Test Acc=0.5593; Entropy={'forw': '8.6901'}; Entropy_Test=\n",
      "\n",
      "1 1.7958342 1.7365365\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:52epoch=31; Loss Pred=1.7521; Val Loss=1.7365; Val Acc=0.5877; Loss Att={'forw': '1.0000'}; Train Acc=0.584; Test Acc=0.5777; Entropy={'forw': '8.7272'}; Entropy_Test=\n",
      "\n",
      "0 1.7365365 1.7338012\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:53epoch=32; Loss Pred=1.7517; Val Loss=1.7338; Val Acc=0.5884; Loss Att={'forw': '1.0000'}; Train Acc=0.584; Test Acc=0.5762; Entropy={'forw': '8.7272'}; Entropy_Test=\n",
      "\n",
      "0 1.7338012 1.6778467\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:54epoch=33; Loss Pred=1.6964; Val Loss=1.6778; Val Acc=0.5997; Loss Att={'forw': '1.0000'}; Train Acc=0.593; Test Acc=0.5857; Entropy={'forw': '8.6829'}; Entropy_Test=\n",
      "\n",
      "0 1.6778467 1.6767634\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:54epoch=34; Loss Pred=1.6965; Val Loss=1.6768; Val Acc=0.5971; Loss Att={'forw': '1.0000'}; Train Acc=0.593; Test Acc=0.5859; Entropy={'forw': '8.6829'}; Entropy_Test=\n",
      "\n",
      "0 1.6767634 1.625855\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:55epoch=35; Loss Pred=1.6449; Val Loss=1.6259; Val Acc=0.6011; Loss Att={'forw': '1.0000'}; Train Acc=0.597; Test Acc=0.5891; Entropy={'forw': '8.6164'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.625855 1.6260668\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:56epoch=36; Loss Pred=1.6462; Val Loss=1.6261; Val Acc=0.6008; Loss Att={'forw': '1.0000'}; Train Acc=0.595; Test Acc=0.5888; Entropy={'forw': '8.6164'}; Entropy_Test=\n",
      "\n",
      "1 1.625855 1.5809948\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:57epoch=37; Loss Pred=1.6011; Val Loss=1.5810; Val Acc=0.6026; Loss Att={'forw': '1.0000'}; Train Acc=0.599; Test Acc=0.5943; Entropy={'forw': '8.6751'}; Entropy_Test=\n",
      "\n",
      "0 1.5809948 1.5800012\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:58epoch=38; Loss Pred=1.6003; Val Loss=1.5800; Val Acc=0.6046; Loss Att={'forw': '1.0000'}; Train Acc=0.600; Test Acc=0.5942; Entropy={'forw': '8.6751'}; Entropy_Test=\n",
      "\n",
      "0 1.5800012 1.5468185\n",
      "Logged Successfully: \n",
      "2018-05-17 19:07:59epoch=39; Loss Pred=1.5676; Val Loss=1.5468; Val Acc=0.6078; Loss Att={'forw': '1.0000'}; Train Acc=0.604; Test Acc=0.5963; Entropy={'forw': '8.7100'}; Entropy_Test=\n",
      "\n",
      "0 1.5468185 1.5449989\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:00epoch=40; Loss Pred=1.5671; Val Loss=1.5450; Val Acc=0.6095; Loss Att={'forw': '1.0000'}; Train Acc=0.603; Test Acc=0.5997; Entropy={'forw': '8.7100'}; Entropy_Test=\n",
      "\n",
      "0 1.5449989 1.5217426\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:00epoch=41; Loss Pred=1.5416; Val Loss=1.5217; Val Acc=0.6113; Loss Att={'forw': '1.0000'}; Train Acc=0.607; Test Acc=0.6031; Entropy={'forw': '8.7351'}; Entropy_Test=\n",
      "\n",
      "0 1.5217426 1.5196558\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:01epoch=42; Loss Pred=1.5421; Val Loss=1.5197; Val Acc=0.6128; Loss Att={'forw': '1.0000'}; Train Acc=0.606; Test Acc=0.6003; Entropy={'forw': '8.7351'}; Entropy_Test=\n",
      "\n",
      "0 1.5196558 1.4976108\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:02epoch=43; Loss Pred=1.5166; Val Loss=1.4976; Val Acc=0.6172; Loss Att={'forw': '1.0000'}; Train Acc=0.612; Test Acc=0.6063; Entropy={'forw': '8.6835'}; Entropy_Test=\n",
      "\n",
      "0 1.4976108 1.4972148\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:03epoch=44; Loss Pred=1.5158; Val Loss=1.4972; Val Acc=0.6158; Loss Att={'forw': '1.0000'}; Train Acc=0.612; Test Acc=0.6056; Entropy={'forw': '8.6835'}; Entropy_Test=\n",
      "\n",
      "0 1.4972148 1.4710592\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:04epoch=45; Loss Pred=1.4883; Val Loss=1.4711; Val Acc=0.6224; Loss Att={'forw': '1.0000'}; Train Acc=0.618; Test Acc=0.6145; Entropy={'forw': '8.6901'}; Entropy_Test=\n",
      "\n",
      "0 1.4710592 1.4695178\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:05epoch=46; Loss Pred=1.4884; Val Loss=1.4695; Val Acc=0.6229; Loss Att={'forw': '1.0000'}; Train Acc=0.618; Test Acc=0.6110; Entropy={'forw': '8.6901'}; Entropy_Test=\n",
      "\n",
      "0 1.4695178 1.4472147\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:06epoch=47; Loss Pred=1.4611; Val Loss=1.4472; Val Acc=0.6277; Loss Att={'forw': '1.0000'}; Train Acc=0.625; Test Acc=0.6202; Entropy={'forw': '8.5643'}; Entropy_Test=\n",
      "\n",
      "0 1.4472147 1.4462479\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:06epoch=48; Loss Pred=1.4611; Val Loss=1.4462; Val Acc=0.6262; Loss Att={'forw': '1.0000'}; Train Acc=0.624; Test Acc=0.6207; Entropy={'forw': '8.5643'}; Entropy_Test=\n",
      "\n",
      "0 1.4462479 1.4204663\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:07epoch=49; Loss Pred=1.4354; Val Loss=1.4205; Val Acc=0.6331; Loss Att={'forw': '1.0000'}; Train Acc=0.630; Test Acc=0.6291; Entropy={'forw': '8.5181'}; Entropy_Test=\n",
      "\n",
      "0 1.4204663 1.4230484\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:08epoch=50; Loss Pred=1.4350; Val Loss=1.4230; Val Acc=0.6322; Loss Att={'forw': '1.0000'}; Train Acc=0.631; Test Acc=0.6290; Entropy={'forw': '8.5181'}; Entropy_Test=\n",
      "\n",
      "1 1.4204663 1.3985076\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:09epoch=51; Loss Pred=1.4133; Val Loss=1.3985; Val Acc=0.6402; Loss Att={'forw': '1.0000'}; Train Acc=0.637; Test Acc=0.6331; Entropy={'forw': '8.5401'}; Entropy_Test=\n",
      "\n",
      "0 1.3985076 1.4000058\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:10epoch=52; Loss Pred=1.4125; Val Loss=1.4000; Val Acc=0.6384; Loss Att={'forw': '1.0000'}; Train Acc=0.638; Test Acc=0.6333; Entropy={'forw': '8.5401'}; Entropy_Test=\n",
      "\n",
      "1 1.3985076 1.3815271\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:10epoch=53; Loss Pred=1.3917; Val Loss=1.3815; Val Acc=0.6449; Loss Att={'forw': '1.0000'}; Train Acc=0.642; Test Acc=0.6391; Entropy={'forw': '8.5467'}; Entropy_Test=\n",
      "\n",
      "0 1.3815271 1.3824087\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:11epoch=54; Loss Pred=1.3913; Val Loss=1.3824; Val Acc=0.6437; Loss Att={'forw': '1.0000'}; Train Acc=0.642; Test Acc=0.6422; Entropy={'forw': '8.5467'}; Entropy_Test=\n",
      "\n",
      "1 1.3815271 1.3672441\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:12epoch=55; Loss Pred=1.3750; Val Loss=1.3672; Val Acc=0.6470; Loss Att={'forw': '1.0000'}; Train Acc=0.647; Test Acc=0.6426; Entropy={'forw': '8.4022'}; Entropy_Test=\n",
      "\n",
      "0 1.3672441 1.3670171\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:13epoch=56; Loss Pred=1.3741; Val Loss=1.3670; Val Acc=0.6468; Loss Att={'forw': '1.0000'}; Train Acc=0.646; Test Acc=0.6432; Entropy={'forw': '8.4022'}; Entropy_Test=\n",
      "\n",
      "0 1.3670171 1.3517948\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:14epoch=57; Loss Pred=1.3595; Val Loss=1.3518; Val Acc=0.6499; Loss Att={'forw': '1.0000'}; Train Acc=0.651; Test Acc=0.6470; Entropy={'forw': '8.3740'}; Entropy_Test=\n",
      "\n",
      "0 1.3517948 1.3516216\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:15epoch=58; Loss Pred=1.3606; Val Loss=1.3516; Val Acc=0.6516; Loss Att={'forw': '1.0000'}; Train Acc=0.651; Test Acc=0.6474; Entropy={'forw': '8.3740'}; Entropy_Test=\n",
      "\n",
      "0 1.3516216 1.3393093\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:16epoch=59; Loss Pred=1.3472; Val Loss=1.3393; Val Acc=0.6514; Loss Att={'forw': '1.0000'}; Train Acc=0.655; Test Acc=0.6482; Entropy={'forw': '8.3295'}; Entropy_Test=\n",
      "\n",
      "0 1.3393093 1.341005\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:16epoch=60; Loss Pred=1.3471; Val Loss=1.3410; Val Acc=0.6539; Loss Att={'forw': '1.0000'}; Train Acc=0.655; Test Acc=0.6508; Entropy={'forw': '8.3295'}; Entropy_Test=\n",
      "\n",
      "1 1.3393093 1.333279\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:17epoch=61; Loss Pred=1.3357; Val Loss=1.3333; Val Acc=0.6553; Loss Att={'forw': '1.0000'}; Train Acc=0.656; Test Acc=0.6543; Entropy={'forw': '8.2903'}; Entropy_Test=\n",
      "\n",
      "0 1.333279 1.3309749\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:18epoch=62; Loss Pred=1.3340; Val Loss=1.3310; Val Acc=0.6551; Loss Att={'forw': '1.0000'}; Train Acc=0.658; Test Acc=0.6535; Entropy={'forw': '8.2903'}; Entropy_Test=\n",
      "\n",
      "0 1.3309749 1.3199705\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:19epoch=63; Loss Pred=1.3260; Val Loss=1.3200; Val Acc=0.6577; Loss Att={'forw': '1.0000'}; Train Acc=0.659; Test Acc=0.6555; Entropy={'forw': '8.2981'}; Entropy_Test=\n",
      "\n",
      "0 1.3199705 1.3189048\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:20epoch=64; Loss Pred=1.3245; Val Loss=1.3189; Val Acc=0.6563; Loss Att={'forw': '1.0000'}; Train Acc=0.659; Test Acc=0.6543; Entropy={'forw': '8.2981'}; Entropy_Test=\n",
      "\n",
      "0 1.3189048 1.3104526\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:21epoch=65; Loss Pred=1.3162; Val Loss=1.3105; Val Acc=0.6565; Loss Att={'forw': '1.0000'}; Train Acc=0.661; Test Acc=0.6562; Entropy={'forw': '8.2397'}; Entropy_Test=\n",
      "\n",
      "0 1.3104526 1.3122305\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:21epoch=66; Loss Pred=1.3165; Val Loss=1.3122; Val Acc=0.6582; Loss Att={'forw': '1.0000'}; Train Acc=0.660; Test Acc=0.6543; Entropy={'forw': '8.2397'}; Entropy_Test=\n",
      "\n",
      "1 1.3104526 1.3032591\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:22epoch=67; Loss Pred=1.3065; Val Loss=1.3033; Val Acc=0.6591; Loss Att={'forw': '1.0000'}; Train Acc=0.662; Test Acc=0.6575; Entropy={'forw': '8.2398'}; Entropy_Test=\n",
      "\n",
      "0 1.3032591 1.3017634\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:23epoch=68; Loss Pred=1.3064; Val Loss=1.3018; Val Acc=0.6575; Loss Att={'forw': '1.0000'}; Train Acc=0.662; Test Acc=0.6537; Entropy={'forw': '8.2398'}; Entropy_Test=\n",
      "\n",
      "0 1.3017634 1.2955941\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:24epoch=69; Loss Pred=1.3007; Val Loss=1.2956; Val Acc=0.6589; Loss Att={'forw': '1.0000'}; Train Acc=0.663; Test Acc=0.6589; Entropy={'forw': '8.2578'}; Entropy_Test=\n",
      "\n",
      "0 1.2955941 1.2960256\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:25epoch=70; Loss Pred=1.2995; Val Loss=1.2960; Val Acc=0.6592; Loss Att={'forw': '1.0000'}; Train Acc=0.664; Test Acc=0.6577; Entropy={'forw': '8.2578'}; Entropy_Test=\n",
      "\n",
      "1 1.2955941 1.289938\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:26epoch=71; Loss Pred=1.2944; Val Loss=1.2899; Val Acc=0.6601; Loss Att={'forw': '1.0000'}; Train Acc=0.663; Test Acc=0.6592; Entropy={'forw': '8.2535'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.289938 1.2936041\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:27epoch=72; Loss Pred=1.2925; Val Loss=1.2936; Val Acc=0.6589; Loss Att={'forw': '1.0000'}; Train Acc=0.664; Test Acc=0.6585; Entropy={'forw': '8.2535'}; Entropy_Test=\n",
      "\n",
      "1 1.289938 1.2835417\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:27epoch=73; Loss Pred=1.2883; Val Loss=1.2835; Val Acc=0.6596; Loss Att={'forw': '1.0000'}; Train Acc=0.665; Test Acc=0.6595; Entropy={'forw': '8.1836'}; Entropy_Test=\n",
      "\n",
      "0 1.2835417 1.2877133\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:28epoch=74; Loss Pred=1.2884; Val Loss=1.2877; Val Acc=0.6593; Loss Att={'forw': '1.0000'}; Train Acc=0.665; Test Acc=0.6580; Entropy={'forw': '8.1836'}; Entropy_Test=\n",
      "\n",
      "1 1.2835417 1.2783679\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:29epoch=75; Loss Pred=1.2840; Val Loss=1.2784; Val Acc=0.6605; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6595; Entropy={'forw': '8.1824'}; Entropy_Test=\n",
      "\n",
      "0 1.2783679 1.2815919\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:30epoch=76; Loss Pred=1.2824; Val Loss=1.2816; Val Acc=0.6585; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6572; Entropy={'forw': '8.1824'}; Entropy_Test=\n",
      "\n",
      "1 1.2783679 1.2773243\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:31epoch=77; Loss Pred=1.2800; Val Loss=1.2773; Val Acc=0.6613; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6610; Entropy={'forw': '8.1104'}; Entropy_Test=\n",
      "\n",
      "0 1.2773243 1.2761582\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:31epoch=78; Loss Pred=1.2792; Val Loss=1.2762; Val Acc=0.6613; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6612; Entropy={'forw': '8.1104'}; Entropy_Test=\n",
      "\n",
      "0 1.2761582 1.2716987\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:32epoch=79; Loss Pred=1.2751; Val Loss=1.2717; Val Acc=0.6624; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6623; Entropy={'forw': '8.1775'}; Entropy_Test=\n",
      "\n",
      "0 1.2716987 1.2717888\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:33epoch=80; Loss Pred=1.2753; Val Loss=1.2718; Val Acc=0.6605; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6618; Entropy={'forw': '8.1775'}; Entropy_Test=\n",
      "\n",
      "1 1.2716987 1.2718842\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:34epoch=81; Loss Pred=1.2716; Val Loss=1.2719; Val Acc=0.6606; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6622; Entropy={'forw': '8.1621'}; Entropy_Test=\n",
      "\n",
      "2 1.2716987 1.2710121\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:35epoch=82; Loss Pred=1.2704; Val Loss=1.2710; Val Acc=0.6636; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6619; Entropy={'forw': '8.1621'}; Entropy_Test=\n",
      "\n",
      "0 1.2710121 1.2645127\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:36epoch=83; Loss Pred=1.2657; Val Loss=1.2645; Val Acc=0.6615; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6626; Entropy={'forw': '8.1531'}; Entropy_Test=\n",
      "\n",
      "0 1.2645127 1.2641006\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:36epoch=84; Loss Pred=1.2655; Val Loss=1.2641; Val Acc=0.6618; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6637; Entropy={'forw': '8.1531'}; Entropy_Test=\n",
      "\n",
      "0 1.2641006 1.2622976\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:37epoch=85; Loss Pred=1.2625; Val Loss=1.2623; Val Acc=0.6621; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6603; Entropy={'forw': '8.2317'}; Entropy_Test=\n",
      "\n",
      "0 1.2622976 1.2627782\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:38epoch=86; Loss Pred=1.2600; Val Loss=1.2628; Val Acc=0.6621; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6642; Entropy={'forw': '8.2317'}; Entropy_Test=\n",
      "\n",
      "1 1.2622976 1.2599986\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:39epoch=87; Loss Pred=1.2586; Val Loss=1.2600; Val Acc=0.6619; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6619; Entropy={'forw': '8.2873'}; Entropy_Test=\n",
      "\n",
      "0 1.2599986 1.2602986\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:40epoch=88; Loss Pred=1.2585; Val Loss=1.2603; Val Acc=0.6615; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6622; Entropy={'forw': '8.2873'}; Entropy_Test=\n",
      "\n",
      "1 1.2599986 1.2523873\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:41epoch=89; Loss Pred=1.2550; Val Loss=1.2524; Val Acc=0.6631; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6649; Entropy={'forw': '8.2487'}; Entropy_Test=\n",
      "\n",
      "0 1.2523873 1.2552226\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:41epoch=90; Loss Pred=1.2549; Val Loss=1.2552; Val Acc=0.6645; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6648; Entropy={'forw': '8.2487'}; Entropy_Test=\n",
      "\n",
      "1 1.2523873 1.252258\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:42epoch=91; Loss Pred=1.2526; Val Loss=1.2523; Val Acc=0.6646; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6647; Entropy={'forw': '8.2405'}; Entropy_Test=\n",
      "\n",
      "0 1.252258 1.2515544\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:43epoch=92; Loss Pred=1.2536; Val Loss=1.2516; Val Acc=0.6637; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6622; Entropy={'forw': '8.2405'}; Entropy_Test=\n",
      "\n",
      "0 1.2515544 1.2459316\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:44epoch=93; Loss Pred=1.2471; Val Loss=1.2459; Val Acc=0.6644; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6641; Entropy={'forw': '8.2780'}; Entropy_Test=\n",
      "\n",
      "0 1.2459316 1.2504082\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:45epoch=94; Loss Pred=1.2471; Val Loss=1.2504; Val Acc=0.6618; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6649; Entropy={'forw': '8.2780'}; Entropy_Test=\n",
      "\n",
      "1 1.2459316 1.2466849\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:45epoch=95; Loss Pred=1.2462; Val Loss=1.2467; Val Acc=0.6636; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6629; Entropy={'forw': '8.2598'}; Entropy_Test=\n",
      "\n",
      "2 1.2459316 1.2444866\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:46epoch=96; Loss Pred=1.2467; Val Loss=1.2445; Val Acc=0.6638; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6632; Entropy={'forw': '8.2598'}; Entropy_Test=\n",
      "\n",
      "0 1.2444866 1.2450356\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:47epoch=97; Loss Pred=1.2452; Val Loss=1.2450; Val Acc=0.6646; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6667; Entropy={'forw': '8.3227'}; Entropy_Test=\n",
      "\n",
      "1 1.2444866 1.2448455\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:48epoch=98; Loss Pred=1.2431; Val Loss=1.2448; Val Acc=0.6650; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6655; Entropy={'forw': '8.3227'}; Entropy_Test=\n",
      "\n",
      "2 1.2444866 1.2440077\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:49epoch=99; Loss Pred=1.2410; Val Loss=1.2440; Val Acc=0.6652; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6641; Entropy={'forw': '8.3391'}; Entropy_Test=\n",
      "\n",
      "0 1.2440077 1.2385719\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:50epoch=100; Loss Pred=1.2404; Val Loss=1.2386; Val Acc=0.6648; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6648; Entropy={'forw': '8.3391'}; Entropy_Test=\n",
      "\n",
      "0 1.2385719 1.2367669\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:50epoch=101; Loss Pred=1.2355; Val Loss=1.2368; Val Acc=0.6654; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6653; Entropy={'forw': '8.3633'}; Entropy_Test=\n",
      "\n",
      "0 1.2367669 1.2362157\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:51epoch=102; Loss Pred=1.2375; Val Loss=1.2362; Val Acc=0.6650; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6643; Entropy={'forw': '8.3633'}; Entropy_Test=\n",
      "\n",
      "0 1.2362157 1.231703\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:52epoch=103; Loss Pred=1.2368; Val Loss=1.2317; Val Acc=0.6668; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6676; Entropy={'forw': '8.4152'}; Entropy_Test=\n",
      "\n",
      "0 1.231703 1.2351116\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:53epoch=104; Loss Pred=1.2337; Val Loss=1.2351; Val Acc=0.6652; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6647; Entropy={'forw': '8.4152'}; Entropy_Test=\n",
      "\n",
      "1 1.231703 1.2358758\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:54epoch=105; Loss Pred=1.2324; Val Loss=1.2359; Val Acc=0.6661; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6676; Entropy={'forw': '8.4391'}; Entropy_Test=\n",
      "\n",
      "2 1.231703 1.2362847\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:54epoch=106; Loss Pred=1.2319; Val Loss=1.2363; Val Acc=0.6670; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6652; Entropy={'forw': '8.4391'}; Entropy_Test=\n",
      "\n",
      "3 1.231703 1.2329515\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:55epoch=107; Loss Pred=1.2306; Val Loss=1.2330; Val Acc=0.6674; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6626; Entropy={'forw': '8.4476'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1.231703 1.2305459\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:56epoch=108; Loss Pred=1.2299; Val Loss=1.2305; Val Acc=0.6665; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6641; Entropy={'forw': '8.4476'}; Entropy_Test=\n",
      "\n",
      "0 1.2305459 1.229234\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:57epoch=109; Loss Pred=1.2282; Val Loss=1.2292; Val Acc=0.6678; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6669; Entropy={'forw': '8.4687'}; Entropy_Test=\n",
      "\n",
      "0 1.229234 1.2315247\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:58epoch=110; Loss Pred=1.2271; Val Loss=1.2315; Val Acc=0.6691; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6636; Entropy={'forw': '8.4687'}; Entropy_Test=\n",
      "\n",
      "1 1.229234 1.2242539\n",
      "Logged Successfully: \n",
      "2018-05-17 19:08:59epoch=111; Loss Pred=1.2259; Val Loss=1.2243; Val Acc=0.6667; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6659; Entropy={'forw': '8.4738'}; Entropy_Test=\n",
      "\n",
      "0 1.2242539 1.2268114\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:00epoch=112; Loss Pred=1.2245; Val Loss=1.2268; Val Acc=0.6680; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6666; Entropy={'forw': '8.4738'}; Entropy_Test=\n",
      "\n",
      "1 1.2242539 1.2239945\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:00epoch=113; Loss Pred=1.2229; Val Loss=1.2240; Val Acc=0.6667; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6662; Entropy={'forw': '8.4984'}; Entropy_Test=\n",
      "\n",
      "0 1.2239945 1.2247856\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:01epoch=114; Loss Pred=1.2219; Val Loss=1.2248; Val Acc=0.6671; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6663; Entropy={'forw': '8.4984'}; Entropy_Test=\n",
      "\n",
      "1 1.2239945 1.2216291\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:02epoch=115; Loss Pred=1.2209; Val Loss=1.2216; Val Acc=0.6678; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6669; Entropy={'forw': '8.5704'}; Entropy_Test=\n",
      "\n",
      "0 1.2216291 1.2236099\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:03epoch=116; Loss Pred=1.2196; Val Loss=1.2236; Val Acc=0.6683; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6668; Entropy={'forw': '8.5704'}; Entropy_Test=\n",
      "\n",
      "1 1.2216291 1.2223139\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:04epoch=117; Loss Pred=1.2180; Val Loss=1.2223; Val Acc=0.6672; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6675; Entropy={'forw': '8.5881'}; Entropy_Test=\n",
      "\n",
      "2 1.2216291 1.2192619\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:04epoch=118; Loss Pred=1.2183; Val Loss=1.2193; Val Acc=0.6677; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6663; Entropy={'forw': '8.5881'}; Entropy_Test=\n",
      "\n",
      "0 1.2192619 1.2179282\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:05epoch=119; Loss Pred=1.2177; Val Loss=1.2179; Val Acc=0.6702; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6666; Entropy={'forw': '8.6112'}; Entropy_Test=\n",
      "\n",
      "0 1.2179282 1.21634\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:06epoch=120; Loss Pred=1.2169; Val Loss=1.2163; Val Acc=0.6676; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6657; Entropy={'forw': '8.6112'}; Entropy_Test=\n",
      "\n",
      "0 1.21634 1.2139275\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:07epoch=121; Loss Pred=1.2151; Val Loss=1.2139; Val Acc=0.6685; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6673; Entropy={'forw': '8.6634'}; Entropy_Test=\n",
      "\n",
      "0 1.2139275 1.2130064\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:08epoch=122; Loss Pred=1.2140; Val Loss=1.2130; Val Acc=0.6707; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6667; Entropy={'forw': '8.6634'}; Entropy_Test=\n",
      "\n",
      "0 1.2130064 1.2139672\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:08epoch=123; Loss Pred=1.2124; Val Loss=1.2140; Val Acc=0.6677; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6678; Entropy={'forw': '8.6688'}; Entropy_Test=\n",
      "\n",
      "1 1.2130064 1.2145195\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:09epoch=124; Loss Pred=1.2126; Val Loss=1.2145; Val Acc=0.6685; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6656; Entropy={'forw': '8.6688'}; Entropy_Test=\n",
      "\n",
      "2 1.2130064 1.2099633\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:10epoch=125; Loss Pred=1.2126; Val Loss=1.2100; Val Acc=0.6683; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6654; Entropy={'forw': '8.6925'}; Entropy_Test=\n",
      "\n",
      "0 1.2099633 1.2139674\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:11epoch=126; Loss Pred=1.2107; Val Loss=1.2140; Val Acc=0.6685; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6669; Entropy={'forw': '8.6925'}; Entropy_Test=\n",
      "\n",
      "1 1.2099633 1.2115219\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:11epoch=127; Loss Pred=1.2114; Val Loss=1.2115; Val Acc=0.6697; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6671; Entropy={'forw': '8.7153'}; Entropy_Test=\n",
      "\n",
      "2 1.2099633 1.2151563\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:12epoch=128; Loss Pred=1.2098; Val Loss=1.2152; Val Acc=0.6675; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6672; Entropy={'forw': '8.7153'}; Entropy_Test=\n",
      "\n",
      "3 1.2099633 1.2115477\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:13epoch=129; Loss Pred=1.2084; Val Loss=1.2115; Val Acc=0.6677; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6669; Entropy={'forw': '8.6950'}; Entropy_Test=\n",
      "\n",
      "4 1.2099633 1.2082204\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:14epoch=130; Loss Pred=1.2089; Val Loss=1.2082; Val Acc=0.6699; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6629; Entropy={'forw': '8.6950'}; Entropy_Test=\n",
      "\n",
      "0 1.2082204 1.2068896\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:14epoch=131; Loss Pred=1.2076; Val Loss=1.2069; Val Acc=0.6692; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6669; Entropy={'forw': '8.7322'}; Entropy_Test=\n",
      "\n",
      "0 1.2068896 1.2098589\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:15epoch=132; Loss Pred=1.2056; Val Loss=1.2099; Val Acc=0.6681; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6665; Entropy={'forw': '8.7322'}; Entropy_Test=\n",
      "\n",
      "1 1.2068896 1.2129018\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:16epoch=133; Loss Pred=1.2037; Val Loss=1.2129; Val Acc=0.6690; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6648; Entropy={'forw': '8.7350'}; Entropy_Test=\n",
      "\n",
      "2 1.2068896 1.2031784\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:17epoch=134; Loss Pred=1.2056; Val Loss=1.2032; Val Acc=0.6693; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6673; Entropy={'forw': '8.7350'}; Entropy_Test=\n",
      "\n",
      "0 1.2031784 1.2062448\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:18epoch=135; Loss Pred=1.2036; Val Loss=1.2062; Val Acc=0.6684; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6668; Entropy={'forw': '8.7707'}; Entropy_Test=\n",
      "\n",
      "1 1.2031784 1.2054306\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:18epoch=136; Loss Pred=1.2044; Val Loss=1.2054; Val Acc=0.6673; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6664; Entropy={'forw': '8.7707'}; Entropy_Test=\n",
      "\n",
      "2 1.2031784 1.2034127\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:19epoch=137; Loss Pred=1.2016; Val Loss=1.2034; Val Acc=0.6682; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6658; Entropy={'forw': '8.7691'}; Entropy_Test=\n",
      "\n",
      "3 1.2031784 1.2063313\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:20epoch=138; Loss Pred=1.2021; Val Loss=1.2063; Val Acc=0.6673; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6676; Entropy={'forw': '8.7691'}; Entropy_Test=\n",
      "\n",
      "4 1.2031784 1.2040151\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:21epoch=139; Loss Pred=1.2016; Val Loss=1.2040; Val Acc=0.6699; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6679; Entropy={'forw': '8.7900'}; Entropy_Test=\n",
      "\n",
      "5 1.2031784 1.1994011\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:21epoch=140; Loss Pred=1.2005; Val Loss=1.1994; Val Acc=0.6686; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6672; Entropy={'forw': '8.7900'}; Entropy_Test=\n",
      "\n",
      "0 1.1994011 1.2045647\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:22epoch=141; Loss Pred=1.2010; Val Loss=1.2046; Val Acc=0.6673; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6676; Entropy={'forw': '8.8141'}; Entropy_Test=\n",
      "\n",
      "1 1.1994011 1.2025146\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:23epoch=142; Loss Pred=1.2003; Val Loss=1.2025; Val Acc=0.6679; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6676; Entropy={'forw': '8.8141'}; Entropy_Test=\n",
      "\n",
      "2 1.1994011 1.2025582\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:24epoch=143; Loss Pred=1.1978; Val Loss=1.2026; Val Acc=0.6698; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6688; Entropy={'forw': '8.8671'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1.1994011 1.2017353\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:25epoch=144; Loss Pred=1.1987; Val Loss=1.2017; Val Acc=0.6679; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6664; Entropy={'forw': '8.8671'}; Entropy_Test=\n",
      "\n",
      "4 1.1994011 1.1988779\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:25epoch=145; Loss Pred=1.1988; Val Loss=1.1989; Val Acc=0.6708; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6677; Entropy={'forw': '8.8801'}; Entropy_Test=\n",
      "\n",
      "0 1.1988779 1.1998618\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:26epoch=146; Loss Pred=1.1967; Val Loss=1.1999; Val Acc=0.6677; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6671; Entropy={'forw': '8.8801'}; Entropy_Test=\n",
      "\n",
      "1 1.1988779 1.198002\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:27epoch=147; Loss Pred=1.1953; Val Loss=1.1980; Val Acc=0.6695; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6680; Entropy={'forw': '8.8846'}; Entropy_Test=\n",
      "\n",
      "0 1.198002 1.1955723\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:28epoch=148; Loss Pred=1.1970; Val Loss=1.1956; Val Acc=0.6705; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6687; Entropy={'forw': '8.8846'}; Entropy_Test=\n",
      "\n",
      "0 1.1955723 1.1982651\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:28epoch=149; Loss Pred=1.1973; Val Loss=1.1983; Val Acc=0.6688; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6667; Entropy={'forw': '8.9095'}; Entropy_Test=\n",
      "\n",
      "1 1.1955723 1.1955928\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:29epoch=150; Loss Pred=1.1950; Val Loss=1.1956; Val Acc=0.6723; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6676; Entropy={'forw': '8.9095'}; Entropy_Test=\n",
      "\n",
      "2 1.1955723 1.1947576\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:30epoch=151; Loss Pred=1.1934; Val Loss=1.1948; Val Acc=0.6702; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6659; Entropy={'forw': '8.9204'}; Entropy_Test=\n",
      "\n",
      "0 1.1947576 1.1924319\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:31epoch=152; Loss Pred=1.1933; Val Loss=1.1924; Val Acc=0.6688; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6675; Entropy={'forw': '8.9204'}; Entropy_Test=\n",
      "\n",
      "0 1.1924319 1.1947124\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:32epoch=153; Loss Pred=1.1918; Val Loss=1.1947; Val Acc=0.6690; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6681; Entropy={'forw': '8.9253'}; Entropy_Test=\n",
      "\n",
      "1 1.1924319 1.1970029\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:32epoch=154; Loss Pred=1.1941; Val Loss=1.1970; Val Acc=0.6708; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6660; Entropy={'forw': '8.9253'}; Entropy_Test=\n",
      "\n",
      "2 1.1924319 1.1955323\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:33epoch=155; Loss Pred=1.1923; Val Loss=1.1955; Val Acc=0.6702; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6703; Entropy={'forw': '8.9259'}; Entropy_Test=\n",
      "\n",
      "3 1.1924319 1.190617\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:34epoch=156; Loss Pred=1.1920; Val Loss=1.1906; Val Acc=0.6712; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6683; Entropy={'forw': '8.9259'}; Entropy_Test=\n",
      "\n",
      "0 1.190617 1.1922998\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:35epoch=157; Loss Pred=1.1917; Val Loss=1.1923; Val Acc=0.6707; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6672; Entropy={'forw': '8.9204'}; Entropy_Test=\n",
      "\n",
      "1 1.190617 1.1902239\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:35epoch=158; Loss Pred=1.1922; Val Loss=1.1902; Val Acc=0.6712; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6673; Entropy={'forw': '8.9204'}; Entropy_Test=\n",
      "\n",
      "0 1.1902239 1.192439\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:36epoch=159; Loss Pred=1.1892; Val Loss=1.1924; Val Acc=0.6706; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6690; Entropy={'forw': '8.9263'}; Entropy_Test=\n",
      "\n",
      "1 1.1902239 1.1953139\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:37epoch=160; Loss Pred=1.1909; Val Loss=1.1953; Val Acc=0.6691; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6685; Entropy={'forw': '8.9263'}; Entropy_Test=\n",
      "\n",
      "2 1.1902239 1.1887817\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:38epoch=161; Loss Pred=1.1907; Val Loss=1.1888; Val Acc=0.6702; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6683; Entropy={'forw': '8.9403'}; Entropy_Test=\n",
      "\n",
      "0 1.1887817 1.1883588\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:39epoch=162; Loss Pred=1.1879; Val Loss=1.1884; Val Acc=0.6700; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6693; Entropy={'forw': '8.9403'}; Entropy_Test=\n",
      "\n",
      "0 1.1883588 1.187425\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:39epoch=163; Loss Pred=1.1895; Val Loss=1.1874; Val Acc=0.6696; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6689; Entropy={'forw': '8.9586'}; Entropy_Test=\n",
      "\n",
      "0 1.187425 1.1850877\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:40epoch=164; Loss Pred=1.1881; Val Loss=1.1851; Val Acc=0.6697; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6681; Entropy={'forw': '8.9586'}; Entropy_Test=\n",
      "\n",
      "0 1.1850877 1.1897726\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:41epoch=165; Loss Pred=1.1876; Val Loss=1.1898; Val Acc=0.6711; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6683; Entropy={'forw': '8.9654'}; Entropy_Test=\n",
      "\n",
      "1 1.1850877 1.1872315\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:42epoch=166; Loss Pred=1.1864; Val Loss=1.1872; Val Acc=0.6708; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6671; Entropy={'forw': '8.9654'}; Entropy_Test=\n",
      "\n",
      "2 1.1850877 1.191169\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:42epoch=167; Loss Pred=1.1868; Val Loss=1.1912; Val Acc=0.6710; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6677; Entropy={'forw': '8.9868'}; Entropy_Test=\n",
      "\n",
      "3 1.1850877 1.1859791\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:43epoch=168; Loss Pred=1.1849; Val Loss=1.1860; Val Acc=0.6694; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6688; Entropy={'forw': '8.9868'}; Entropy_Test=\n",
      "\n",
      "4 1.1850877 1.1858273\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:44epoch=169; Loss Pred=1.1850; Val Loss=1.1858; Val Acc=0.6698; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6696; Entropy={'forw': '8.9978'}; Entropy_Test=\n",
      "\n",
      "5 1.1850877 1.1881328\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:45epoch=170; Loss Pred=1.1837; Val Loss=1.1881; Val Acc=0.6693; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6703; Entropy={'forw': '8.9978'}; Entropy_Test=\n",
      "\n",
      "6 1.1850877 1.1854037\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:46epoch=171; Loss Pred=1.1839; Val Loss=1.1854; Val Acc=0.6714; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6691; Entropy={'forw': '9.0184'}; Entropy_Test=\n",
      "\n",
      "7 1.1850877 1.183512\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:46epoch=172; Loss Pred=1.1835; Val Loss=1.1835; Val Acc=0.6706; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6673; Entropy={'forw': '9.0184'}; Entropy_Test=\n",
      "\n",
      "0 1.183512 1.1794456\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:47epoch=173; Loss Pred=1.1835; Val Loss=1.1794; Val Acc=0.6709; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6661; Entropy={'forw': '9.0205'}; Entropy_Test=\n",
      "\n",
      "0 1.1794456 1.1841139\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:48epoch=174; Loss Pred=1.1835; Val Loss=1.1841; Val Acc=0.6722; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6682; Entropy={'forw': '9.0205'}; Entropy_Test=\n",
      "\n",
      "1 1.1794456 1.1824094\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:49epoch=175; Loss Pred=1.1832; Val Loss=1.1824; Val Acc=0.6698; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6668; Entropy={'forw': '9.0272'}; Entropy_Test=\n",
      "\n",
      "2 1.1794456 1.1795434\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:49epoch=176; Loss Pred=1.1821; Val Loss=1.1795; Val Acc=0.6716; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6687; Entropy={'forw': '9.0272'}; Entropy_Test=\n",
      "\n",
      "3 1.1794456 1.1837298\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:50epoch=177; Loss Pred=1.1820; Val Loss=1.1837; Val Acc=0.6714; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6691; Entropy={'forw': '9.0302'}; Entropy_Test=\n",
      "\n",
      "4 1.1794456 1.1844543\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:51epoch=178; Loss Pred=1.1807; Val Loss=1.1845; Val Acc=0.6701; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6675; Entropy={'forw': '9.0302'}; Entropy_Test=\n",
      "\n",
      "5 1.1794456 1.1797546\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:52epoch=179; Loss Pred=1.1794; Val Loss=1.1798; Val Acc=0.6696; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6698; Entropy={'forw': '9.0356'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1.1794456 1.1849756\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:52epoch=180; Loss Pred=1.1820; Val Loss=1.1850; Val Acc=0.6717; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6691; Entropy={'forw': '9.0356'}; Entropy_Test=\n",
      "\n",
      "7 1.1794456 1.1793892\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:53epoch=181; Loss Pred=1.1811; Val Loss=1.1794; Val Acc=0.6727; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6694; Entropy={'forw': '9.0398'}; Entropy_Test=\n",
      "\n",
      "0 1.1793892 1.1803254\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:54epoch=182; Loss Pred=1.1800; Val Loss=1.1803; Val Acc=0.6724; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6677; Entropy={'forw': '9.0398'}; Entropy_Test=\n",
      "\n",
      "1 1.1793892 1.1837202\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:55epoch=183; Loss Pred=1.1801; Val Loss=1.1837; Val Acc=0.6710; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6686; Entropy={'forw': '9.0542'}; Entropy_Test=\n",
      "\n",
      "2 1.1793892 1.1805818\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:56epoch=184; Loss Pred=1.1793; Val Loss=1.1806; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6679; Entropy={'forw': '9.0542'}; Entropy_Test=\n",
      "\n",
      "3 1.1793892 1.1828983\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:56epoch=185; Loss Pred=1.1773; Val Loss=1.1829; Val Acc=0.6711; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6690; Entropy={'forw': '9.0448'}; Entropy_Test=\n",
      "\n",
      "4 1.1793892 1.176886\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:57epoch=186; Loss Pred=1.1773; Val Loss=1.1769; Val Acc=0.6709; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6684; Entropy={'forw': '9.0448'}; Entropy_Test=\n",
      "\n",
      "0 1.176886 1.1790577\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:58epoch=187; Loss Pred=1.1779; Val Loss=1.1791; Val Acc=0.6735; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6688; Entropy={'forw': '9.0502'}; Entropy_Test=\n",
      "\n",
      "1 1.176886 1.1781546\n",
      "Logged Successfully: \n",
      "2018-05-17 19:09:59epoch=188; Loss Pred=1.1773; Val Loss=1.1782; Val Acc=0.6710; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6700; Entropy={'forw': '9.0502'}; Entropy_Test=\n",
      "\n",
      "2 1.176886 1.1776922\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:00epoch=189; Loss Pred=1.1765; Val Loss=1.1777; Val Acc=0.6731; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6711; Entropy={'forw': '9.0285'}; Entropy_Test=\n",
      "\n",
      "3 1.176886 1.1799664\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:00epoch=190; Loss Pred=1.1767; Val Loss=1.1800; Val Acc=0.6721; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6694; Entropy={'forw': '9.0285'}; Entropy_Test=\n",
      "\n",
      "4 1.176886 1.1736828\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:01epoch=191; Loss Pred=1.1762; Val Loss=1.1737; Val Acc=0.6733; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6695; Entropy={'forw': '9.0281'}; Entropy_Test=\n",
      "\n",
      "0 1.1736828 1.1748084\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:02epoch=192; Loss Pred=1.1761; Val Loss=1.1748; Val Acc=0.6727; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6690; Entropy={'forw': '9.0281'}; Entropy_Test=\n",
      "\n",
      "1 1.1736828 1.1783062\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:03epoch=193; Loss Pred=1.1748; Val Loss=1.1783; Val Acc=0.6711; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6693; Entropy={'forw': '9.0260'}; Entropy_Test=\n",
      "\n",
      "2 1.1736828 1.1772813\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:03epoch=194; Loss Pred=1.1752; Val Loss=1.1773; Val Acc=0.6714; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6685; Entropy={'forw': '9.0260'}; Entropy_Test=\n",
      "\n",
      "3 1.1736828 1.1751778\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:04epoch=195; Loss Pred=1.1740; Val Loss=1.1752; Val Acc=0.6722; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6720; Entropy={'forw': '9.0353'}; Entropy_Test=\n",
      "\n",
      "4 1.1736828 1.1752527\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:05epoch=196; Loss Pred=1.1745; Val Loss=1.1753; Val Acc=0.6746; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6677; Entropy={'forw': '9.0353'}; Entropy_Test=\n",
      "\n",
      "5 1.1736828 1.1749439\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:06epoch=197; Loss Pred=1.1741; Val Loss=1.1749; Val Acc=0.6733; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6706; Entropy={'forw': '9.0694'}; Entropy_Test=\n",
      "\n",
      "6 1.1736828 1.1759043\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:06epoch=198; Loss Pred=1.1734; Val Loss=1.1759; Val Acc=0.6713; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6713; Entropy={'forw': '9.0694'}; Entropy_Test=\n",
      "\n",
      "7 1.1736828 1.1720204\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:07epoch=199; Loss Pred=1.1725; Val Loss=1.1720; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6683; Entropy={'forw': '9.0733'}; Entropy_Test=\n",
      "\n",
      "0 1.1720204 1.1732693\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:08epoch=200; Loss Pred=1.1718; Val Loss=1.1733; Val Acc=0.6725; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6691; Entropy={'forw': '9.0733'}; Entropy_Test=\n",
      "\n",
      "1 1.1720204 1.1727747\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:09epoch=201; Loss Pred=1.1705; Val Loss=1.1728; Val Acc=0.6728; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6716; Entropy={'forw': '9.0896'}; Entropy_Test=\n",
      "\n",
      "2 1.1720204 1.1692706\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:10epoch=202; Loss Pred=1.1724; Val Loss=1.1693; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6713; Entropy={'forw': '9.0896'}; Entropy_Test=\n",
      "\n",
      "0 1.1692706 1.1721587\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:10epoch=203; Loss Pred=1.1731; Val Loss=1.1722; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6693; Entropy={'forw': '9.0847'}; Entropy_Test=\n",
      "\n",
      "1 1.1692706 1.1714773\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:11epoch=204; Loss Pred=1.1728; Val Loss=1.1715; Val Acc=0.6729; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6720; Entropy={'forw': '9.0847'}; Entropy_Test=\n",
      "\n",
      "2 1.1692706 1.1722493\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:12epoch=205; Loss Pred=1.1696; Val Loss=1.1722; Val Acc=0.6735; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6702; Entropy={'forw': '9.0945'}; Entropy_Test=\n",
      "\n",
      "3 1.1692706 1.1756846\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:13epoch=206; Loss Pred=1.1705; Val Loss=1.1757; Val Acc=0.6715; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6697; Entropy={'forw': '9.0945'}; Entropy_Test=\n",
      "\n",
      "4 1.1692706 1.173691\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:13epoch=207; Loss Pred=1.1701; Val Loss=1.1737; Val Acc=0.6731; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6702; Entropy={'forw': '9.0854'}; Entropy_Test=\n",
      "\n",
      "5 1.1692706 1.1728065\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:14epoch=208; Loss Pred=1.1696; Val Loss=1.1728; Val Acc=0.6725; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6695; Entropy={'forw': '9.0854'}; Entropy_Test=\n",
      "\n",
      "6 1.1692706 1.1696244\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:15epoch=209; Loss Pred=1.1690; Val Loss=1.1696; Val Acc=0.6753; Loss Att={'forw': '1.0000'}; Train Acc=0.673; Test Acc=0.6715; Entropy={'forw': '9.1104'}; Entropy_Test=\n",
      "\n",
      "7 1.1692706 1.1685481\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:16epoch=210; Loss Pred=1.1693; Val Loss=1.1685; Val Acc=0.6725; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6725; Entropy={'forw': '9.1104'}; Entropy_Test=\n",
      "\n",
      "0 1.1685481 1.1703461\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:16epoch=211; Loss Pred=1.1688; Val Loss=1.1703; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6687; Entropy={'forw': '9.1525'}; Entropy_Test=\n",
      "\n",
      "1 1.1685481 1.1719072\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:17epoch=212; Loss Pred=1.1685; Val Loss=1.1719; Val Acc=0.6737; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6715; Entropy={'forw': '9.1525'}; Entropy_Test=\n",
      "\n",
      "2 1.1685481 1.1684029\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:18epoch=213; Loss Pred=1.1677; Val Loss=1.1684; Val Acc=0.6735; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6694; Entropy={'forw': '9.1625'}; Entropy_Test=\n",
      "\n",
      "0 1.1684029 1.1674829\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:19epoch=214; Loss Pred=1.1685; Val Loss=1.1675; Val Acc=0.6718; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6703; Entropy={'forw': '9.1625'}; Entropy_Test=\n",
      "\n",
      "0 1.1674829 1.1687719\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:19epoch=215; Loss Pred=1.1652; Val Loss=1.1688; Val Acc=0.6736; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6706; Entropy={'forw': '9.1531'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.1674829 1.1677581\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:20epoch=216; Loss Pred=1.1678; Val Loss=1.1678; Val Acc=0.6751; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6690; Entropy={'forw': '9.1531'}; Entropy_Test=\n",
      "\n",
      "2 1.1674829 1.1662028\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:21epoch=217; Loss Pred=1.1648; Val Loss=1.1662; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6717; Entropy={'forw': '9.1628'}; Entropy_Test=\n",
      "\n",
      "0 1.1662028 1.1686077\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:22epoch=218; Loss Pred=1.1671; Val Loss=1.1686; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6697; Entropy={'forw': '9.1628'}; Entropy_Test=\n",
      "\n",
      "1 1.1662028 1.1639657\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:23epoch=219; Loss Pred=1.1660; Val Loss=1.1640; Val Acc=0.6711; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6691; Entropy={'forw': '9.1512'}; Entropy_Test=\n",
      "\n",
      "0 1.1639657 1.1677965\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:23epoch=220; Loss Pred=1.1657; Val Loss=1.1678; Val Acc=0.6752; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6714; Entropy={'forw': '9.1512'}; Entropy_Test=\n",
      "\n",
      "1 1.1639657 1.1646528\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:24epoch=221; Loss Pred=1.1635; Val Loss=1.1647; Val Acc=0.6722; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6733; Entropy={'forw': '9.1709'}; Entropy_Test=\n",
      "\n",
      "2 1.1639657 1.1658326\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:25epoch=222; Loss Pred=1.1656; Val Loss=1.1658; Val Acc=0.6718; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6691; Entropy={'forw': '9.1709'}; Entropy_Test=\n",
      "\n",
      "3 1.1639657 1.1663064\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:26epoch=223; Loss Pred=1.1651; Val Loss=1.1663; Val Acc=0.6726; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6719; Entropy={'forw': '9.1889'}; Entropy_Test=\n",
      "\n",
      "4 1.1639657 1.1686369\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:26epoch=224; Loss Pred=1.1653; Val Loss=1.1686; Val Acc=0.6725; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6720; Entropy={'forw': '9.1889'}; Entropy_Test=\n",
      "\n",
      "5 1.1639657 1.1651294\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:27epoch=225; Loss Pred=1.1620; Val Loss=1.1651; Val Acc=0.6730; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6693; Entropy={'forw': '9.1969'}; Entropy_Test=\n",
      "\n",
      "6 1.1639657 1.1685098\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:28epoch=226; Loss Pred=1.1645; Val Loss=1.1685; Val Acc=0.6717; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6711; Entropy={'forw': '9.1969'}; Entropy_Test=\n",
      "\n",
      "7 1.1639657 1.167017\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:29epoch=227; Loss Pred=1.1631; Val Loss=1.1670; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6675; Entropy={'forw': '9.2035'}; Entropy_Test=\n",
      "\n",
      "8 1.1639657 1.1650994\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:29epoch=228; Loss Pred=1.1616; Val Loss=1.1651; Val Acc=0.6721; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6720; Entropy={'forw': '9.2035'}; Entropy_Test=\n",
      "\n",
      "9 1.1639657 1.1627954\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:30epoch=229; Loss Pred=1.1624; Val Loss=1.1628; Val Acc=0.6742; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6717; Entropy={'forw': '9.2097'}; Entropy_Test=\n",
      "\n",
      "0 1.1627954 1.1663908\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:31epoch=230; Loss Pred=1.1618; Val Loss=1.1664; Val Acc=0.6741; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6690; Entropy={'forw': '9.2097'}; Entropy_Test=\n",
      "\n",
      "1 1.1627954 1.1647261\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:32epoch=231; Loss Pred=1.1614; Val Loss=1.1647; Val Acc=0.6753; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6699; Entropy={'forw': '9.2069'}; Entropy_Test=\n",
      "\n",
      "2 1.1627954 1.1642199\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:32epoch=232; Loss Pred=1.1612; Val Loss=1.1642; Val Acc=0.6726; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6699; Entropy={'forw': '9.2069'}; Entropy_Test=\n",
      "\n",
      "3 1.1627954 1.1640471\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:33epoch=233; Loss Pred=1.1613; Val Loss=1.1640; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6695; Entropy={'forw': '9.2242'}; Entropy_Test=\n",
      "\n",
      "4 1.1627954 1.1651791\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:34epoch=234; Loss Pred=1.1596; Val Loss=1.1652; Val Acc=0.6745; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6701; Entropy={'forw': '9.2242'}; Entropy_Test=\n",
      "\n",
      "5 1.1627954 1.1612363\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:35epoch=235; Loss Pred=1.1623; Val Loss=1.1612; Val Acc=0.6721; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6704; Entropy={'forw': '9.2226'}; Entropy_Test=\n",
      "\n",
      "0 1.1612363 1.1578441\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:35epoch=236; Loss Pred=1.1604; Val Loss=1.1578; Val Acc=0.6733; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6703; Entropy={'forw': '9.2226'}; Entropy_Test=\n",
      "\n",
      "0 1.1578441 1.1606731\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:36epoch=237; Loss Pred=1.1608; Val Loss=1.1607; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6695; Entropy={'forw': '9.2254'}; Entropy_Test=\n",
      "\n",
      "1 1.1578441 1.1652857\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:37epoch=238; Loss Pred=1.1606; Val Loss=1.1653; Val Acc=0.6714; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6728; Entropy={'forw': '9.2254'}; Entropy_Test=\n",
      "\n",
      "2 1.1578441 1.1590046\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:38epoch=239; Loss Pred=1.1592; Val Loss=1.1590; Val Acc=0.6731; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6711; Entropy={'forw': '9.2159'}; Entropy_Test=\n",
      "\n",
      "3 1.1578441 1.1634369\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:38epoch=240; Loss Pred=1.1583; Val Loss=1.1634; Val Acc=0.6726; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6691; Entropy={'forw': '9.2159'}; Entropy_Test=\n",
      "\n",
      "4 1.1578441 1.1652997\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:39epoch=241; Loss Pred=1.1592; Val Loss=1.1653; Val Acc=0.6739; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6719; Entropy={'forw': '9.2164'}; Entropy_Test=\n",
      "\n",
      "5 1.1578441 1.1651995\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:40epoch=242; Loss Pred=1.1602; Val Loss=1.1652; Val Acc=0.6745; Loss Att={'forw': '1.0000'}; Train Acc=0.674; Test Acc=0.6705; Entropy={'forw': '9.2164'}; Entropy_Test=\n",
      "\n",
      "6 1.1578441 1.1644278\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:41epoch=243; Loss Pred=1.1576; Val Loss=1.1644; Val Acc=0.6745; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6697; Entropy={'forw': '9.2200'}; Entropy_Test=\n",
      "\n",
      "7 1.1578441 1.1615585\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:41epoch=244; Loss Pred=1.1576; Val Loss=1.1616; Val Acc=0.6736; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6709; Entropy={'forw': '9.2200'}; Entropy_Test=\n",
      "\n",
      "8 1.1578441 1.1608957\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:42epoch=245; Loss Pred=1.1576; Val Loss=1.1609; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6721; Entropy={'forw': '9.2259'}; Entropy_Test=\n",
      "\n",
      "9 1.1578441 1.1649327\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:43epoch=246; Loss Pred=1.1568; Val Loss=1.1649; Val Acc=0.6738; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6706; Entropy={'forw': '9.2259'}; Entropy_Test=\n",
      "\n",
      "10 1.1578441 1.1592166\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:44epoch=247; Loss Pred=1.1581; Val Loss=1.1592; Val Acc=0.6767; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6701; Entropy={'forw': '9.2309'}; Entropy_Test=\n",
      "\n",
      "11 1.1578441 1.1604816\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:44epoch=248; Loss Pred=1.1586; Val Loss=1.1605; Val Acc=0.6735; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6721; Entropy={'forw': '9.2309'}; Entropy_Test=\n",
      "\n",
      "12 1.1578441 1.1572131\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:45epoch=249; Loss Pred=1.1552; Val Loss=1.1572; Val Acc=0.6737; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6714; Entropy={'forw': '9.2317'}; Entropy_Test=\n",
      "\n",
      "0 1.1572131 1.1605705\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:46epoch=250; Loss Pred=1.1573; Val Loss=1.1606; Val Acc=0.6720; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6711; Entropy={'forw': '9.2317'}; Entropy_Test=\n",
      "\n",
      "1 1.1572131 1.1596043\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:47epoch=251; Loss Pred=1.1568; Val Loss=1.1596; Val Acc=0.6733; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6701; Entropy={'forw': '9.2352'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1.1572131 1.1589288\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:47epoch=252; Loss Pred=1.1554; Val Loss=1.1589; Val Acc=0.6749; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6700; Entropy={'forw': '9.2352'}; Entropy_Test=\n",
      "\n",
      "3 1.1572131 1.1577281\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:48epoch=253; Loss Pred=1.1578; Val Loss=1.1577; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6697; Entropy={'forw': '9.2528'}; Entropy_Test=\n",
      "\n",
      "4 1.1572131 1.162537\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:49epoch=254; Loss Pred=1.1562; Val Loss=1.1625; Val Acc=0.6723; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6712; Entropy={'forw': '9.2528'}; Entropy_Test=\n",
      "\n",
      "5 1.1572131 1.1568966\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:50epoch=255; Loss Pred=1.1551; Val Loss=1.1569; Val Acc=0.6746; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6711; Entropy={'forw': '9.2605'}; Entropy_Test=\n",
      "\n",
      "0 1.1568966 1.1582212\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:50epoch=256; Loss Pred=1.1552; Val Loss=1.1582; Val Acc=0.6745; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6728; Entropy={'forw': '9.2605'}; Entropy_Test=\n",
      "\n",
      "1 1.1568966 1.1568629\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:51epoch=257; Loss Pred=1.1538; Val Loss=1.1569; Val Acc=0.6727; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6710; Entropy={'forw': '9.2612'}; Entropy_Test=\n",
      "\n",
      "0 1.1568629 1.1590563\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:52epoch=258; Loss Pred=1.1560; Val Loss=1.1591; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6706; Entropy={'forw': '9.2612'}; Entropy_Test=\n",
      "\n",
      "1 1.1568629 1.158737\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:53epoch=259; Loss Pred=1.1537; Val Loss=1.1587; Val Acc=0.6742; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6718; Entropy={'forw': '9.2637'}; Entropy_Test=\n",
      "\n",
      "2 1.1568629 1.1578104\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:53epoch=260; Loss Pred=1.1537; Val Loss=1.1578; Val Acc=0.6748; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6706; Entropy={'forw': '9.2637'}; Entropy_Test=\n",
      "\n",
      "3 1.1568629 1.1599792\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:54epoch=261; Loss Pred=1.1560; Val Loss=1.1600; Val Acc=0.6727; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6714; Entropy={'forw': '9.2720'}; Entropy_Test=\n",
      "\n",
      "4 1.1568629 1.1588327\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:55epoch=262; Loss Pred=1.1537; Val Loss=1.1588; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6698; Entropy={'forw': '9.2720'}; Entropy_Test=\n",
      "\n",
      "5 1.1568629 1.159996\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:56epoch=263; Loss Pred=1.1527; Val Loss=1.1600; Val Acc=0.6751; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6706; Entropy={'forw': '9.2794'}; Entropy_Test=\n",
      "\n",
      "6 1.1568629 1.157463\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:56epoch=264; Loss Pred=1.1514; Val Loss=1.1575; Val Acc=0.6748; Loss Att={'forw': '1.0000'}; Train Acc=0.675; Test Acc=0.6729; Entropy={'forw': '9.2794'}; Entropy_Test=\n",
      "\n",
      "7 1.1568629 1.1594163\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:57epoch=265; Loss Pred=1.1534; Val Loss=1.1594; Val Acc=0.6730; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6721; Entropy={'forw': '9.2866'}; Entropy_Test=\n",
      "\n",
      "8 1.1568629 1.1615661\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:58epoch=266; Loss Pred=1.1518; Val Loss=1.1616; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6722; Entropy={'forw': '9.2866'}; Entropy_Test=\n",
      "\n",
      "9 1.1568629 1.1593108\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:59epoch=267; Loss Pred=1.1518; Val Loss=1.1593; Val Acc=0.6725; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6712; Entropy={'forw': '9.2832'}; Entropy_Test=\n",
      "\n",
      "10 1.1568629 1.1579922\n",
      "Logged Successfully: \n",
      "2018-05-17 19:10:59epoch=268; Loss Pred=1.1510; Val Loss=1.1580; Val Acc=0.6764; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6706; Entropy={'forw': '9.2832'}; Entropy_Test=\n",
      "\n",
      "11 1.1568629 1.1589873\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:00epoch=269; Loss Pred=1.1516; Val Loss=1.1590; Val Acc=0.6725; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6708; Entropy={'forw': '9.2855'}; Entropy_Test=\n",
      "\n",
      "12 1.1568629 1.158398\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:01epoch=270; Loss Pred=1.1519; Val Loss=1.1584; Val Acc=0.6738; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6729; Entropy={'forw': '9.2855'}; Entropy_Test=\n",
      "\n",
      "13 1.1568629 1.1541826\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:02epoch=271; Loss Pred=1.1516; Val Loss=1.1542; Val Acc=0.6736; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6716; Entropy={'forw': '9.2893'}; Entropy_Test=\n",
      "\n",
      "0 1.1541826 1.1558412\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:02epoch=272; Loss Pred=1.1502; Val Loss=1.1558; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.676; Test Acc=0.6704; Entropy={'forw': '9.2893'}; Entropy_Test=\n",
      "\n",
      "1 1.1541826 1.1565753\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:03epoch=273; Loss Pred=1.1508; Val Loss=1.1566; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6694; Entropy={'forw': '9.2774'}; Entropy_Test=\n",
      "\n",
      "2 1.1541826 1.1577597\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:04epoch=274; Loss Pred=1.1490; Val Loss=1.1578; Val Acc=0.6751; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6718; Entropy={'forw': '9.2774'}; Entropy_Test=\n",
      "\n",
      "3 1.1541826 1.1553653\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:05epoch=275; Loss Pred=1.1504; Val Loss=1.1554; Val Acc=0.6751; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6705; Entropy={'forw': '9.2755'}; Entropy_Test=\n",
      "\n",
      "4 1.1541826 1.1592009\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:05epoch=276; Loss Pred=1.1496; Val Loss=1.1592; Val Acc=0.6746; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6729; Entropy={'forw': '9.2755'}; Entropy_Test=\n",
      "\n",
      "5 1.1541826 1.1570382\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:06epoch=277; Loss Pred=1.1505; Val Loss=1.1570; Val Acc=0.6738; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6724; Entropy={'forw': '9.2790'}; Entropy_Test=\n",
      "\n",
      "6 1.1541826 1.1543479\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:07epoch=278; Loss Pred=1.1499; Val Loss=1.1543; Val Acc=0.6726; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6728; Entropy={'forw': '9.2790'}; Entropy_Test=\n",
      "\n",
      "7 1.1541826 1.1556814\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:08epoch=279; Loss Pred=1.1505; Val Loss=1.1557; Val Acc=0.6746; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6715; Entropy={'forw': '9.2769'}; Entropy_Test=\n",
      "\n",
      "8 1.1541826 1.1545794\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:08epoch=280; Loss Pred=1.1493; Val Loss=1.1546; Val Acc=0.6746; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6715; Entropy={'forw': '9.2769'}; Entropy_Test=\n",
      "\n",
      "9 1.1541826 1.1570034\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:09epoch=281; Loss Pred=1.1502; Val Loss=1.1570; Val Acc=0.6720; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6692; Entropy={'forw': '9.2877'}; Entropy_Test=\n",
      "\n",
      "10 1.1541826 1.1545944\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:10epoch=282; Loss Pred=1.1491; Val Loss=1.1546; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6730; Entropy={'forw': '9.2877'}; Entropy_Test=\n",
      "\n",
      "11 1.1541826 1.1528438\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:11epoch=283; Loss Pred=1.1485; Val Loss=1.1528; Val Acc=0.6751; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6751; Entropy={'forw': '9.2828'}; Entropy_Test=\n",
      "\n",
      "0 1.1528438 1.1531056\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:11epoch=284; Loss Pred=1.1482; Val Loss=1.1531; Val Acc=0.6741; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6700; Entropy={'forw': '9.2828'}; Entropy_Test=\n",
      "\n",
      "1 1.1528438 1.1564026\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:12epoch=285; Loss Pred=1.1475; Val Loss=1.1564; Val Acc=0.6761; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6737; Entropy={'forw': '9.2933'}; Entropy_Test=\n",
      "\n",
      "2 1.1528438 1.1532297\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:13epoch=286; Loss Pred=1.1489; Val Loss=1.1532; Val Acc=0.6751; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6720; Entropy={'forw': '9.2933'}; Entropy_Test=\n",
      "\n",
      "3 1.1528438 1.1507319\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:14epoch=287; Loss Pred=1.1469; Val Loss=1.1507; Val Acc=0.6737; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6725; Entropy={'forw': '9.2895'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1507319 1.1535752\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:14epoch=288; Loss Pred=1.1472; Val Loss=1.1536; Val Acc=0.6762; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6730; Entropy={'forw': '9.2895'}; Entropy_Test=\n",
      "\n",
      "1 1.1507319 1.1562792\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:15epoch=289; Loss Pred=1.1485; Val Loss=1.1563; Val Acc=0.6765; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6720; Entropy={'forw': '9.2962'}; Entropy_Test=\n",
      "\n",
      "2 1.1507319 1.1561946\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:16epoch=290; Loss Pred=1.1477; Val Loss=1.1562; Val Acc=0.6737; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6709; Entropy={'forw': '9.2962'}; Entropy_Test=\n",
      "\n",
      "3 1.1507319 1.1541377\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:17epoch=291; Loss Pred=1.1474; Val Loss=1.1541; Val Acc=0.6765; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6732; Entropy={'forw': '9.2875'}; Entropy_Test=\n",
      "\n",
      "4 1.1507319 1.1526461\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:17epoch=292; Loss Pred=1.1477; Val Loss=1.1526; Val Acc=0.6741; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6723; Entropy={'forw': '9.2875'}; Entropy_Test=\n",
      "\n",
      "5 1.1507319 1.1539087\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:18epoch=293; Loss Pred=1.1468; Val Loss=1.1539; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6727; Entropy={'forw': '9.3046'}; Entropy_Test=\n",
      "\n",
      "6 1.1507319 1.1506621\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:19epoch=294; Loss Pred=1.1453; Val Loss=1.1507; Val Acc=0.6733; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6732; Entropy={'forw': '9.3046'}; Entropy_Test=\n",
      "\n",
      "0 1.1506621 1.1540661\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:20epoch=295; Loss Pred=1.1458; Val Loss=1.1541; Val Acc=0.6750; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6719; Entropy={'forw': '9.3067'}; Entropy_Test=\n",
      "\n",
      "1 1.1506621 1.1495862\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:20epoch=296; Loss Pred=1.1460; Val Loss=1.1496; Val Acc=0.6770; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6712; Entropy={'forw': '9.3067'}; Entropy_Test=\n",
      "\n",
      "0 1.1495862 1.1560732\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:21epoch=297; Loss Pred=1.1450; Val Loss=1.1561; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6731; Entropy={'forw': '9.3040'}; Entropy_Test=\n",
      "\n",
      "1 1.1495862 1.1545379\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:22epoch=298; Loss Pred=1.1452; Val Loss=1.1545; Val Acc=0.6747; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6733; Entropy={'forw': '9.3040'}; Entropy_Test=\n",
      "\n",
      "2 1.1495862 1.1521059\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:23epoch=299; Loss Pred=1.1442; Val Loss=1.1521; Val Acc=0.6768; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6735; Entropy={'forw': '9.3054'}; Entropy_Test=\n",
      "\n",
      "3 1.1495862 1.1516497\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:23epoch=300; Loss Pred=1.1448; Val Loss=1.1516; Val Acc=0.6739; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6699; Entropy={'forw': '9.3054'}; Entropy_Test=\n",
      "\n",
      "4 1.1495862 1.1509713\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:24epoch=301; Loss Pred=1.1442; Val Loss=1.1510; Val Acc=0.6753; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6722; Entropy={'forw': '9.3083'}; Entropy_Test=\n",
      "\n",
      "5 1.1495862 1.1489012\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:25epoch=302; Loss Pred=1.1438; Val Loss=1.1489; Val Acc=0.6770; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6723; Entropy={'forw': '9.3083'}; Entropy_Test=\n",
      "\n",
      "0 1.1489012 1.1520365\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:26epoch=303; Loss Pred=1.1445; Val Loss=1.1520; Val Acc=0.6753; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6718; Entropy={'forw': '9.3177'}; Entropy_Test=\n",
      "\n",
      "1 1.1489012 1.1514814\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:26epoch=304; Loss Pred=1.1458; Val Loss=1.1515; Val Acc=0.6765; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6731; Entropy={'forw': '9.3177'}; Entropy_Test=\n",
      "\n",
      "2 1.1489012 1.1496147\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:27epoch=305; Loss Pred=1.1424; Val Loss=1.1496; Val Acc=0.6764; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6731; Entropy={'forw': '9.3148'}; Entropy_Test=\n",
      "\n",
      "3 1.1489012 1.1502177\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:28epoch=306; Loss Pred=1.1429; Val Loss=1.1502; Val Acc=0.6758; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6712; Entropy={'forw': '9.3148'}; Entropy_Test=\n",
      "\n",
      "4 1.1489012 1.150596\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:29epoch=307; Loss Pred=1.1432; Val Loss=1.1506; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6734; Entropy={'forw': '9.3152'}; Entropy_Test=\n",
      "\n",
      "5 1.1489012 1.1523538\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:29epoch=308; Loss Pred=1.1422; Val Loss=1.1524; Val Acc=0.6744; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6712; Entropy={'forw': '9.3152'}; Entropy_Test=\n",
      "\n",
      "6 1.1489012 1.1509534\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:30epoch=309; Loss Pred=1.1427; Val Loss=1.1510; Val Acc=0.6736; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6725; Entropy={'forw': '9.3168'}; Entropy_Test=\n",
      "\n",
      "7 1.1489012 1.1481512\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:31epoch=310; Loss Pred=1.1442; Val Loss=1.1482; Val Acc=0.6763; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6750; Entropy={'forw': '9.3168'}; Entropy_Test=\n",
      "\n",
      "0 1.1481512 1.1527269\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:32epoch=311; Loss Pred=1.1425; Val Loss=1.1527; Val Acc=0.6755; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6723; Entropy={'forw': '9.3116'}; Entropy_Test=\n",
      "\n",
      "1 1.1481512 1.1489224\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:32epoch=312; Loss Pred=1.1427; Val Loss=1.1489; Val Acc=0.6741; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6716; Entropy={'forw': '9.3116'}; Entropy_Test=\n",
      "\n",
      "2 1.1481512 1.1506377\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:33epoch=313; Loss Pred=1.1415; Val Loss=1.1506; Val Acc=0.6736; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6730; Entropy={'forw': '9.3149'}; Entropy_Test=\n",
      "\n",
      "3 1.1481512 1.152928\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:34epoch=314; Loss Pred=1.1419; Val Loss=1.1529; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6750; Entropy={'forw': '9.3149'}; Entropy_Test=\n",
      "\n",
      "4 1.1481512 1.1485156\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:35epoch=315; Loss Pred=1.1417; Val Loss=1.1485; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6730; Entropy={'forw': '9.3229'}; Entropy_Test=\n",
      "\n",
      "5 1.1481512 1.1509084\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:35epoch=316; Loss Pred=1.1408; Val Loss=1.1509; Val Acc=0.6755; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6733; Entropy={'forw': '9.3229'}; Entropy_Test=\n",
      "\n",
      "6 1.1481512 1.1507213\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:36epoch=317; Loss Pred=1.1405; Val Loss=1.1507; Val Acc=0.6737; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6755; Entropy={'forw': '9.3233'}; Entropy_Test=\n",
      "\n",
      "7 1.1481512 1.1510956\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:37epoch=318; Loss Pred=1.1399; Val Loss=1.1511; Val Acc=0.6747; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6757; Entropy={'forw': '9.3233'}; Entropy_Test=\n",
      "\n",
      "8 1.1481512 1.150033\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:38epoch=319; Loss Pred=1.1416; Val Loss=1.1500; Val Acc=0.6761; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6728; Entropy={'forw': '9.3220'}; Entropy_Test=\n",
      "\n",
      "9 1.1481512 1.1473475\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:38epoch=320; Loss Pred=1.1424; Val Loss=1.1473; Val Acc=0.6741; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6729; Entropy={'forw': '9.3220'}; Entropy_Test=\n",
      "\n",
      "0 1.1473475 1.148673\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:39epoch=321; Loss Pred=1.1400; Val Loss=1.1487; Val Acc=0.6758; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6718; Entropy={'forw': '9.3287'}; Entropy_Test=\n",
      "\n",
      "1 1.1473475 1.1469462\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:40epoch=322; Loss Pred=1.1418; Val Loss=1.1469; Val Acc=0.6745; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6717; Entropy={'forw': '9.3287'}; Entropy_Test=\n",
      "\n",
      "0 1.1469462 1.1501122\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:40epoch=323; Loss Pred=1.1408; Val Loss=1.1501; Val Acc=0.6741; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6745; Entropy={'forw': '9.3220'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.1469462 1.1499126\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:41epoch=324; Loss Pred=1.1409; Val Loss=1.1499; Val Acc=0.6760; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6725; Entropy={'forw': '9.3220'}; Entropy_Test=\n",
      "\n",
      "2 1.1469462 1.1486777\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:42epoch=325; Loss Pred=1.1396; Val Loss=1.1487; Val Acc=0.6752; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6728; Entropy={'forw': '9.3126'}; Entropy_Test=\n",
      "\n",
      "3 1.1469462 1.1527519\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:43epoch=326; Loss Pred=1.1381; Val Loss=1.1528; Val Acc=0.6721; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6728; Entropy={'forw': '9.3126'}; Entropy_Test=\n",
      "\n",
      "4 1.1469462 1.1518779\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:43epoch=327; Loss Pred=1.1382; Val Loss=1.1519; Val Acc=0.6758; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6749; Entropy={'forw': '9.3317'}; Entropy_Test=\n",
      "\n",
      "5 1.1469462 1.1477429\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:44epoch=328; Loss Pred=1.1384; Val Loss=1.1477; Val Acc=0.6764; Loss Att={'forw': '1.0000'}; Train Acc=0.677; Test Acc=0.6723; Entropy={'forw': '9.3317'}; Entropy_Test=\n",
      "\n",
      "6 1.1469462 1.1507212\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:45epoch=329; Loss Pred=1.1379; Val Loss=1.1507; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6721; Entropy={'forw': '9.3330'}; Entropy_Test=\n",
      "\n",
      "7 1.1469462 1.1475224\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:46epoch=330; Loss Pred=1.1390; Val Loss=1.1475; Val Acc=0.6761; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6723; Entropy={'forw': '9.3330'}; Entropy_Test=\n",
      "\n",
      "8 1.1469462 1.1469386\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:46epoch=331; Loss Pred=1.1381; Val Loss=1.1469; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6730; Entropy={'forw': '9.3438'}; Entropy_Test=\n",
      "\n",
      "0 1.1469386 1.1487896\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:47epoch=332; Loss Pred=1.1382; Val Loss=1.1488; Val Acc=0.6765; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6713; Entropy={'forw': '9.3438'}; Entropy_Test=\n",
      "\n",
      "1 1.1469386 1.1478249\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:48epoch=333; Loss Pred=1.1377; Val Loss=1.1478; Val Acc=0.6754; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6718; Entropy={'forw': '9.3387'}; Entropy_Test=\n",
      "\n",
      "2 1.1469386 1.1507057\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:49epoch=334; Loss Pred=1.1386; Val Loss=1.1507; Val Acc=0.6726; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6712; Entropy={'forw': '9.3387'}; Entropy_Test=\n",
      "\n",
      "3 1.1469386 1.1528175\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:49epoch=335; Loss Pred=1.1376; Val Loss=1.1528; Val Acc=0.6730; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6745; Entropy={'forw': '9.3310'}; Entropy_Test=\n",
      "\n",
      "4 1.1469386 1.1489544\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:50epoch=336; Loss Pred=1.1376; Val Loss=1.1490; Val Acc=0.6744; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6707; Entropy={'forw': '9.3310'}; Entropy_Test=\n",
      "\n",
      "5 1.1469386 1.1510103\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:51epoch=337; Loss Pred=1.1365; Val Loss=1.1510; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6720; Entropy={'forw': '9.3308'}; Entropy_Test=\n",
      "\n",
      "6 1.1469386 1.1449548\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:52epoch=338; Loss Pred=1.1375; Val Loss=1.1450; Val Acc=0.6756; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6741; Entropy={'forw': '9.3308'}; Entropy_Test=\n",
      "\n",
      "0 1.1449548 1.1461875\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:52epoch=339; Loss Pred=1.1375; Val Loss=1.1462; Val Acc=0.6744; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6724; Entropy={'forw': '9.3474'}; Entropy_Test=\n",
      "\n",
      "1 1.1449548 1.1472921\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:53epoch=340; Loss Pred=1.1358; Val Loss=1.1473; Val Acc=0.6745; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6720; Entropy={'forw': '9.3474'}; Entropy_Test=\n",
      "\n",
      "2 1.1449548 1.146706\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:54epoch=341; Loss Pred=1.1372; Val Loss=1.1467; Val Acc=0.6744; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6727; Entropy={'forw': '9.3333'}; Entropy_Test=\n",
      "\n",
      "3 1.1449548 1.148571\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:55epoch=342; Loss Pred=1.1347; Val Loss=1.1486; Val Acc=0.6778; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6732; Entropy={'forw': '9.3333'}; Entropy_Test=\n",
      "\n",
      "4 1.1449548 1.1459067\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:55epoch=343; Loss Pred=1.1362; Val Loss=1.1459; Val Acc=0.6731; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6711; Entropy={'forw': '9.3471'}; Entropy_Test=\n",
      "\n",
      "5 1.1449548 1.147913\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:56epoch=344; Loss Pred=1.1370; Val Loss=1.1479; Val Acc=0.6749; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6724; Entropy={'forw': '9.3471'}; Entropy_Test=\n",
      "\n",
      "6 1.1449548 1.1487293\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:57epoch=345; Loss Pred=1.1360; Val Loss=1.1487; Val Acc=0.6748; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6736; Entropy={'forw': '9.3514'}; Entropy_Test=\n",
      "\n",
      "7 1.1449548 1.1479577\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:58epoch=346; Loss Pred=1.1351; Val Loss=1.1480; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6734; Entropy={'forw': '9.3514'}; Entropy_Test=\n",
      "\n",
      "8 1.1449548 1.1486229\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:58epoch=347; Loss Pred=1.1353; Val Loss=1.1486; Val Acc=0.6745; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6730; Entropy={'forw': '9.3425'}; Entropy_Test=\n",
      "\n",
      "9 1.1449548 1.1444904\n",
      "Logged Successfully: \n",
      "2018-05-17 19:11:59epoch=348; Loss Pred=1.1350; Val Loss=1.1445; Val Acc=0.6738; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6729; Entropy={'forw': '9.3425'}; Entropy_Test=\n",
      "\n",
      "0 1.1444904 1.1460193\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:00epoch=349; Loss Pred=1.1355; Val Loss=1.1460; Val Acc=0.6752; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6735; Entropy={'forw': '9.3417'}; Entropy_Test=\n",
      "\n",
      "1 1.1444904 1.1475248\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:01epoch=350; Loss Pred=1.1360; Val Loss=1.1475; Val Acc=0.6753; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6726; Entropy={'forw': '9.3417'}; Entropy_Test=\n",
      "\n",
      "2 1.1444904 1.1443557\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:01epoch=351; Loss Pred=1.1349; Val Loss=1.1444; Val Acc=0.6732; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6726; Entropy={'forw': '9.3431'}; Entropy_Test=\n",
      "\n",
      "0 1.1443557 1.1489897\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:02epoch=352; Loss Pred=1.1361; Val Loss=1.1490; Val Acc=0.6742; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6729; Entropy={'forw': '9.3431'}; Entropy_Test=\n",
      "\n",
      "1 1.1443557 1.1456486\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:03epoch=353; Loss Pred=1.1352; Val Loss=1.1456; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6713; Entropy={'forw': '9.3504'}; Entropy_Test=\n",
      "\n",
      "2 1.1443557 1.1451113\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:04epoch=354; Loss Pred=1.1330; Val Loss=1.1451; Val Acc=0.6753; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6721; Entropy={'forw': '9.3504'}; Entropy_Test=\n",
      "\n",
      "3 1.1443557 1.1474559\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:04epoch=355; Loss Pred=1.1348; Val Loss=1.1475; Val Acc=0.6765; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6738; Entropy={'forw': '9.3355'}; Entropy_Test=\n",
      "\n",
      "4 1.1443557 1.1481674\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:05epoch=356; Loss Pred=1.1340; Val Loss=1.1482; Val Acc=0.6726; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6710; Entropy={'forw': '9.3355'}; Entropy_Test=\n",
      "\n",
      "5 1.1443557 1.1424524\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:06epoch=357; Loss Pred=1.1335; Val Loss=1.1425; Val Acc=0.6752; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6736; Entropy={'forw': '9.3346'}; Entropy_Test=\n",
      "\n",
      "0 1.1424524 1.1463275\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:06epoch=358; Loss Pred=1.1337; Val Loss=1.1463; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6743; Entropy={'forw': '9.3346'}; Entropy_Test=\n",
      "\n",
      "1 1.1424524 1.1458304\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:07epoch=359; Loss Pred=1.1338; Val Loss=1.1458; Val Acc=0.6759; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6717; Entropy={'forw': '9.3371'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1.1424524 1.1491466\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:08epoch=360; Loss Pred=1.1332; Val Loss=1.1491; Val Acc=0.6762; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6736; Entropy={'forw': '9.3371'}; Entropy_Test=\n",
      "\n",
      "3 1.1424524 1.1436782\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:09epoch=361; Loss Pred=1.1331; Val Loss=1.1437; Val Acc=0.6735; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6744; Entropy={'forw': '9.3441'}; Entropy_Test=\n",
      "\n",
      "4 1.1424524 1.1452274\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:09epoch=362; Loss Pred=1.1332; Val Loss=1.1452; Val Acc=0.6745; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6718; Entropy={'forw': '9.3441'}; Entropy_Test=\n",
      "\n",
      "5 1.1424524 1.1449375\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:10epoch=363; Loss Pred=1.1339; Val Loss=1.1449; Val Acc=0.6749; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6760; Entropy={'forw': '9.3489'}; Entropy_Test=\n",
      "\n",
      "6 1.1424524 1.144459\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:11epoch=364; Loss Pred=1.1330; Val Loss=1.1445; Val Acc=0.6742; Loss Att={'forw': '1.0000'}; Train Acc=0.678; Test Acc=0.6745; Entropy={'forw': '9.3489'}; Entropy_Test=\n",
      "\n",
      "7 1.1424524 1.1442115\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:12epoch=365; Loss Pred=1.1330; Val Loss=1.1442; Val Acc=0.6776; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6732; Entropy={'forw': '9.3417'}; Entropy_Test=\n",
      "\n",
      "8 1.1424524 1.1457852\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:12epoch=366; Loss Pred=1.1340; Val Loss=1.1458; Val Acc=0.6747; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6725; Entropy={'forw': '9.3417'}; Entropy_Test=\n",
      "\n",
      "9 1.1424524 1.1445756\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:13epoch=367; Loss Pred=1.1333; Val Loss=1.1446; Val Acc=0.6741; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6733; Entropy={'forw': '9.3362'}; Entropy_Test=\n",
      "\n",
      "10 1.1424524 1.1454176\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:14epoch=368; Loss Pred=1.1311; Val Loss=1.1454; Val Acc=0.6739; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6752; Entropy={'forw': '9.3362'}; Entropy_Test=\n",
      "\n",
      "11 1.1424524 1.1462553\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:15epoch=369; Loss Pred=1.1329; Val Loss=1.1463; Val Acc=0.6727; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6714; Entropy={'forw': '9.3465'}; Entropy_Test=\n",
      "\n",
      "12 1.1424524 1.145218\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:15epoch=370; Loss Pred=1.1318; Val Loss=1.1452; Val Acc=0.6730; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6736; Entropy={'forw': '9.3465'}; Entropy_Test=\n",
      "\n",
      "13 1.1424524 1.1429814\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:16epoch=371; Loss Pred=1.1319; Val Loss=1.1430; Val Acc=0.6756; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6723; Entropy={'forw': '9.3525'}; Entropy_Test=\n",
      "\n",
      "14 1.1424524 1.1433223\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:17epoch=372; Loss Pred=1.1322; Val Loss=1.1433; Val Acc=0.6751; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6736; Entropy={'forw': '9.3525'}; Entropy_Test=\n",
      "\n",
      "15 1.1424524 1.1413358\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:18epoch=373; Loss Pred=1.1314; Val Loss=1.1413; Val Acc=0.6767; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6735; Entropy={'forw': '9.3465'}; Entropy_Test=\n",
      "\n",
      "0 1.1413358 1.1432583\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:18epoch=374; Loss Pred=1.1310; Val Loss=1.1433; Val Acc=0.6749; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6743; Entropy={'forw': '9.3465'}; Entropy_Test=\n",
      "\n",
      "1 1.1413358 1.1408665\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:19epoch=375; Loss Pred=1.1303; Val Loss=1.1409; Val Acc=0.6742; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6743; Entropy={'forw': '9.3558'}; Entropy_Test=\n",
      "\n",
      "0 1.1408665 1.1410733\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:20epoch=376; Loss Pred=1.1311; Val Loss=1.1411; Val Acc=0.6760; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6725; Entropy={'forw': '9.3558'}; Entropy_Test=\n",
      "\n",
      "1 1.1408665 1.141825\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:21epoch=377; Loss Pred=1.1298; Val Loss=1.1418; Val Acc=0.6723; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6729; Entropy={'forw': '9.3566'}; Entropy_Test=\n",
      "\n",
      "2 1.1408665 1.1462333\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:21epoch=378; Loss Pred=1.1305; Val Loss=1.1462; Val Acc=0.6763; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6711; Entropy={'forw': '9.3566'}; Entropy_Test=\n",
      "\n",
      "3 1.1408665 1.1437547\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:22epoch=379; Loss Pred=1.1303; Val Loss=1.1438; Val Acc=0.6755; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6728; Entropy={'forw': '9.3575'}; Entropy_Test=\n",
      "\n",
      "4 1.1408665 1.1444978\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:23epoch=380; Loss Pred=1.1316; Val Loss=1.1445; Val Acc=0.6733; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6716; Entropy={'forw': '9.3575'}; Entropy_Test=\n",
      "\n",
      "5 1.1408665 1.144574\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:24epoch=381; Loss Pred=1.1289; Val Loss=1.1446; Val Acc=0.6749; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6726; Entropy={'forw': '9.3586'}; Entropy_Test=\n",
      "\n",
      "6 1.1408665 1.1411102\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:24epoch=382; Loss Pred=1.1309; Val Loss=1.1411; Val Acc=0.6770; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6727; Entropy={'forw': '9.3586'}; Entropy_Test=\n",
      "\n",
      "7 1.1408665 1.1473824\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:25epoch=383; Loss Pred=1.1305; Val Loss=1.1474; Val Acc=0.6730; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6737; Entropy={'forw': '9.3600'}; Entropy_Test=\n",
      "\n",
      "8 1.1408665 1.1423091\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:26epoch=384; Loss Pred=1.1301; Val Loss=1.1423; Val Acc=0.6771; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6734; Entropy={'forw': '9.3600'}; Entropy_Test=\n",
      "\n",
      "9 1.1408665 1.1398638\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:27epoch=385; Loss Pred=1.1299; Val Loss=1.1399; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6753; Entropy={'forw': '9.3544'}; Entropy_Test=\n",
      "\n",
      "0 1.1398638 1.1410375\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:27epoch=386; Loss Pred=1.1286; Val Loss=1.1410; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6736; Entropy={'forw': '9.3544'}; Entropy_Test=\n",
      "\n",
      "1 1.1398638 1.138956\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:28epoch=387; Loss Pred=1.1294; Val Loss=1.1390; Val Acc=0.6770; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6719; Entropy={'forw': '9.3460'}; Entropy_Test=\n",
      "\n",
      "0 1.138956 1.1421039\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:29epoch=388; Loss Pred=1.1305; Val Loss=1.1421; Val Acc=0.6746; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6712; Entropy={'forw': '9.3460'}; Entropy_Test=\n",
      "\n",
      "1 1.138956 1.1427429\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:29epoch=389; Loss Pred=1.1286; Val Loss=1.1427; Val Acc=0.6752; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6721; Entropy={'forw': '9.3529'}; Entropy_Test=\n",
      "\n",
      "2 1.138956 1.1411022\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:30epoch=390; Loss Pred=1.1280; Val Loss=1.1411; Val Acc=0.6742; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6749; Entropy={'forw': '9.3529'}; Entropy_Test=\n",
      "\n",
      "3 1.138956 1.1429693\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:31epoch=391; Loss Pred=1.1267; Val Loss=1.1430; Val Acc=0.6755; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6747; Entropy={'forw': '9.3646'}; Entropy_Test=\n",
      "\n",
      "4 1.138956 1.1429005\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:32epoch=392; Loss Pred=1.1288; Val Loss=1.1429; Val Acc=0.6748; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6719; Entropy={'forw': '9.3646'}; Entropy_Test=\n",
      "\n",
      "5 1.138956 1.1442959\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:32epoch=393; Loss Pred=1.1277; Val Loss=1.1443; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6721; Entropy={'forw': '9.3702'}; Entropy_Test=\n",
      "\n",
      "6 1.138956 1.139494\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:33epoch=394; Loss Pred=1.1278; Val Loss=1.1395; Val Acc=0.6760; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6742; Entropy={'forw': '9.3702'}; Entropy_Test=\n",
      "\n",
      "7 1.138956 1.140696\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:34epoch=395; Loss Pred=1.1301; Val Loss=1.1407; Val Acc=0.6768; Loss Att={'forw': '1.0000'}; Train Acc=0.679; Test Acc=0.6730; Entropy={'forw': '9.3512'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1.138956 1.1427295\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:35epoch=396; Loss Pred=1.1283; Val Loss=1.1427; Val Acc=0.6755; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6739; Entropy={'forw': '9.3512'}; Entropy_Test=\n",
      "\n",
      "9 1.138956 1.1426214\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:35epoch=397; Loss Pred=1.1299; Val Loss=1.1426; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6737; Entropy={'forw': '9.3447'}; Entropy_Test=\n",
      "\n",
      "10 1.138956 1.1419479\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:36epoch=398; Loss Pred=1.1270; Val Loss=1.1419; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6751; Entropy={'forw': '9.3447'}; Entropy_Test=\n",
      "\n",
      "11 1.138956 1.1368746\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:37epoch=399; Loss Pred=1.1278; Val Loss=1.1369; Val Acc=0.6774; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6722; Entropy={'forw': '9.3409'}; Entropy_Test=\n",
      "\n",
      "0 1.1368746 1.1425322\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:38epoch=400; Loss Pred=1.1265; Val Loss=1.1425; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6747; Entropy={'forw': '9.3409'}; Entropy_Test=\n",
      "\n",
      "1 1.1368746 1.1438056\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:38epoch=401; Loss Pred=1.1294; Val Loss=1.1438; Val Acc=0.6747; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6747; Entropy={'forw': '9.3531'}; Entropy_Test=\n",
      "\n",
      "2 1.1368746 1.1460556\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:39epoch=402; Loss Pred=1.1266; Val Loss=1.1461; Val Acc=0.6740; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6749; Entropy={'forw': '9.3531'}; Entropy_Test=\n",
      "\n",
      "3 1.1368746 1.1439582\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:40epoch=403; Loss Pred=1.1263; Val Loss=1.1440; Val Acc=0.6746; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6739; Entropy={'forw': '9.3487'}; Entropy_Test=\n",
      "\n",
      "4 1.1368746 1.145105\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:40epoch=404; Loss Pred=1.1277; Val Loss=1.1451; Val Acc=0.6732; Loss Att={'forw': '1.0000'}; Train Acc=0.682; Test Acc=0.6748; Entropy={'forw': '9.3487'}; Entropy_Test=\n",
      "\n",
      "5 1.1368746 1.1430091\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:41epoch=405; Loss Pred=1.1270; Val Loss=1.1430; Val Acc=0.6750; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6722; Entropy={'forw': '9.3476'}; Entropy_Test=\n",
      "\n",
      "6 1.1368746 1.1446124\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:42epoch=406; Loss Pred=1.1265; Val Loss=1.1446; Val Acc=0.6747; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6736; Entropy={'forw': '9.3476'}; Entropy_Test=\n",
      "\n",
      "7 1.1368746 1.1389744\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:43epoch=407; Loss Pred=1.1264; Val Loss=1.1390; Val Acc=0.6750; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6752; Entropy={'forw': '9.3472'}; Entropy_Test=\n",
      "\n",
      "8 1.1368746 1.1399252\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:43epoch=408; Loss Pred=1.1258; Val Loss=1.1399; Val Acc=0.6749; Loss Att={'forw': '1.0000'}; Train Acc=0.680; Test Acc=0.6722; Entropy={'forw': '9.3472'}; Entropy_Test=\n",
      "\n",
      "9 1.1368746 1.1429678\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:44epoch=409; Loss Pred=1.1265; Val Loss=1.1430; Val Acc=0.6741; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6727; Entropy={'forw': '9.3488'}; Entropy_Test=\n",
      "\n",
      "10 1.1368746 1.1429492\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:45epoch=410; Loss Pred=1.1260; Val Loss=1.1429; Val Acc=0.6734; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6737; Entropy={'forw': '9.3488'}; Entropy_Test=\n",
      "\n",
      "11 1.1368746 1.1414028\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:46epoch=411; Loss Pred=1.1242; Val Loss=1.1414; Val Acc=0.6738; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6752; Entropy={'forw': '9.3602'}; Entropy_Test=\n",
      "\n",
      "12 1.1368746 1.1403539\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:46epoch=412; Loss Pred=1.1242; Val Loss=1.1404; Val Acc=0.6718; Loss Att={'forw': '1.0000'}; Train Acc=0.682; Test Acc=0.6717; Entropy={'forw': '9.3602'}; Entropy_Test=\n",
      "\n",
      "13 1.1368746 1.138843\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:47epoch=413; Loss Pred=1.1250; Val Loss=1.1388; Val Acc=0.6762; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6730; Entropy={'forw': '9.3629'}; Entropy_Test=\n",
      "\n",
      "14 1.1368746 1.1419636\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:48epoch=414; Loss Pred=1.1253; Val Loss=1.1420; Val Acc=0.6748; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6732; Entropy={'forw': '9.3629'}; Entropy_Test=\n",
      "\n",
      "15 1.1368746 1.1394904\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:49epoch=415; Loss Pred=1.1240; Val Loss=1.1395; Val Acc=0.6751; Loss Att={'forw': '1.0000'}; Train Acc=0.682; Test Acc=0.6722; Entropy={'forw': '9.3518'}; Entropy_Test=\n",
      "\n",
      "16 1.1368746 1.1442854\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:49epoch=416; Loss Pred=1.1243; Val Loss=1.1443; Val Acc=0.6744; Loss Att={'forw': '1.0000'}; Train Acc=0.682; Test Acc=0.6737; Entropy={'forw': '9.3518'}; Entropy_Test=\n",
      "\n",
      "17 1.1368746 1.1378882\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:50epoch=417; Loss Pred=1.1246; Val Loss=1.1379; Val Acc=0.6758; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6734; Entropy={'forw': '9.3602'}; Entropy_Test=\n",
      "\n",
      "18 1.1368746 1.1426321\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:51epoch=418; Loss Pred=1.1251; Val Loss=1.1426; Val Acc=0.6757; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6728; Entropy={'forw': '9.3602'}; Entropy_Test=\n",
      "\n",
      "19 1.1368746 1.1443433\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:52epoch=419; Loss Pred=1.1247; Val Loss=1.1443; Val Acc=0.6743; Loss Att={'forw': '1.0000'}; Train Acc=0.681; Test Acc=0.6748; Entropy={'forw': '9.3616'}; Entropy_Test=\n",
      "\n",
      "20 1.1368746 1.1373619\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 421\n",
      "Optimization Finished!\n",
      "Saved Results Successfully\n",
      "L2 reg-n\n",
      "********** replication  0  **********\n",
      "msnbc\n",
      "(7305, 40) (7305, 40) (74, 40) (74, 40)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tGRU bidir(False), task: msnbc\n",
      "    hid: \t\t\t50,\n",
      "    h_hid: \t\t\t100\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t7305/0, SEQ_LEN: 40\n",
      "Logged Successfully: \n",
      "0 10000000000.0 2.8895264\n",
      "Logged Successfully: \n",
      "2018-05-17 19:12:58epoch=0; Loss Pred=2.8908; Val Loss=2.8895; Val Acc=0.0862; Loss Att={'forw': '1.0264'}; Train Acc=0.070; Test Acc=0.0705; Entropy={'forw': '7.1783'}; Entropy_Test=\n",
      "\n",
      "0 2.8895264 2.7911353\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:02epoch=1; Loss Pred=2.7979; Val Loss=2.7911; Val Acc=0.1284; Loss Att={'forw': '1.0167'}; Train Acc=0.095; Test Acc=0.0950; Entropy={'forw': '7.1234'}; Entropy_Test=\n",
      "\n",
      "0 2.7911353 2.7878883\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:04epoch=2; Loss Pred=2.7943; Val Loss=2.7879; Val Acc=0.1341; Loss Att={'forw': '1.0040'}; Train Acc=0.095; Test Acc=0.0961; Entropy={'forw': '7.2177'}; Entropy_Test=\n",
      "\n",
      "0 2.7878883 2.6941216\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:06epoch=3; Loss Pred=2.7036; Val Loss=2.6941; Val Acc=0.1565; Loss Att={'forw': '0.9876'}; Train Acc=0.129; Test Acc=0.1306; Entropy={'forw': '7.3112'}; Entropy_Test=\n",
      "\n",
      "0 2.6941216 2.683533\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:09epoch=4; Loss Pred=2.6949; Val Loss=2.6835; Val Acc=0.1646; Loss Att={'forw': '0.9724'}; Train Acc=0.138; Test Acc=0.1385; Entropy={'forw': '7.3882'}; Entropy_Test=\n",
      "\n",
      "0 2.683533 2.5803232\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:11epoch=5; Loss Pred=2.5965; Val Loss=2.5803; Val Acc=0.2205; Loss Att={'forw': '0.9360'}; Train Acc=0.217; Test Acc=0.2149; Entropy={'forw': '7.6865'}; Entropy_Test=\n",
      "\n",
      "0 2.5803232 2.5723624\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:14epoch=6; Loss Pred=2.5867; Val Loss=2.5724; Val Acc=0.1855; Loss Att={'forw': '0.9116'}; Train Acc=0.207; Test Acc=0.2037; Entropy={'forw': '7.7073'}; Entropy_Test=\n",
      "\n",
      "0 2.5723624 2.5067377\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:16epoch=7; Loss Pred=2.5127; Val Loss=2.5067; Val Acc=0.1739; Loss Att={'forw': '0.8574'}; Train Acc=0.209; Test Acc=0.2043; Entropy={'forw': '7.3798'}; Entropy_Test=\n",
      "\n",
      "0 2.5067377 2.5064924\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:18epoch=8; Loss Pred=2.5142; Val Loss=2.5065; Val Acc=0.1786; Loss Att={'forw': '0.8428'}; Train Acc=0.204; Test Acc=0.1992; Entropy={'forw': '6.9459'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.5064924 2.4756331\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:20epoch=9; Loss Pred=2.4780; Val Loss=2.4756; Val Acc=0.1702; Loss Att={'forw': '0.8263'}; Train Acc=0.208; Test Acc=0.2074; Entropy={'forw': '6.8215'}; Entropy_Test=\n",
      "\n",
      "0 2.4756331 2.4823046\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:23epoch=10; Loss Pred=2.4772; Val Loss=2.4823; Val Acc=0.1762; Loss Att={'forw': '0.8152'}; Train Acc=0.209; Test Acc=0.2032; Entropy={'forw': '6.8180'}; Entropy_Test=\n",
      "\n",
      "1 2.4756331 2.4306183\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:25epoch=11; Loss Pred=2.4329; Val Loss=2.4306; Val Acc=0.2007; Loss Att={'forw': '0.8240'}; Train Acc=0.233; Test Acc=0.2300; Entropy={'forw': '6.9703'}; Entropy_Test=\n",
      "\n",
      "0 2.4306183 2.4231114\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:27epoch=12; Loss Pred=2.4271; Val Loss=2.4231; Val Acc=0.2078; Loss Att={'forw': '0.8210'}; Train Acc=0.244; Test Acc=0.2397; Entropy={'forw': '7.0602'}; Entropy_Test=\n",
      "\n",
      "0 2.4231114 2.3513768\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:29epoch=13; Loss Pred=2.3654; Val Loss=2.3514; Val Acc=0.2854; Loss Att={'forw': '0.8486'}; Train Acc=0.289; Test Acc=0.2814; Entropy={'forw': '7.3127'}; Entropy_Test=\n",
      "\n",
      "0 2.3513768 2.345936\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:32epoch=14; Loss Pred=2.3590; Val Loss=2.3459; Val Acc=0.2921; Loss Att={'forw': '0.8506'}; Train Acc=0.297; Test Acc=0.2895; Entropy={'forw': '7.4986'}; Entropy_Test=\n",
      "\n",
      "0 2.345936 2.2751193\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:34epoch=15; Loss Pred=2.2978; Val Loss=2.2751; Val Acc=0.3217; Loss Att={'forw': '0.8742'}; Train Acc=0.336; Test Acc=0.3284; Entropy={'forw': '7.8676'}; Entropy_Test=\n",
      "\n",
      "0 2.2751193 2.2769701\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:36epoch=16; Loss Pred=2.2945; Val Loss=2.2770; Val Acc=0.3333; Loss Att={'forw': '0.8719'}; Train Acc=0.343; Test Acc=0.3355; Entropy={'forw': '8.0533'}; Entropy_Test=\n",
      "\n",
      "1 2.2751193 2.200328\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:39epoch=17; Loss Pred=2.2288; Val Loss=2.2003; Val Acc=0.3940; Loss Att={'forw': '0.8785'}; Train Acc=0.392; Test Acc=0.3875; Entropy={'forw': '8.2686'}; Entropy_Test=\n",
      "\n",
      "0 2.200328 2.1855347\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:41epoch=18; Loss Pred=2.2242; Val Loss=2.1855; Val Acc=0.4075; Loss Att={'forw': '0.8724'}; Train Acc=0.394; Test Acc=0.3837; Entropy={'forw': '8.4222'}; Entropy_Test=\n",
      "\n",
      "0 2.1855347 2.130026\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:43epoch=19; Loss Pred=2.1771; Val Loss=2.1300; Val Acc=0.4161; Loss Att={'forw': '0.8620'}; Train Acc=0.405; Test Acc=0.3986; Entropy={'forw': '8.4310'}; Entropy_Test=\n",
      "\n",
      "0 2.130026 2.1335695\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:46epoch=20; Loss Pred=2.1870; Val Loss=2.1336; Val Acc=0.4189; Loss Att={'forw': '0.8486'}; Train Acc=0.383; Test Acc=0.3752; Entropy={'forw': '8.4265'}; Entropy_Test=\n",
      "\n",
      "1 2.130026 2.1155753\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:48epoch=21; Loss Pred=2.1554; Val Loss=2.1156; Val Acc=0.4237; Loss Att={'forw': '0.8418'}; Train Acc=0.392; Test Acc=0.3882; Entropy={'forw': '8.4561'}; Entropy_Test=\n",
      "\n",
      "0 2.1155753 2.1445618\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:50epoch=22; Loss Pred=2.1817; Val Loss=2.1446; Val Acc=0.4154; Loss Att={'forw': '0.8280'}; Train Acc=0.368; Test Acc=0.3642; Entropy={'forw': '8.3881'}; Entropy_Test=\n",
      "\n",
      "1 2.1155753 2.1084669\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:52epoch=23; Loss Pred=2.1493; Val Loss=2.1085; Val Acc=0.4233; Loss Att={'forw': '0.8303'}; Train Acc=0.390; Test Acc=0.3826; Entropy={'forw': '8.5280'}; Entropy_Test=\n",
      "\n",
      "0 2.1084669 2.142127\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:54epoch=24; Loss Pred=2.1728; Val Loss=2.1421; Val Acc=0.4090; Loss Att={'forw': '0.8216'}; Train Acc=0.382; Test Acc=0.3785; Entropy={'forw': '8.4554'}; Entropy_Test=\n",
      "\n",
      "1 2.1084669 2.053854\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:57epoch=25; Loss Pred=2.0737; Val Loss=2.0539; Val Acc=0.4282; Loss Att={'forw': '0.8302'}; Train Acc=0.416; Test Acc=0.4099; Entropy={'forw': '8.5768'}; Entropy_Test=\n",
      "\n",
      "0 2.053854 2.055138\n",
      "Logged Successfully: \n",
      "2018-05-17 19:13:59epoch=26; Loss Pred=2.0785; Val Loss=2.0551; Val Acc=0.4293; Loss Att={'forw': '0.8243'}; Train Acc=0.409; Test Acc=0.4016; Entropy={'forw': '8.5795'}; Entropy_Test=\n",
      "\n",
      "1 2.053854 1.9825202\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:01epoch=27; Loss Pred=1.9987; Val Loss=1.9825; Val Acc=0.4406; Loss Att={'forw': '0.8309'}; Train Acc=0.433; Test Acc=0.4260; Entropy={'forw': '8.4592'}; Entropy_Test=\n",
      "\n",
      "0 1.9825202 1.9874818\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:03epoch=28; Loss Pred=1.9979; Val Loss=1.9875; Val Acc=0.4349; Loss Att={'forw': '0.8252'}; Train Acc=0.430; Test Acc=0.4221; Entropy={'forw': '8.4021'}; Entropy_Test=\n",
      "\n",
      "1 1.9825202 1.9624077\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:06epoch=29; Loss Pred=1.9699; Val Loss=1.9624; Val Acc=0.3834; Loss Att={'forw': '0.8234'}; Train Acc=0.419; Test Acc=0.4101; Entropy={'forw': '8.1194'}; Entropy_Test=\n",
      "\n",
      "0 1.9624077 1.9725124\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:08epoch=30; Loss Pred=1.9750; Val Loss=1.9725; Val Acc=0.3880; Loss Att={'forw': '0.8163'}; Train Acc=0.413; Test Acc=0.4016; Entropy={'forw': '8.1240'}; Entropy_Test=\n",
      "\n",
      "1 1.9624077 1.9474514\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:10epoch=31; Loss Pred=1.9504; Val Loss=1.9475; Val Acc=0.3976; Loss Att={'forw': '0.8119'}; Train Acc=0.433; Test Acc=0.4272; Entropy={'forw': '7.9948'}; Entropy_Test=\n",
      "\n",
      "0 1.9474514 1.9567173\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:12epoch=32; Loss Pred=1.9572; Val Loss=1.9567; Val Acc=0.3935; Loss Att={'forw': '0.8039'}; Train Acc=0.430; Test Acc=0.4244; Entropy={'forw': '8.0123'}; Entropy_Test=\n",
      "\n",
      "1 1.9474514 1.928102\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:14epoch=33; Loss Pred=1.9207; Val Loss=1.9281; Val Acc=0.4390; Loss Att={'forw': '0.7975'}; Train Acc=0.469; Test Acc=0.4602; Entropy={'forw': '8.0550'}; Entropy_Test=\n",
      "\n",
      "0 1.928102 1.9303486\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:16epoch=34; Loss Pred=1.9262; Val Loss=1.9303; Val Acc=0.4493; Loss Att={'forw': '0.7891'}; Train Acc=0.470; Test Acc=0.4633; Entropy={'forw': '8.0973'}; Entropy_Test=\n",
      "\n",
      "1 1.928102 1.8745241\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:19epoch=35; Loss Pred=1.8744; Val Loss=1.8745; Val Acc=0.5064; Loss Att={'forw': '0.7833'}; Train Acc=0.510; Test Acc=0.5039; Entropy={'forw': '8.2351'}; Entropy_Test=\n",
      "\n",
      "0 1.8745241 1.8763704\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:21epoch=36; Loss Pred=1.8778; Val Loss=1.8764; Val Acc=0.5106; Loss Att={'forw': '0.7742'}; Train Acc=0.515; Test Acc=0.5104; Entropy={'forw': '8.2459'}; Entropy_Test=\n",
      "\n",
      "1 1.8745241 1.8190879\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:24epoch=37; Loss Pred=1.8180; Val Loss=1.8191; Val Acc=0.5578; Loss Att={'forw': '0.7682'}; Train Acc=0.553; Test Acc=0.5459; Entropy={'forw': '8.4157'}; Entropy_Test=\n",
      "\n",
      "0 1.8190879 1.8153788\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:26epoch=38; Loss Pred=1.8189; Val Loss=1.8154; Val Acc=0.5665; Loss Att={'forw': '0.7597'}; Train Acc=0.555; Test Acc=0.5485; Entropy={'forw': '8.4646'}; Entropy_Test=\n",
      "\n",
      "0 1.8153788 1.7692401\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:28epoch=39; Loss Pred=1.7722; Val Loss=1.7692; Val Acc=0.5787; Loss Att={'forw': '0.7530'}; Train Acc=0.571; Test Acc=0.5622; Entropy={'forw': '8.6228'}; Entropy_Test=\n",
      "\n",
      "0 1.7692401 1.7778133\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:30epoch=40; Loss Pred=1.7772; Val Loss=1.7778; Val Acc=0.5811; Loss Att={'forw': '0.7436'}; Train Acc=0.570; Test Acc=0.5615; Entropy={'forw': '8.6939'}; Entropy_Test=\n",
      "\n",
      "1 1.7692401 1.7415901\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:33epoch=41; Loss Pred=1.7504; Val Loss=1.7416; Val Acc=0.5568; Loss Att={'forw': '0.7341'}; Train Acc=0.567; Test Acc=0.5612; Entropy={'forw': '8.7376'}; Entropy_Test=\n",
      "\n",
      "0 1.7415901 1.7553754\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:35epoch=42; Loss Pred=1.7635; Val Loss=1.7554; Val Acc=0.5477; Loss Att={'forw': '0.7239'}; Train Acc=0.562; Test Acc=0.5539; Entropy={'forw': '8.7486'}; Entropy_Test=\n",
      "\n",
      "1 1.7415901 1.7159512\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:37epoch=43; Loss Pred=1.7215; Val Loss=1.7160; Val Acc=0.5590; Loss Att={'forw': '0.7154'}; Train Acc=0.567; Test Acc=0.5580; Entropy={'forw': '8.8145'}; Entropy_Test=\n",
      "\n",
      "0 1.7159512 1.7251085\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:39epoch=44; Loss Pred=1.7305; Val Loss=1.7251; Val Acc=0.5455; Loss Att={'forw': '0.7065'}; Train Acc=0.562; Test Acc=0.5555; Entropy={'forw': '8.8196'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.7159512 1.6471499\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:41epoch=45; Loss Pred=1.6608; Val Loss=1.6471; Val Acc=0.5836; Loss Att={'forw': '0.6991'}; Train Acc=0.580; Test Acc=0.5747; Entropy={'forw': '8.7859'}; Entropy_Test=\n",
      "\n",
      "0 1.6471499 1.6464299\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:43epoch=46; Loss Pred=1.6618; Val Loss=1.6464; Val Acc=0.5804; Loss Att={'forw': '0.6915'}; Train Acc=0.581; Test Acc=0.5731; Entropy={'forw': '8.7797'}; Entropy_Test=\n",
      "\n",
      "0 1.6464299 1.5800053\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:46epoch=47; Loss Pred=1.6025; Val Loss=1.5800; Val Acc=0.5994; Loss Att={'forw': '0.6833'}; Train Acc=0.589; Test Acc=0.5810; Entropy={'forw': '8.7165'}; Entropy_Test=\n",
      "\n",
      "0 1.5800053 1.5726824\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:48epoch=48; Loss Pred=1.6018; Val Loss=1.5727; Val Acc=0.5967; Loss Att={'forw': '0.6744'}; Train Acc=0.590; Test Acc=0.5811; Entropy={'forw': '8.7270'}; Entropy_Test=\n",
      "\n",
      "0 1.5726824 1.5337846\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:50epoch=49; Loss Pred=1.5651; Val Loss=1.5338; Val Acc=0.6061; Loss Att={'forw': '0.6644'}; Train Acc=0.592; Test Acc=0.5855; Entropy={'forw': '8.6676'}; Entropy_Test=\n",
      "\n",
      "0 1.5337846 1.5348369\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:52epoch=50; Loss Pred=1.5657; Val Loss=1.5348; Val Acc=0.6097; Loss Att={'forw': '0.6547'}; Train Acc=0.593; Test Acc=0.5861; Entropy={'forw': '8.6679'}; Entropy_Test=\n",
      "\n",
      "1 1.5337846 1.5039003\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:55epoch=51; Loss Pred=1.5407; Val Loss=1.5039; Val Acc=0.6176; Loss Att={'forw': '0.6439'}; Train Acc=0.596; Test Acc=0.5885; Entropy={'forw': '8.7305'}; Entropy_Test=\n",
      "\n",
      "0 1.5039003 1.5020413\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:57epoch=52; Loss Pred=1.5441; Val Loss=1.5020; Val Acc=0.6116; Loss Att={'forw': '0.6336'}; Train Acc=0.595; Test Acc=0.5882; Entropy={'forw': '8.7068'}; Entropy_Test=\n",
      "\n",
      "0 1.5020413 1.4919543\n",
      "Logged Successfully: \n",
      "2018-05-17 19:14:59epoch=53; Loss Pred=1.5257; Val Loss=1.4920; Val Acc=0.6169; Loss Att={'forw': '0.6229'}; Train Acc=0.600; Test Acc=0.5906; Entropy={'forw': '8.7458'}; Entropy_Test=\n",
      "\n",
      "0 1.4919543 1.5005609\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:01epoch=54; Loss Pred=1.5304; Val Loss=1.5006; Val Acc=0.6165; Loss Att={'forw': '0.6125'}; Train Acc=0.599; Test Acc=0.5928; Entropy={'forw': '8.7639'}; Entropy_Test=\n",
      "\n",
      "1 1.4919543 1.4840068\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:03epoch=55; Loss Pred=1.5147; Val Loss=1.4840; Val Acc=0.6069; Loss Att={'forw': '0.6027'}; Train Acc=0.604; Test Acc=0.5960; Entropy={'forw': '8.7398'}; Entropy_Test=\n",
      "\n",
      "0 1.4840068 1.4859118\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:05epoch=56; Loss Pred=1.5183; Val Loss=1.4859; Val Acc=0.6204; Loss Att={'forw': '0.5930'}; Train Acc=0.604; Test Acc=0.5993; Entropy={'forw': '8.7714'}; Entropy_Test=\n",
      "\n",
      "1 1.4840068 1.4639157\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:08epoch=57; Loss Pred=1.4992; Val Loss=1.4639; Val Acc=0.6215; Loss Att={'forw': '0.5838'}; Train Acc=0.610; Test Acc=0.6036; Entropy={'forw': '8.7765'}; Entropy_Test=\n",
      "\n",
      "0 1.4639157 1.4662298\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:10epoch=58; Loss Pred=1.5009; Val Loss=1.4662; Val Acc=0.6234; Loss Att={'forw': '0.5734'}; Train Acc=0.609; Test Acc=0.6022; Entropy={'forw': '8.8195'}; Entropy_Test=\n",
      "\n",
      "1 1.4639157 1.448518\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:12epoch=59; Loss Pred=1.4792; Val Loss=1.4485; Val Acc=0.6246; Loss Att={'forw': '0.5651'}; Train Acc=0.615; Test Acc=0.6090; Entropy={'forw': '8.8189'}; Entropy_Test=\n",
      "\n",
      "0 1.448518 1.4408609\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:15epoch=60; Loss Pred=1.4791; Val Loss=1.4409; Val Acc=0.6255; Loss Att={'forw': '0.5552'}; Train Acc=0.615; Test Acc=0.6085; Entropy={'forw': '8.8049'}; Entropy_Test=\n",
      "\n",
      "0 1.4408609 1.4244239\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:17epoch=61; Loss Pred=1.4578; Val Loss=1.4244; Val Acc=0.6284; Loss Att={'forw': '0.5459'}; Train Acc=0.620; Test Acc=0.6157; Entropy={'forw': '8.7314'}; Entropy_Test=\n",
      "\n",
      "0 1.4244239 1.4270234\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:19epoch=62; Loss Pred=1.4583; Val Loss=1.4270; Val Acc=0.6226; Loss Att={'forw': '0.5361'}; Train Acc=0.621; Test Acc=0.6157; Entropy={'forw': '8.7380'}; Entropy_Test=\n",
      "\n",
      "1 1.4244239 1.4003712\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:21epoch=63; Loss Pred=1.4410; Val Loss=1.4004; Val Acc=0.6324; Loss Att={'forw': '0.5264'}; Train Acc=0.626; Test Acc=0.6213; Entropy={'forw': '8.7545'}; Entropy_Test=\n",
      "\n",
      "0 1.4003712 1.4104471\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:24epoch=64; Loss Pred=1.4410; Val Loss=1.4104; Val Acc=0.6317; Loss Att={'forw': '0.5170'}; Train Acc=0.625; Test Acc=0.6205; Entropy={'forw': '8.7373'}; Entropy_Test=\n",
      "\n",
      "1 1.4003712 1.3985281\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:26epoch=65; Loss Pred=1.4285; Val Loss=1.3985; Val Acc=0.6353; Loss Att={'forw': '0.5077'}; Train Acc=0.630; Test Acc=0.6262; Entropy={'forw': '8.6527'}; Entropy_Test=\n",
      "\n",
      "0 1.3985281 1.3998156\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:28epoch=66; Loss Pred=1.4292; Val Loss=1.3998; Val Acc=0.6301; Loss Att={'forw': '0.4982'}; Train Acc=0.631; Test Acc=0.6269; Entropy={'forw': '8.6859'}; Entropy_Test=\n",
      "\n",
      "1 1.3985281 1.3856837\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:30epoch=67; Loss Pred=1.4192; Val Loss=1.3857; Val Acc=0.6356; Loss Att={'forw': '0.4887'}; Train Acc=0.633; Test Acc=0.6282; Entropy={'forw': '8.6429'}; Entropy_Test=\n",
      "\n",
      "0 1.3856837 1.3850719\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:32epoch=68; Loss Pred=1.4190; Val Loss=1.3851; Val Acc=0.6445; Loss Att={'forw': '0.4796'}; Train Acc=0.634; Test Acc=0.6286; Entropy={'forw': '8.5920'}; Entropy_Test=\n",
      "\n",
      "0 1.3850719 1.3665681\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:35epoch=69; Loss Pred=1.4099; Val Loss=1.3666; Val Acc=0.6399; Loss Att={'forw': '0.4708'}; Train Acc=0.637; Test Acc=0.6319; Entropy={'forw': '8.6479'}; Entropy_Test=\n",
      "\n",
      "0 1.3665681 1.3730431\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:37epoch=70; Loss Pred=1.4115; Val Loss=1.3730; Val Acc=0.6407; Loss Att={'forw': '0.4617'}; Train Acc=0.637; Test Acc=0.6323; Entropy={'forw': '8.6031'}; Entropy_Test=\n",
      "\n",
      "1 1.3665681 1.3597549\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:39epoch=71; Loss Pred=1.4020; Val Loss=1.3598; Val Acc=0.6486; Loss Att={'forw': '0.4531'}; Train Acc=0.639; Test Acc=0.6315; Entropy={'forw': '8.5365'}; Entropy_Test=\n",
      "\n",
      "0 1.3597549 1.3697964\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:41epoch=72; Loss Pred=1.4037; Val Loss=1.3698; Val Acc=0.6462; Loss Att={'forw': '0.4442'}; Train Acc=0.639; Test Acc=0.6338; Entropy={'forw': '8.5438'}; Entropy_Test=\n",
      "\n",
      "1 1.3597549 1.3741612\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:43epoch=73; Loss Pred=1.3952; Val Loss=1.3742; Val Acc=0.6456; Loss Att={'forw': '0.4355'}; Train Acc=0.641; Test Acc=0.6371; Entropy={'forw': '8.4611'}; Entropy_Test=\n",
      "\n",
      "2 1.3597549 1.3638699\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:45epoch=74; Loss Pred=1.3975; Val Loss=1.3639; Val Acc=0.6478; Loss Att={'forw': '0.4269'}; Train Acc=0.640; Test Acc=0.6359; Entropy={'forw': '8.5027'}; Entropy_Test=\n",
      "\n",
      "3 1.3597549 1.3501248\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:48epoch=75; Loss Pred=1.3888; Val Loss=1.3501; Val Acc=0.6492; Loss Att={'forw': '0.4185'}; Train Acc=0.642; Test Acc=0.6396; Entropy={'forw': '8.4634'}; Entropy_Test=\n",
      "\n",
      "0 1.3501248 1.3629398\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:50epoch=76; Loss Pred=1.3900; Val Loss=1.3629; Val Acc=0.6464; Loss Att={'forw': '0.4102'}; Train Acc=0.642; Test Acc=0.6405; Entropy={'forw': '8.4272'}; Entropy_Test=\n",
      "\n",
      "1 1.3501248 1.3496195\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:52epoch=77; Loss Pred=1.3818; Val Loss=1.3496; Val Acc=0.6491; Loss Att={'forw': '0.4024'}; Train Acc=0.643; Test Acc=0.6417; Entropy={'forw': '8.3444'}; Entropy_Test=\n",
      "\n",
      "0 1.3496195 1.3505355\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:54epoch=78; Loss Pred=1.3826; Val Loss=1.3505; Val Acc=0.6515; Loss Att={'forw': '0.3947'}; Train Acc=0.644; Test Acc=0.6410; Entropy={'forw': '8.3556'}; Entropy_Test=\n",
      "\n",
      "1 1.3496195 1.3384604\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:57epoch=79; Loss Pred=1.3759; Val Loss=1.3385; Val Acc=0.6459; Loss Att={'forw': '0.3871'}; Train Acc=0.646; Test Acc=0.6424; Entropy={'forw': '8.3603'}; Entropy_Test=\n",
      "\n",
      "0 1.3384604 1.3512247\n",
      "Logged Successfully: \n",
      "2018-05-17 19:15:59epoch=80; Loss Pred=1.3773; Val Loss=1.3512; Val Acc=0.6487; Loss Att={'forw': '0.3791'}; Train Acc=0.646; Test Acc=0.6412; Entropy={'forw': '8.3488'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.3384604 1.3370736\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:01epoch=81; Loss Pred=1.3686; Val Loss=1.3371; Val Acc=0.6475; Loss Att={'forw': '0.3720'}; Train Acc=0.648; Test Acc=0.6446; Entropy={'forw': '8.2977'}; Entropy_Test=\n",
      "\n",
      "0 1.3370736 1.344105\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:03epoch=82; Loss Pred=1.3718; Val Loss=1.3441; Val Acc=0.6490; Loss Att={'forw': '0.3646'}; Train Acc=0.647; Test Acc=0.6444; Entropy={'forw': '8.2150'}; Entropy_Test=\n",
      "\n",
      "1 1.3370736 1.3351202\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:05epoch=83; Loss Pred=1.3628; Val Loss=1.3351; Val Acc=0.6454; Loss Att={'forw': '0.3581'}; Train Acc=0.649; Test Acc=0.6458; Entropy={'forw': '8.2423'}; Entropy_Test=\n",
      "\n",
      "0 1.3351202 1.3342882\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:07epoch=84; Loss Pred=1.3652; Val Loss=1.3343; Val Acc=0.6480; Loss Att={'forw': '0.3508'}; Train Acc=0.649; Test Acc=0.6454; Entropy={'forw': '8.2481'}; Entropy_Test=\n",
      "\n",
      "0 1.3342882 1.3350003\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:09epoch=85; Loss Pred=1.3578; Val Loss=1.3350; Val Acc=0.6454; Loss Att={'forw': '0.3447'}; Train Acc=0.650; Test Acc=0.6496; Entropy={'forw': '8.1890'}; Entropy_Test=\n",
      "\n",
      "1 1.3342882 1.3261337\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:12epoch=86; Loss Pred=1.3598; Val Loss=1.3261; Val Acc=0.6520; Loss Att={'forw': '0.3376'}; Train Acc=0.650; Test Acc=0.6460; Entropy={'forw': '8.1490'}; Entropy_Test=\n",
      "\n",
      "0 1.3261337 1.3211074\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:14epoch=87; Loss Pred=1.3531; Val Loss=1.3211; Val Acc=0.6516; Loss Att={'forw': '0.3318'}; Train Acc=0.651; Test Acc=0.6468; Entropy={'forw': '8.1289'}; Entropy_Test=\n",
      "\n",
      "0 1.3211074 1.3238648\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:16epoch=88; Loss Pred=1.3551; Val Loss=1.3239; Val Acc=0.6507; Loss Att={'forw': '0.3255'}; Train Acc=0.651; Test Acc=0.6478; Entropy={'forw': '8.1250'}; Entropy_Test=\n",
      "\n",
      "1 1.3211074 1.3203332\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:18epoch=89; Loss Pred=1.3486; Val Loss=1.3203; Val Acc=0.6506; Loss Att={'forw': '0.3200'}; Train Acc=0.651; Test Acc=0.6465; Entropy={'forw': '8.0928'}; Entropy_Test=\n",
      "\n",
      "0 1.3203332 1.3198669\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:20epoch=90; Loss Pred=1.3509; Val Loss=1.3199; Val Acc=0.6534; Loss Att={'forw': '0.3141'}; Train Acc=0.651; Test Acc=0.6497; Entropy={'forw': '8.0704'}; Entropy_Test=\n",
      "\n",
      "0 1.3198669 1.3113105\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:23epoch=91; Loss Pred=1.3432; Val Loss=1.3113; Val Acc=0.6520; Loss Att={'forw': '0.3090'}; Train Acc=0.652; Test Acc=0.6504; Entropy={'forw': '8.1091'}; Entropy_Test=\n",
      "\n",
      "0 1.3113105 1.3167603\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:25epoch=92; Loss Pred=1.3466; Val Loss=1.3168; Val Acc=0.6519; Loss Att={'forw': '0.3036'}; Train Acc=0.652; Test Acc=0.6488; Entropy={'forw': '8.1484'}; Entropy_Test=\n",
      "\n",
      "1 1.3113105 1.3136545\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:27epoch=93; Loss Pred=1.3408; Val Loss=1.3137; Val Acc=0.6570; Loss Att={'forw': '0.2986'}; Train Acc=0.652; Test Acc=0.6484; Entropy={'forw': '8.0996'}; Entropy_Test=\n",
      "\n",
      "2 1.3113105 1.3083489\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:29epoch=94; Loss Pred=1.3416; Val Loss=1.3083; Val Acc=0.6549; Loss Att={'forw': '0.2935'}; Train Acc=0.652; Test Acc=0.6515; Entropy={'forw': '8.0980'}; Entropy_Test=\n",
      "\n",
      "0 1.3083489 1.3137969\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:32epoch=95; Loss Pred=1.3356; Val Loss=1.3138; Val Acc=0.6533; Loss Att={'forw': '0.2894'}; Train Acc=0.653; Test Acc=0.6472; Entropy={'forw': '8.0405'}; Entropy_Test=\n",
      "\n",
      "1 1.3083489 1.3087374\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:34epoch=96; Loss Pred=1.3367; Val Loss=1.3087; Val Acc=0.6540; Loss Att={'forw': '0.2847'}; Train Acc=0.653; Test Acc=0.6491; Entropy={'forw': '8.0585'}; Entropy_Test=\n",
      "\n",
      "2 1.3083489 1.2987515\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:36epoch=97; Loss Pred=1.3305; Val Loss=1.2988; Val Acc=0.6574; Loss Att={'forw': '0.2808'}; Train Acc=0.653; Test Acc=0.6525; Entropy={'forw': '8.0649'}; Entropy_Test=\n",
      "\n",
      "0 1.2987515 1.2982404\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:38epoch=98; Loss Pred=1.3317; Val Loss=1.2982; Val Acc=0.6587; Loss Att={'forw': '0.2762'}; Train Acc=0.653; Test Acc=0.6507; Entropy={'forw': '8.0416'}; Entropy_Test=\n",
      "\n",
      "0 1.2982404 1.2869909\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:41epoch=99; Loss Pred=1.3253; Val Loss=1.2870; Val Acc=0.6609; Loss Att={'forw': '0.2727'}; Train Acc=0.654; Test Acc=0.6513; Entropy={'forw': '8.0874'}; Entropy_Test=\n",
      "\n",
      "0 1.2869909 1.3052576\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:43epoch=100; Loss Pred=1.3269; Val Loss=1.3053; Val Acc=0.6561; Loss Att={'forw': '0.2689'}; Train Acc=0.654; Test Acc=0.6504; Entropy={'forw': '8.0813'}; Entropy_Test=\n",
      "\n",
      "1 1.2869909 1.2974927\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:45epoch=101; Loss Pred=1.3204; Val Loss=1.2975; Val Acc=0.6586; Loss Att={'forw': '0.2656'}; Train Acc=0.654; Test Acc=0.6515; Entropy={'forw': '8.0283'}; Entropy_Test=\n",
      "\n",
      "2 1.2869909 1.2936511\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:47epoch=102; Loss Pred=1.3207; Val Loss=1.2937; Val Acc=0.6548; Loss Att={'forw': '0.2622'}; Train Acc=0.654; Test Acc=0.6498; Entropy={'forw': '8.0171'}; Entropy_Test=\n",
      "\n",
      "3 1.2869909 1.2965081\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:50epoch=103; Loss Pred=1.3178; Val Loss=1.2965; Val Acc=0.6531; Loss Att={'forw': '0.2596'}; Train Acc=0.655; Test Acc=0.6526; Entropy={'forw': '7.9931'}; Entropy_Test=\n",
      "\n",
      "4 1.2869909 1.2830719\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:52epoch=104; Loss Pred=1.3173; Val Loss=1.2831; Val Acc=0.6611; Loss Att={'forw': '0.2558'}; Train Acc=0.655; Test Acc=0.6520; Entropy={'forw': '8.0087'}; Entropy_Test=\n",
      "\n",
      "0 1.2830719 1.2846687\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:54epoch=105; Loss Pred=1.3136; Val Loss=1.2847; Val Acc=0.6624; Loss Att={'forw': '0.2534'}; Train Acc=0.655; Test Acc=0.6507; Entropy={'forw': '7.9795'}; Entropy_Test=\n",
      "\n",
      "1 1.2830719 1.2896978\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:56epoch=106; Loss Pred=1.3129; Val Loss=1.2897; Val Acc=0.6577; Loss Att={'forw': '0.2502'}; Train Acc=0.655; Test Acc=0.6523; Entropy={'forw': '7.9753'}; Entropy_Test=\n",
      "\n",
      "2 1.2830719 1.273834\n",
      "Logged Successfully: \n",
      "2018-05-17 19:16:59epoch=107; Loss Pred=1.3103; Val Loss=1.2738; Val Acc=0.6633; Loss Att={'forw': '0.2480'}; Train Acc=0.655; Test Acc=0.6534; Entropy={'forw': '7.9715'}; Entropy_Test=\n",
      "\n",
      "0 1.273834 1.2909473\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:01epoch=108; Loss Pred=1.3101; Val Loss=1.2909; Val Acc=0.6602; Loss Att={'forw': '0.2452'}; Train Acc=0.655; Test Acc=0.6521; Entropy={'forw': '7.9738'}; Entropy_Test=\n",
      "\n",
      "1 1.273834 1.2823931\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:03epoch=109; Loss Pred=1.3065; Val Loss=1.2824; Val Acc=0.6578; Loss Att={'forw': '0.2434'}; Train Acc=0.656; Test Acc=0.6525; Entropy={'forw': '8.0327'}; Entropy_Test=\n",
      "\n",
      "2 1.273834 1.280827\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:05epoch=110; Loss Pred=1.3064; Val Loss=1.2808; Val Acc=0.6636; Loss Att={'forw': '0.2411'}; Train Acc=0.655; Test Acc=0.6510; Entropy={'forw': '8.0434'}; Entropy_Test=\n",
      "\n",
      "3 1.273834 1.2787329\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:08epoch=111; Loss Pred=1.3039; Val Loss=1.2787; Val Acc=0.6627; Loss Att={'forw': '0.2390'}; Train Acc=0.656; Test Acc=0.6528; Entropy={'forw': '8.0038'}; Entropy_Test=\n",
      "\n",
      "4 1.273834 1.2852803\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:10epoch=112; Loss Pred=1.3031; Val Loss=1.2853; Val Acc=0.6612; Loss Att={'forw': '0.2368'}; Train Acc=0.656; Test Acc=0.6551; Entropy={'forw': '7.9791'}; Entropy_Test=\n",
      "\n",
      "5 1.273834 1.2736896\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:12epoch=113; Loss Pred=1.3005; Val Loss=1.2737; Val Acc=0.6615; Loss Att={'forw': '0.2352'}; Train Acc=0.656; Test Acc=0.6518; Entropy={'forw': '8.0012'}; Entropy_Test=\n",
      "\n",
      "0 1.2736896 1.2707409\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:14epoch=114; Loss Pred=1.2999; Val Loss=1.2707; Val Acc=0.6645; Loss Att={'forw': '0.2332'}; Train Acc=0.657; Test Acc=0.6558; Entropy={'forw': '8.0140'}; Entropy_Test=\n",
      "\n",
      "0 1.2707409 1.2621844\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:16epoch=115; Loss Pred=1.2971; Val Loss=1.2622; Val Acc=0.6634; Loss Att={'forw': '0.2322'}; Train Acc=0.657; Test Acc=0.6533; Entropy={'forw': '8.0196'}; Entropy_Test=\n",
      "\n",
      "0 1.2621844 1.2630264\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:19epoch=116; Loss Pred=1.2966; Val Loss=1.2630; Val Acc=0.6659; Loss Att={'forw': '0.2307'}; Train Acc=0.656; Test Acc=0.6537; Entropy={'forw': '7.9904'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.2621844 1.2603536\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:21epoch=117; Loss Pred=1.2949; Val Loss=1.2604; Val Acc=0.6658; Loss Att={'forw': '0.2289'}; Train Acc=0.656; Test Acc=0.6531; Entropy={'forw': '7.9889'}; Entropy_Test=\n",
      "\n",
      "0 1.2603536 1.2716793\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:23epoch=118; Loss Pred=1.2934; Val Loss=1.2717; Val Acc=0.6632; Loss Att={'forw': '0.2273'}; Train Acc=0.657; Test Acc=0.6558; Entropy={'forw': '8.0327'}; Entropy_Test=\n",
      "\n",
      "1 1.2603536 1.2597599\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:25epoch=119; Loss Pred=1.2924; Val Loss=1.2598; Val Acc=0.6673; Loss Att={'forw': '0.2265'}; Train Acc=0.657; Test Acc=0.6552; Entropy={'forw': '8.0820'}; Entropy_Test=\n",
      "\n",
      "0 1.2597599 1.2669641\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:28epoch=120; Loss Pred=1.2922; Val Loss=1.2670; Val Acc=0.6712; Loss Att={'forw': '0.2245'}; Train Acc=0.657; Test Acc=0.6559; Entropy={'forw': '8.0433'}; Entropy_Test=\n",
      "\n",
      "1 1.2597599 1.2570099\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:30epoch=121; Loss Pred=1.2891; Val Loss=1.2570; Val Acc=0.6648; Loss Att={'forw': '0.2238'}; Train Acc=0.657; Test Acc=0.6526; Entropy={'forw': '8.0973'}; Entropy_Test=\n",
      "\n",
      "0 1.2570099 1.2512347\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:32epoch=122; Loss Pred=1.2895; Val Loss=1.2512; Val Acc=0.6677; Loss Att={'forw': '0.2224'}; Train Acc=0.657; Test Acc=0.6545; Entropy={'forw': '8.0840'}; Entropy_Test=\n",
      "\n",
      "0 1.2512347 1.261352\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:34epoch=123; Loss Pred=1.2875; Val Loss=1.2614; Val Acc=0.6688; Loss Att={'forw': '0.2218'}; Train Acc=0.658; Test Acc=0.6530; Entropy={'forw': '8.0684'}; Entropy_Test=\n",
      "\n",
      "1 1.2512347 1.2548434\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:36epoch=124; Loss Pred=1.2868; Val Loss=1.2548; Val Acc=0.6722; Loss Att={'forw': '0.2208'}; Train Acc=0.658; Test Acc=0.6585; Entropy={'forw': '8.0398'}; Entropy_Test=\n",
      "\n",
      "2 1.2512347 1.2464489\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:39epoch=125; Loss Pred=1.2833; Val Loss=1.2464; Val Acc=0.6701; Loss Att={'forw': '0.2194'}; Train Acc=0.658; Test Acc=0.6551; Entropy={'forw': '7.9919'}; Entropy_Test=\n",
      "\n",
      "0 1.2464489 1.261118\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:41epoch=126; Loss Pred=1.2857; Val Loss=1.2611; Val Acc=0.6661; Loss Att={'forw': '0.2190'}; Train Acc=0.658; Test Acc=0.6541; Entropy={'forw': '7.9987'}; Entropy_Test=\n",
      "\n",
      "1 1.2464489 1.2401398\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:43epoch=127; Loss Pred=1.2813; Val Loss=1.2401; Val Acc=0.6704; Loss Att={'forw': '0.2185'}; Train Acc=0.658; Test Acc=0.6547; Entropy={'forw': '7.9926'}; Entropy_Test=\n",
      "\n",
      "0 1.2401398 1.2617121\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:45epoch=128; Loss Pred=1.2819; Val Loss=1.2617; Val Acc=0.6697; Loss Att={'forw': '0.2175'}; Train Acc=0.658; Test Acc=0.6556; Entropy={'forw': '7.9715'}; Entropy_Test=\n",
      "\n",
      "1 1.2401398 1.2455518\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:48epoch=129; Loss Pred=1.2810; Val Loss=1.2456; Val Acc=0.6708; Loss Att={'forw': '0.2169'}; Train Acc=0.659; Test Acc=0.6540; Entropy={'forw': '7.9684'}; Entropy_Test=\n",
      "\n",
      "2 1.2401398 1.255691\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:50epoch=130; Loss Pred=1.2793; Val Loss=1.2557; Val Acc=0.6691; Loss Att={'forw': '0.2160'}; Train Acc=0.659; Test Acc=0.6563; Entropy={'forw': '7.9395'}; Entropy_Test=\n",
      "\n",
      "3 1.2401398 1.2593987\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:52epoch=131; Loss Pred=1.2789; Val Loss=1.2594; Val Acc=0.6746; Loss Att={'forw': '0.2160'}; Train Acc=0.659; Test Acc=0.6554; Entropy={'forw': '7.9398'}; Entropy_Test=\n",
      "\n",
      "4 1.2401398 1.2458558\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:54epoch=132; Loss Pred=1.2787; Val Loss=1.2459; Val Acc=0.6721; Loss Att={'forw': '0.2150'}; Train Acc=0.659; Test Acc=0.6571; Entropy={'forw': '7.9357'}; Entropy_Test=\n",
      "\n",
      "5 1.2401398 1.2584842\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:57epoch=133; Loss Pred=1.2754; Val Loss=1.2585; Val Acc=0.6706; Loss Att={'forw': '0.2145'}; Train Acc=0.660; Test Acc=0.6587; Entropy={'forw': '7.9765'}; Entropy_Test=\n",
      "\n",
      "6 1.2401398 1.2536141\n",
      "Logged Successfully: \n",
      "2018-05-17 19:17:59epoch=134; Loss Pred=1.2760; Val Loss=1.2536; Val Acc=0.6685; Loss Att={'forw': '0.2134'}; Train Acc=0.659; Test Acc=0.6571; Entropy={'forw': '7.9787'}; Entropy_Test=\n",
      "\n",
      "7 1.2401398 1.2484535\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:01epoch=135; Loss Pred=1.2752; Val Loss=1.2485; Val Acc=0.6756; Loss Att={'forw': '0.2134'}; Train Acc=0.660; Test Acc=0.6575; Entropy={'forw': '7.9570'}; Entropy_Test=\n",
      "\n",
      "8 1.2401398 1.2428029\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:03epoch=136; Loss Pred=1.2747; Val Loss=1.2428; Val Acc=0.6715; Loss Att={'forw': '0.2131'}; Train Acc=0.661; Test Acc=0.6600; Entropy={'forw': '7.9731'}; Entropy_Test=\n",
      "\n",
      "9 1.2401398 1.2369391\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:05epoch=137; Loss Pred=1.2729; Val Loss=1.2369; Val Acc=0.6684; Loss Att={'forw': '0.2127'}; Train Acc=0.661; Test Acc=0.6601; Entropy={'forw': '7.9374'}; Entropy_Test=\n",
      "\n",
      "0 1.2369391 1.2357103\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:08epoch=138; Loss Pred=1.2734; Val Loss=1.2357; Val Acc=0.6751; Loss Att={'forw': '0.2120'}; Train Acc=0.661; Test Acc=0.6607; Entropy={'forw': '7.9424'}; Entropy_Test=\n",
      "\n",
      "0 1.2357103 1.238314\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:10epoch=139; Loss Pred=1.2721; Val Loss=1.2383; Val Acc=0.6779; Loss Att={'forw': '0.2119'}; Train Acc=0.662; Test Acc=0.6602; Entropy={'forw': '7.9971'}; Entropy_Test=\n",
      "\n",
      "1 1.2357103 1.2436558\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:12epoch=140; Loss Pred=1.2704; Val Loss=1.2437; Val Acc=0.6731; Loss Att={'forw': '0.2111'}; Train Acc=0.661; Test Acc=0.6572; Entropy={'forw': '7.9927'}; Entropy_Test=\n",
      "\n",
      "2 1.2357103 1.2500576\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:14epoch=141; Loss Pred=1.2698; Val Loss=1.2501; Val Acc=0.6746; Loss Att={'forw': '0.2110'}; Train Acc=0.662; Test Acc=0.6587; Entropy={'forw': '7.9547'}; Entropy_Test=\n",
      "\n",
      "3 1.2357103 1.2445639\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:17epoch=142; Loss Pred=1.2691; Val Loss=1.2446; Val Acc=0.6777; Loss Att={'forw': '0.2105'}; Train Acc=0.662; Test Acc=0.6626; Entropy={'forw': '7.9272'}; Entropy_Test=\n",
      "\n",
      "4 1.2357103 1.2479259\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:19epoch=143; Loss Pred=1.2691; Val Loss=1.2479; Val Acc=0.6732; Loss Att={'forw': '0.2103'}; Train Acc=0.663; Test Acc=0.6618; Entropy={'forw': '7.9621'}; Entropy_Test=\n",
      "\n",
      "5 1.2357103 1.2309239\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:21epoch=144; Loss Pred=1.2675; Val Loss=1.2309; Val Acc=0.6793; Loss Att={'forw': '0.2102'}; Train Acc=0.663; Test Acc=0.6598; Entropy={'forw': '7.9337'}; Entropy_Test=\n",
      "\n",
      "0 1.2309239 1.2402486\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:23epoch=145; Loss Pred=1.2664; Val Loss=1.2402; Val Acc=0.6786; Loss Att={'forw': '0.2098'}; Train Acc=0.663; Test Acc=0.6631; Entropy={'forw': '7.9614'}; Entropy_Test=\n",
      "\n",
      "1 1.2309239 1.250217\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:26epoch=146; Loss Pred=1.2676; Val Loss=1.2502; Val Acc=0.6742; Loss Att={'forw': '0.2094'}; Train Acc=0.663; Test Acc=0.6629; Entropy={'forw': '7.9271'}; Entropy_Test=\n",
      "\n",
      "2 1.2309239 1.2336658\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:28epoch=147; Loss Pred=1.2655; Val Loss=1.2337; Val Acc=0.6779; Loss Att={'forw': '0.2095'}; Train Acc=0.664; Test Acc=0.6635; Entropy={'forw': '7.9280'}; Entropy_Test=\n",
      "\n",
      "3 1.2309239 1.2411928\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:30epoch=148; Loss Pred=1.2648; Val Loss=1.2412; Val Acc=0.6791; Loss Att={'forw': '0.2089'}; Train Acc=0.665; Test Acc=0.6600; Entropy={'forw': '7.9355'}; Entropy_Test=\n",
      "\n",
      "4 1.2309239 1.2259083\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:32epoch=149; Loss Pred=1.2643; Val Loss=1.2259; Val Acc=0.6740; Loss Att={'forw': '0.2088'}; Train Acc=0.664; Test Acc=0.6633; Entropy={'forw': '7.9630'}; Entropy_Test=\n",
      "\n",
      "0 1.2259083 1.2294834\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:34epoch=150; Loss Pred=1.2646; Val Loss=1.2295; Val Acc=0.6786; Loss Att={'forw': '0.2089'}; Train Acc=0.665; Test Acc=0.6619; Entropy={'forw': '7.9617'}; Entropy_Test=\n",
      "\n",
      "1 1.2259083 1.2318901\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:36epoch=151; Loss Pred=1.2630; Val Loss=1.2319; Val Acc=0.6768; Loss Att={'forw': '0.2088'}; Train Acc=0.665; Test Acc=0.6629; Entropy={'forw': '7.9549'}; Entropy_Test=\n",
      "\n",
      "2 1.2259083 1.2388852\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:38epoch=152; Loss Pred=1.2635; Val Loss=1.2389; Val Acc=0.6815; Loss Att={'forw': '0.2082'}; Train Acc=0.664; Test Acc=0.6660; Entropy={'forw': '7.9568'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1.2259083 1.2394836\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:41epoch=153; Loss Pred=1.2622; Val Loss=1.2395; Val Acc=0.6740; Loss Att={'forw': '0.2086'}; Train Acc=0.665; Test Acc=0.6643; Entropy={'forw': '7.9069'}; Entropy_Test=\n",
      "\n",
      "4 1.2259083 1.2400951\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:43epoch=154; Loss Pred=1.2620; Val Loss=1.2401; Val Acc=0.6804; Loss Att={'forw': '0.2086'}; Train Acc=0.665; Test Acc=0.6630; Entropy={'forw': '7.8869'}; Entropy_Test=\n",
      "\n",
      "5 1.2259083 1.2327898\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:45epoch=155; Loss Pred=1.2602; Val Loss=1.2328; Val Acc=0.6785; Loss Att={'forw': '0.2084'}; Train Acc=0.665; Test Acc=0.6636; Entropy={'forw': '7.8834'}; Entropy_Test=\n",
      "\n",
      "6 1.2259083 1.2330201\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:47epoch=156; Loss Pred=1.2605; Val Loss=1.2330; Val Acc=0.6755; Loss Att={'forw': '0.2077'}; Train Acc=0.665; Test Acc=0.6633; Entropy={'forw': '7.8823'}; Entropy_Test=\n",
      "\n",
      "7 1.2259083 1.2236193\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:50epoch=157; Loss Pred=1.2595; Val Loss=1.2236; Val Acc=0.6768; Loss Att={'forw': '0.2075'}; Train Acc=0.665; Test Acc=0.6631; Entropy={'forw': '7.9123'}; Entropy_Test=\n",
      "\n",
      "0 1.2236193 1.2298576\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:52epoch=158; Loss Pred=1.2596; Val Loss=1.2299; Val Acc=0.6799; Loss Att={'forw': '0.2079'}; Train Acc=0.665; Test Acc=0.6621; Entropy={'forw': '7.9240'}; Entropy_Test=\n",
      "\n",
      "1 1.2236193 1.2280796\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:54epoch=159; Loss Pred=1.2584; Val Loss=1.2281; Val Acc=0.6802; Loss Att={'forw': '0.2077'}; Train Acc=0.665; Test Acc=0.6644; Entropy={'forw': '7.9806'}; Entropy_Test=\n",
      "\n",
      "2 1.2236193 1.2337918\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:56epoch=160; Loss Pred=1.2570; Val Loss=1.2338; Val Acc=0.6817; Loss Att={'forw': '0.2078'}; Train Acc=0.666; Test Acc=0.6627; Entropy={'forw': '7.9726'}; Entropy_Test=\n",
      "\n",
      "3 1.2236193 1.2328669\n",
      "Logged Successfully: \n",
      "2018-05-17 19:18:59epoch=161; Loss Pred=1.2558; Val Loss=1.2329; Val Acc=0.6760; Loss Att={'forw': '0.2073'}; Train Acc=0.666; Test Acc=0.6651; Entropy={'forw': '7.9614'}; Entropy_Test=\n",
      "\n",
      "4 1.2236193 1.2312421\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:01epoch=162; Loss Pred=1.2570; Val Loss=1.2312; Val Acc=0.6825; Loss Att={'forw': '0.2073'}; Train Acc=0.666; Test Acc=0.6640; Entropy={'forw': '7.9902'}; Entropy_Test=\n",
      "\n",
      "5 1.2236193 1.2287438\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:03epoch=163; Loss Pred=1.2556; Val Loss=1.2287; Val Acc=0.6878; Loss Att={'forw': '0.2073'}; Train Acc=0.666; Test Acc=0.6642; Entropy={'forw': '7.9847'}; Entropy_Test=\n",
      "\n",
      "6 1.2236193 1.2240705\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:05epoch=164; Loss Pred=1.2575; Val Loss=1.2241; Val Acc=0.6817; Loss Att={'forw': '0.2074'}; Train Acc=0.666; Test Acc=0.6650; Entropy={'forw': '8.0046'}; Entropy_Test=\n",
      "\n",
      "7 1.2236193 1.2340648\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:08epoch=165; Loss Pred=1.2547; Val Loss=1.2341; Val Acc=0.6836; Loss Att={'forw': '0.2071'}; Train Acc=0.666; Test Acc=0.6642; Entropy={'forw': '8.0342'}; Entropy_Test=\n",
      "\n",
      "8 1.2236193 1.2240956\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:10epoch=166; Loss Pred=1.2544; Val Loss=1.2241; Val Acc=0.6808; Loss Att={'forw': '0.2074'}; Train Acc=0.667; Test Acc=0.6648; Entropy={'forw': '8.0200'}; Entropy_Test=\n",
      "\n",
      "9 1.2236193 1.2283278\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:12epoch=167; Loss Pred=1.2532; Val Loss=1.2283; Val Acc=0.6832; Loss Att={'forw': '0.2072'}; Train Acc=0.667; Test Acc=0.6641; Entropy={'forw': '8.0577'}; Entropy_Test=\n",
      "\n",
      "10 1.2236193 1.226251\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:14epoch=168; Loss Pred=1.2546; Val Loss=1.2263; Val Acc=0.6833; Loss Att={'forw': '0.2071'}; Train Acc=0.667; Test Acc=0.6654; Entropy={'forw': '8.0465'}; Entropy_Test=\n",
      "\n",
      "11 1.2236193 1.2251127\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:16epoch=169; Loss Pred=1.2530; Val Loss=1.2251; Val Acc=0.6803; Loss Att={'forw': '0.2069'}; Train Acc=0.667; Test Acc=0.6646; Entropy={'forw': '8.0835'}; Entropy_Test=\n",
      "\n",
      "12 1.2236193 1.2256546\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:18epoch=170; Loss Pred=1.2527; Val Loss=1.2257; Val Acc=0.6777; Loss Att={'forw': '0.2070'}; Train Acc=0.668; Test Acc=0.6633; Entropy={'forw': '8.1030'}; Entropy_Test=\n",
      "\n",
      "13 1.2236193 1.226201\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:21epoch=171; Loss Pred=1.2509; Val Loss=1.2262; Val Acc=0.6868; Loss Att={'forw': '0.2069'}; Train Acc=0.668; Test Acc=0.6653; Entropy={'forw': '8.1002'}; Entropy_Test=\n",
      "\n",
      "14 1.2236193 1.2248086\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:23epoch=172; Loss Pred=1.2527; Val Loss=1.2248; Val Acc=0.6802; Loss Att={'forw': '0.2067'}; Train Acc=0.668; Test Acc=0.6680; Entropy={'forw': '8.0983'}; Entropy_Test=\n",
      "\n",
      "15 1.2236193 1.2125269\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:25epoch=173; Loss Pred=1.2511; Val Loss=1.2125; Val Acc=0.6836; Loss Att={'forw': '0.2067'}; Train Acc=0.668; Test Acc=0.6664; Entropy={'forw': '8.1371'}; Entropy_Test=\n",
      "\n",
      "0 1.2125269 1.2142544\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:27epoch=174; Loss Pred=1.2514; Val Loss=1.2143; Val Acc=0.6851; Loss Att={'forw': '0.2068'}; Train Acc=0.668; Test Acc=0.6683; Entropy={'forw': '8.1300'}; Entropy_Test=\n",
      "\n",
      "1 1.2125269 1.2212285\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:29epoch=175; Loss Pred=1.2504; Val Loss=1.2212; Val Acc=0.6839; Loss Att={'forw': '0.2068'}; Train Acc=0.668; Test Acc=0.6653; Entropy={'forw': '8.1488'}; Entropy_Test=\n",
      "\n",
      "2 1.2125269 1.2111156\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:32epoch=176; Loss Pred=1.2485; Val Loss=1.2111; Val Acc=0.6842; Loss Att={'forw': '0.2070'}; Train Acc=0.669; Test Acc=0.6647; Entropy={'forw': '8.1260'}; Entropy_Test=\n",
      "\n",
      "0 1.2111156 1.2284747\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:34epoch=177; Loss Pred=1.2486; Val Loss=1.2285; Val Acc=0.6834; Loss Att={'forw': '0.2064'}; Train Acc=0.669; Test Acc=0.6640; Entropy={'forw': '8.1219'}; Entropy_Test=\n",
      "\n",
      "1 1.2111156 1.2142974\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:36epoch=178; Loss Pred=1.2487; Val Loss=1.2143; Val Acc=0.6850; Loss Att={'forw': '0.2069'}; Train Acc=0.668; Test Acc=0.6660; Entropy={'forw': '8.1372'}; Entropy_Test=\n",
      "\n",
      "2 1.2111156 1.2136148\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:38epoch=179; Loss Pred=1.2476; Val Loss=1.2136; Val Acc=0.6823; Loss Att={'forw': '0.2067'}; Train Acc=0.668; Test Acc=0.6683; Entropy={'forw': '8.1353'}; Entropy_Test=\n",
      "\n",
      "3 1.2111156 1.2197708\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:40epoch=180; Loss Pred=1.2480; Val Loss=1.2198; Val Acc=0.6825; Loss Att={'forw': '0.2062'}; Train Acc=0.668; Test Acc=0.6683; Entropy={'forw': '8.1270'}; Entropy_Test=\n",
      "\n",
      "4 1.2111156 1.2187489\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:43epoch=181; Loss Pred=1.2466; Val Loss=1.2187; Val Acc=0.6814; Loss Att={'forw': '0.2061'}; Train Acc=0.669; Test Acc=0.6671; Entropy={'forw': '8.1128'}; Entropy_Test=\n",
      "\n",
      "5 1.2111156 1.2032382\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:45epoch=182; Loss Pred=1.2466; Val Loss=1.2032; Val Acc=0.6812; Loss Att={'forw': '0.2066'}; Train Acc=0.669; Test Acc=0.6661; Entropy={'forw': '8.1071'}; Entropy_Test=\n",
      "\n",
      "0 1.2032382 1.2092514\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:47epoch=183; Loss Pred=1.2461; Val Loss=1.2093; Val Acc=0.6817; Loss Att={'forw': '0.2067'}; Train Acc=0.669; Test Acc=0.6668; Entropy={'forw': '8.1095'}; Entropy_Test=\n",
      "\n",
      "1 1.2032382 1.2140834\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:49epoch=184; Loss Pred=1.2467; Val Loss=1.2141; Val Acc=0.6805; Loss Att={'forw': '0.2066'}; Train Acc=0.669; Test Acc=0.6654; Entropy={'forw': '8.0973'}; Entropy_Test=\n",
      "\n",
      "2 1.2032382 1.217651\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:51epoch=185; Loss Pred=1.2447; Val Loss=1.2177; Val Acc=0.6822; Loss Att={'forw': '0.2066'}; Train Acc=0.669; Test Acc=0.6659; Entropy={'forw': '8.0880'}; Entropy_Test=\n",
      "\n",
      "3 1.2032382 1.2117069\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:53epoch=186; Loss Pred=1.2446; Val Loss=1.2117; Val Acc=0.6887; Loss Att={'forw': '0.2065'}; Train Acc=0.669; Test Acc=0.6671; Entropy={'forw': '8.1029'}; Entropy_Test=\n",
      "\n",
      "4 1.2032382 1.2205683\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:56epoch=187; Loss Pred=1.2445; Val Loss=1.2206; Val Acc=0.6857; Loss Att={'forw': '0.2066'}; Train Acc=0.669; Test Acc=0.6671; Entropy={'forw': '8.1018'}; Entropy_Test=\n",
      "\n",
      "5 1.2032382 1.2040861\n",
      "Logged Successfully: \n",
      "2018-05-17 19:19:58epoch=188; Loss Pred=1.2452; Val Loss=1.2041; Val Acc=0.6844; Loss Att={'forw': '0.2062'}; Train Acc=0.669; Test Acc=0.6699; Entropy={'forw': '8.0810'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1.2032382 1.2226559\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:00epoch=189; Loss Pred=1.2441; Val Loss=1.2227; Val Acc=0.6909; Loss Att={'forw': '0.2061'}; Train Acc=0.670; Test Acc=0.6660; Entropy={'forw': '8.1021'}; Entropy_Test=\n",
      "\n",
      "7 1.2032382 1.204002\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:03epoch=190; Loss Pred=1.2434; Val Loss=1.2040; Val Acc=0.6857; Loss Att={'forw': '0.2060'}; Train Acc=0.670; Test Acc=0.6679; Entropy={'forw': '8.1091'}; Entropy_Test=\n",
      "\n",
      "8 1.2032382 1.2025892\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:05epoch=191; Loss Pred=1.2423; Val Loss=1.2026; Val Acc=0.6833; Loss Att={'forw': '0.2059'}; Train Acc=0.670; Test Acc=0.6651; Entropy={'forw': '8.1441'}; Entropy_Test=\n",
      "\n",
      "0 1.2025892 1.2015061\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:07epoch=192; Loss Pred=1.2424; Val Loss=1.2015; Val Acc=0.6879; Loss Att={'forw': '0.2061'}; Train Acc=0.670; Test Acc=0.6682; Entropy={'forw': '8.1644'}; Entropy_Test=\n",
      "\n",
      "0 1.2015061 1.2032636\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:09epoch=193; Loss Pred=1.2412; Val Loss=1.2033; Val Acc=0.6868; Loss Att={'forw': '0.2063'}; Train Acc=0.669; Test Acc=0.6669; Entropy={'forw': '8.1585'}; Entropy_Test=\n",
      "\n",
      "1 1.2015061 1.2029078\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:11epoch=194; Loss Pred=1.2418; Val Loss=1.2029; Val Acc=0.6886; Loss Att={'forw': '0.2057'}; Train Acc=0.670; Test Acc=0.6656; Entropy={'forw': '8.1494'}; Entropy_Test=\n",
      "\n",
      "2 1.2015061 1.2113742\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:14epoch=195; Loss Pred=1.2411; Val Loss=1.2114; Val Acc=0.6879; Loss Att={'forw': '0.2060'}; Train Acc=0.670; Test Acc=0.6677; Entropy={'forw': '8.1513'}; Entropy_Test=\n",
      "\n",
      "3 1.2015061 1.1975013\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:16epoch=196; Loss Pred=1.2408; Val Loss=1.1975; Val Acc=0.6892; Loss Att={'forw': '0.2062'}; Train Acc=0.670; Test Acc=0.6669; Entropy={'forw': '8.1566'}; Entropy_Test=\n",
      "\n",
      "0 1.1975013 1.2132154\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:18epoch=197; Loss Pred=1.2406; Val Loss=1.2132; Val Acc=0.6874; Loss Att={'forw': '0.2061'}; Train Acc=0.669; Test Acc=0.6693; Entropy={'forw': '8.1447'}; Entropy_Test=\n",
      "\n",
      "1 1.1975013 1.1988957\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:20epoch=198; Loss Pred=1.2401; Val Loss=1.1989; Val Acc=0.6876; Loss Att={'forw': '0.2057'}; Train Acc=0.669; Test Acc=0.6684; Entropy={'forw': '8.1562'}; Entropy_Test=\n",
      "\n",
      "2 1.1975013 1.2023326\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:22epoch=199; Loss Pred=1.2386; Val Loss=1.2023; Val Acc=0.6912; Loss Att={'forw': '0.2060'}; Train Acc=0.670; Test Acc=0.6673; Entropy={'forw': '8.1662'}; Entropy_Test=\n",
      "\n",
      "3 1.1975013 1.1989557\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:25epoch=200; Loss Pred=1.2393; Val Loss=1.1990; Val Acc=0.6818; Loss Att={'forw': '0.2055'}; Train Acc=0.669; Test Acc=0.6685; Entropy={'forw': '8.1693'}; Entropy_Test=\n",
      "\n",
      "4 1.1975013 1.208917\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:27epoch=201; Loss Pred=1.2388; Val Loss=1.2089; Val Acc=0.6809; Loss Att={'forw': '0.2055'}; Train Acc=0.669; Test Acc=0.6679; Entropy={'forw': '8.1828'}; Entropy_Test=\n",
      "\n",
      "5 1.1975013 1.1977084\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:29epoch=202; Loss Pred=1.2397; Val Loss=1.1977; Val Acc=0.6905; Loss Att={'forw': '0.2057'}; Train Acc=0.669; Test Acc=0.6675; Entropy={'forw': '8.1915'}; Entropy_Test=\n",
      "\n",
      "6 1.1975013 1.2044834\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:31epoch=203; Loss Pred=1.2377; Val Loss=1.2045; Val Acc=0.6841; Loss Att={'forw': '0.2056'}; Train Acc=0.670; Test Acc=0.6669; Entropy={'forw': '8.1686'}; Entropy_Test=\n",
      "\n",
      "7 1.1975013 1.1926352\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:33epoch=204; Loss Pred=1.2393; Val Loss=1.1926; Val Acc=0.6888; Loss Att={'forw': '0.2056'}; Train Acc=0.670; Test Acc=0.6658; Entropy={'forw': '8.1499'}; Entropy_Test=\n",
      "\n",
      "0 1.1926352 1.1997211\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:36epoch=205; Loss Pred=1.2382; Val Loss=1.1997; Val Acc=0.6812; Loss Att={'forw': '0.2060'}; Train Acc=0.669; Test Acc=0.6672; Entropy={'forw': '8.1594'}; Entropy_Test=\n",
      "\n",
      "1 1.1926352 1.2042081\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:38epoch=206; Loss Pred=1.2376; Val Loss=1.2042; Val Acc=0.6885; Loss Att={'forw': '0.2054'}; Train Acc=0.670; Test Acc=0.6678; Entropy={'forw': '8.1699'}; Entropy_Test=\n",
      "\n",
      "2 1.1926352 1.2071809\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:40epoch=207; Loss Pred=1.2360; Val Loss=1.2072; Val Acc=0.6843; Loss Att={'forw': '0.2054'}; Train Acc=0.670; Test Acc=0.6688; Entropy={'forw': '8.1935'}; Entropy_Test=\n",
      "\n",
      "3 1.1926352 1.1935635\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:42epoch=208; Loss Pred=1.2368; Val Loss=1.1936; Val Acc=0.6869; Loss Att={'forw': '0.2055'}; Train Acc=0.670; Test Acc=0.6651; Entropy={'forw': '8.2188'}; Entropy_Test=\n",
      "\n",
      "4 1.1926352 1.2109449\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:44epoch=209; Loss Pred=1.2356; Val Loss=1.2109; Val Acc=0.6818; Loss Att={'forw': '0.2053'}; Train Acc=0.670; Test Acc=0.6680; Entropy={'forw': '8.2292'}; Entropy_Test=\n",
      "\n",
      "5 1.1926352 1.2040753\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:46epoch=210; Loss Pred=1.2358; Val Loss=1.2041; Val Acc=0.6916; Loss Att={'forw': '0.2054'}; Train Acc=0.670; Test Acc=0.6692; Entropy={'forw': '8.2034'}; Entropy_Test=\n",
      "\n",
      "6 1.1926352 1.210044\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:49epoch=211; Loss Pred=1.2359; Val Loss=1.2100; Val Acc=0.6835; Loss Att={'forw': '0.2055'}; Train Acc=0.670; Test Acc=0.6653; Entropy={'forw': '8.1903'}; Entropy_Test=\n",
      "\n",
      "7 1.1926352 1.1973178\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:51epoch=212; Loss Pred=1.2357; Val Loss=1.1973; Val Acc=0.6866; Loss Att={'forw': '0.2053'}; Train Acc=0.670; Test Acc=0.6665; Entropy={'forw': '8.2134'}; Entropy_Test=\n",
      "\n",
      "8 1.1926352 1.1988288\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:53epoch=213; Loss Pred=1.2344; Val Loss=1.1988; Val Acc=0.6819; Loss Att={'forw': '0.2052'}; Train Acc=0.670; Test Acc=0.6691; Entropy={'forw': '8.1933'}; Entropy_Test=\n",
      "\n",
      "9 1.1926352 1.1940842\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:55epoch=214; Loss Pred=1.2350; Val Loss=1.1941; Val Acc=0.6827; Loss Att={'forw': '0.2049'}; Train Acc=0.669; Test Acc=0.6675; Entropy={'forw': '8.1748'}; Entropy_Test=\n",
      "\n",
      "10 1.1926352 1.2026006\n",
      "Logged Successfully: \n",
      "2018-05-17 19:20:58epoch=215; Loss Pred=1.2344; Val Loss=1.2026; Val Acc=0.6851; Loss Att={'forw': '0.2048'}; Train Acc=0.669; Test Acc=0.6648; Entropy={'forw': '8.1863'}; Entropy_Test=\n",
      "\n",
      "11 1.1926352 1.1979396\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:00epoch=216; Loss Pred=1.2346; Val Loss=1.1979; Val Acc=0.6886; Loss Att={'forw': '0.2048'}; Train Acc=0.671; Test Acc=0.6679; Entropy={'forw': '8.1781'}; Entropy_Test=\n",
      "\n",
      "12 1.1926352 1.2123286\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:02epoch=217; Loss Pred=1.2334; Val Loss=1.2123; Val Acc=0.6841; Loss Att={'forw': '0.2045'}; Train Acc=0.670; Test Acc=0.6688; Entropy={'forw': '8.1553'}; Entropy_Test=\n",
      "\n",
      "13 1.1926352 1.2030925\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:04epoch=218; Loss Pred=1.2341; Val Loss=1.2031; Val Acc=0.6811; Loss Att={'forw': '0.2044'}; Train Acc=0.670; Test Acc=0.6672; Entropy={'forw': '8.1758'}; Entropy_Test=\n",
      "\n",
      "14 1.1926352 1.207748\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:06epoch=219; Loss Pred=1.2326; Val Loss=1.2077; Val Acc=0.6840; Loss Att={'forw': '0.2046'}; Train Acc=0.670; Test Acc=0.6686; Entropy={'forw': '8.1993'}; Entropy_Test=\n",
      "\n",
      "15 1.1926352 1.2011341\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:08epoch=220; Loss Pred=1.2334; Val Loss=1.2011; Val Acc=0.6900; Loss Att={'forw': '0.2042'}; Train Acc=0.670; Test Acc=0.6671; Entropy={'forw': '8.2202'}; Entropy_Test=\n",
      "\n",
      "16 1.1926352 1.1910586\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:11epoch=221; Loss Pred=1.2324; Val Loss=1.1911; Val Acc=0.6848; Loss Att={'forw': '0.2039'}; Train Acc=0.670; Test Acc=0.6664; Entropy={'forw': '8.2412'}; Entropy_Test=\n",
      "\n",
      "0 1.1910586 1.192044\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:13epoch=222; Loss Pred=1.2322; Val Loss=1.1920; Val Acc=0.6816; Loss Att={'forw': '0.2040'}; Train Acc=0.670; Test Acc=0.6678; Entropy={'forw': '8.2176'}; Entropy_Test=\n",
      "\n",
      "1 1.1910586 1.1944807\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:15epoch=223; Loss Pred=1.2314; Val Loss=1.1945; Val Acc=0.6900; Loss Att={'forw': '0.2039'}; Train Acc=0.670; Test Acc=0.6683; Entropy={'forw': '8.2655'}; Entropy_Test=\n",
      "\n",
      "2 1.1910586 1.2040402\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:17epoch=224; Loss Pred=1.2303; Val Loss=1.2040; Val Acc=0.6840; Loss Att={'forw': '0.2039'}; Train Acc=0.670; Test Acc=0.6664; Entropy={'forw': '8.2623'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1.1910586 1.2059202\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:19epoch=225; Loss Pred=1.2319; Val Loss=1.2059; Val Acc=0.6850; Loss Att={'forw': '0.2038'}; Train Acc=0.670; Test Acc=0.6691; Entropy={'forw': '8.3114'}; Entropy_Test=\n",
      "\n",
      "4 1.1910586 1.1946644\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:22epoch=226; Loss Pred=1.2326; Val Loss=1.1947; Val Acc=0.6837; Loss Att={'forw': '0.2039'}; Train Acc=0.670; Test Acc=0.6701; Entropy={'forw': '8.3187'}; Entropy_Test=\n",
      "\n",
      "5 1.1910586 1.203246\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:24epoch=227; Loss Pred=1.2300; Val Loss=1.2032; Val Acc=0.6833; Loss Att={'forw': '0.2032'}; Train Acc=0.670; Test Acc=0.6670; Entropy={'forw': '8.3148'}; Entropy_Test=\n",
      "\n",
      "6 1.1910586 1.2026118\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:26epoch=228; Loss Pred=1.2317; Val Loss=1.2026; Val Acc=0.6829; Loss Att={'forw': '0.2034'}; Train Acc=0.670; Test Acc=0.6687; Entropy={'forw': '8.3185'}; Entropy_Test=\n",
      "\n",
      "7 1.1910586 1.1984127\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:28epoch=229; Loss Pred=1.2295; Val Loss=1.1984; Val Acc=0.6879; Loss Att={'forw': '0.2036'}; Train Acc=0.670; Test Acc=0.6687; Entropy={'forw': '8.3867'}; Entropy_Test=\n",
      "\n",
      "8 1.1910586 1.2036766\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:30epoch=230; Loss Pred=1.2306; Val Loss=1.2037; Val Acc=0.6853; Loss Att={'forw': '0.2035'}; Train Acc=0.670; Test Acc=0.6657; Entropy={'forw': '8.3396'}; Entropy_Test=\n",
      "\n",
      "9 1.1910586 1.2035702\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:33epoch=231; Loss Pred=1.2305; Val Loss=1.2036; Val Acc=0.6830; Loss Att={'forw': '0.2029'}; Train Acc=0.670; Test Acc=0.6688; Entropy={'forw': '8.3995'}; Entropy_Test=\n",
      "\n",
      "10 1.1910586 1.2006375\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:35epoch=232; Loss Pred=1.2312; Val Loss=1.2006; Val Acc=0.6850; Loss Att={'forw': '0.2031'}; Train Acc=0.670; Test Acc=0.6687; Entropy={'forw': '8.3702'}; Entropy_Test=\n",
      "\n",
      "11 1.1910586 1.1923755\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:37epoch=233; Loss Pred=1.2293; Val Loss=1.1924; Val Acc=0.6859; Loss Att={'forw': '0.2027'}; Train Acc=0.670; Test Acc=0.6690; Entropy={'forw': '8.3698'}; Entropy_Test=\n",
      "\n",
      "12 1.1910586 1.201141\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:39epoch=234; Loss Pred=1.2304; Val Loss=1.2011; Val Acc=0.6873; Loss Att={'forw': '0.2026'}; Train Acc=0.670; Test Acc=0.6668; Entropy={'forw': '8.3848'}; Entropy_Test=\n",
      "\n",
      "13 1.1910586 1.2012591\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:41epoch=235; Loss Pred=1.2292; Val Loss=1.2013; Val Acc=0.6797; Loss Att={'forw': '0.2026'}; Train Acc=0.670; Test Acc=0.6663; Entropy={'forw': '8.3666'}; Entropy_Test=\n",
      "\n",
      "14 1.1910586 1.1882935\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:43epoch=236; Loss Pred=1.2287; Val Loss=1.1883; Val Acc=0.6809; Loss Att={'forw': '0.2021'}; Train Acc=0.671; Test Acc=0.6672; Entropy={'forw': '8.3313'}; Entropy_Test=\n",
      "\n",
      "0 1.1882935 1.197979\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:46epoch=237; Loss Pred=1.2289; Val Loss=1.1980; Val Acc=0.6821; Loss Att={'forw': '0.2020'}; Train Acc=0.671; Test Acc=0.6680; Entropy={'forw': '8.3414'}; Entropy_Test=\n",
      "\n",
      "1 1.1882935 1.2024302\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:48epoch=238; Loss Pred=1.2291; Val Loss=1.2024; Val Acc=0.6871; Loss Att={'forw': '0.2016'}; Train Acc=0.670; Test Acc=0.6691; Entropy={'forw': '8.2851'}; Entropy_Test=\n",
      "\n",
      "2 1.1882935 1.1918759\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:50epoch=239; Loss Pred=1.2282; Val Loss=1.1919; Val Acc=0.6866; Loss Att={'forw': '0.2014'}; Train Acc=0.669; Test Acc=0.6702; Entropy={'forw': '8.2929'}; Entropy_Test=\n",
      "\n",
      "3 1.1882935 1.1938238\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:52epoch=240; Loss Pred=1.2294; Val Loss=1.1938; Val Acc=0.6882; Loss Att={'forw': '0.2011'}; Train Acc=0.670; Test Acc=0.6683; Entropy={'forw': '8.2745'}; Entropy_Test=\n",
      "\n",
      "4 1.1882935 1.1925309\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:54epoch=241; Loss Pred=1.2280; Val Loss=1.1925; Val Acc=0.6843; Loss Att={'forw': '0.2012'}; Train Acc=0.670; Test Acc=0.6699; Entropy={'forw': '8.2727'}; Entropy_Test=\n",
      "\n",
      "5 1.1882935 1.1932803\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:56epoch=242; Loss Pred=1.2280; Val Loss=1.1933; Val Acc=0.6846; Loss Att={'forw': '0.2004'}; Train Acc=0.671; Test Acc=0.6689; Entropy={'forw': '8.2642'}; Entropy_Test=\n",
      "\n",
      "6 1.1882935 1.1969709\n",
      "Logged Successfully: \n",
      "2018-05-17 19:21:59epoch=243; Loss Pred=1.2272; Val Loss=1.1970; Val Acc=0.6852; Loss Att={'forw': '0.2006'}; Train Acc=0.670; Test Acc=0.6676; Entropy={'forw': '8.2989'}; Entropy_Test=\n",
      "\n",
      "7 1.1882935 1.1978828\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:01epoch=244; Loss Pred=1.2287; Val Loss=1.1979; Val Acc=0.6836; Loss Att={'forw': '0.1998'}; Train Acc=0.671; Test Acc=0.6685; Entropy={'forw': '8.2992'}; Entropy_Test=\n",
      "\n",
      "8 1.1882935 1.2042377\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:03epoch=245; Loss Pred=1.2289; Val Loss=1.2042; Val Acc=0.6862; Loss Att={'forw': '0.1999'}; Train Acc=0.670; Test Acc=0.6677; Entropy={'forw': '8.3232'}; Entropy_Test=\n",
      "\n",
      "9 1.1882935 1.2052084\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:05epoch=246; Loss Pred=1.2280; Val Loss=1.2052; Val Acc=0.6843; Loss Att={'forw': '0.1991'}; Train Acc=0.671; Test Acc=0.6700; Entropy={'forw': '8.2735'}; Entropy_Test=\n",
      "\n",
      "10 1.1882935 1.1982007\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:08epoch=247; Loss Pred=1.2288; Val Loss=1.1982; Val Acc=0.6840; Loss Att={'forw': '0.1989'}; Train Acc=0.671; Test Acc=0.6683; Entropy={'forw': '8.3083'}; Entropy_Test=\n",
      "\n",
      "11 1.1882935 1.1850512\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:10epoch=248; Loss Pred=1.2289; Val Loss=1.1851; Val Acc=0.6854; Loss Att={'forw': '0.1986'}; Train Acc=0.671; Test Acc=0.6694; Entropy={'forw': '8.2711'}; Entropy_Test=\n",
      "\n",
      "0 1.1850512 1.2005535\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:12epoch=249; Loss Pred=1.2274; Val Loss=1.2006; Val Acc=0.6889; Loss Att={'forw': '0.1988'}; Train Acc=0.671; Test Acc=0.6686; Entropy={'forw': '8.2352'}; Entropy_Test=\n",
      "\n",
      "1 1.1850512 1.1868191\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:14epoch=250; Loss Pred=1.2282; Val Loss=1.1868; Val Acc=0.6824; Loss Att={'forw': '0.1982'}; Train Acc=0.671; Test Acc=0.6691; Entropy={'forw': '8.2345'}; Entropy_Test=\n",
      "\n",
      "2 1.1850512 1.1929849\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:16epoch=251; Loss Pred=1.2278; Val Loss=1.1930; Val Acc=0.6827; Loss Att={'forw': '0.1977'}; Train Acc=0.670; Test Acc=0.6703; Entropy={'forw': '8.2462'}; Entropy_Test=\n",
      "\n",
      "3 1.1850512 1.2005805\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:18epoch=252; Loss Pred=1.2266; Val Loss=1.2006; Val Acc=0.6807; Loss Att={'forw': '0.1972'}; Train Acc=0.671; Test Acc=0.6688; Entropy={'forw': '8.2492'}; Entropy_Test=\n",
      "\n",
      "4 1.1850512 1.1972535\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:21epoch=253; Loss Pred=1.2260; Val Loss=1.1973; Val Acc=0.6811; Loss Att={'forw': '0.1976'}; Train Acc=0.671; Test Acc=0.6668; Entropy={'forw': '8.3201'}; Entropy_Test=\n",
      "\n",
      "5 1.1850512 1.2055088\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:23epoch=254; Loss Pred=1.2273; Val Loss=1.2055; Val Acc=0.6908; Loss Att={'forw': '0.1970'}; Train Acc=0.670; Test Acc=0.6672; Entropy={'forw': '8.3427'}; Entropy_Test=\n",
      "\n",
      "6 1.1850512 1.1958288\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:25epoch=255; Loss Pred=1.2271; Val Loss=1.1958; Val Acc=0.6846; Loss Att={'forw': '0.1971'}; Train Acc=0.670; Test Acc=0.6659; Entropy={'forw': '8.3221'}; Entropy_Test=\n",
      "\n",
      "7 1.1850512 1.1859784\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:27epoch=256; Loss Pred=1.2268; Val Loss=1.1860; Val Acc=0.6834; Loss Att={'forw': '0.1967'}; Train Acc=0.670; Test Acc=0.6687; Entropy={'forw': '8.3095'}; Entropy_Test=\n",
      "\n",
      "8 1.1850512 1.1922271\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:29epoch=257; Loss Pred=1.2261; Val Loss=1.1922; Val Acc=0.6882; Loss Att={'forw': '0.1965'}; Train Acc=0.670; Test Acc=0.6677; Entropy={'forw': '8.3029'}; Entropy_Test=\n",
      "\n",
      "9 1.1850512 1.1946421\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:31epoch=258; Loss Pred=1.2261; Val Loss=1.1946; Val Acc=0.6836; Loss Att={'forw': '0.1963'}; Train Acc=0.671; Test Acc=0.6683; Entropy={'forw': '8.3122'}; Entropy_Test=\n",
      "\n",
      "10 1.1850512 1.1956447\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:34epoch=259; Loss Pred=1.2260; Val Loss=1.1956; Val Acc=0.6817; Loss Att={'forw': '0.1963'}; Train Acc=0.671; Test Acc=0.6678; Entropy={'forw': '8.3508'}; Entropy_Test=\n",
      "\n",
      "11 1.1850512 1.1940573\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:36epoch=260; Loss Pred=1.2264; Val Loss=1.1941; Val Acc=0.6839; Loss Att={'forw': '0.1958'}; Train Acc=0.671; Test Acc=0.6710; Entropy={'forw': '8.3387'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 1.1850512 1.1846607\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:38epoch=261; Loss Pred=1.2266; Val Loss=1.1847; Val Acc=0.6844; Loss Att={'forw': '0.1960'}; Train Acc=0.670; Test Acc=0.6701; Entropy={'forw': '8.3726'}; Entropy_Test=\n",
      "\n",
      "0 1.1846607 1.1886638\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:40epoch=262; Loss Pred=1.2281; Val Loss=1.1887; Val Acc=0.6888; Loss Att={'forw': '0.1953'}; Train Acc=0.671; Test Acc=0.6703; Entropy={'forw': '8.3401'}; Entropy_Test=\n",
      "\n",
      "1 1.1846607 1.1919572\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:42epoch=263; Loss Pred=1.2272; Val Loss=1.1920; Val Acc=0.6838; Loss Att={'forw': '0.1957'}; Train Acc=0.671; Test Acc=0.6689; Entropy={'forw': '8.3292'}; Entropy_Test=\n",
      "\n",
      "2 1.1846607 1.198666\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:44epoch=264; Loss Pred=1.2259; Val Loss=1.1987; Val Acc=0.6834; Loss Att={'forw': '0.1948'}; Train Acc=0.671; Test Acc=0.6695; Entropy={'forw': '8.3667'}; Entropy_Test=\n",
      "\n",
      "3 1.1846607 1.187403\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:47epoch=265; Loss Pred=1.2251; Val Loss=1.1874; Val Acc=0.6846; Loss Att={'forw': '0.1947'}; Train Acc=0.671; Test Acc=0.6695; Entropy={'forw': '8.3395'}; Entropy_Test=\n",
      "\n",
      "4 1.1846607 1.2014943\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:49epoch=266; Loss Pred=1.2249; Val Loss=1.2015; Val Acc=0.6818; Loss Att={'forw': '0.1944'}; Train Acc=0.671; Test Acc=0.6694; Entropy={'forw': '8.3783'}; Entropy_Test=\n",
      "\n",
      "5 1.1846607 1.1990242\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:51epoch=267; Loss Pred=1.2267; Val Loss=1.1990; Val Acc=0.6831; Loss Att={'forw': '0.1940'}; Train Acc=0.671; Test Acc=0.6694; Entropy={'forw': '8.3909'}; Entropy_Test=\n",
      "\n",
      "6 1.1846607 1.2005848\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:53epoch=268; Loss Pred=1.2269; Val Loss=1.2006; Val Acc=0.6863; Loss Att={'forw': '0.1937'}; Train Acc=0.671; Test Acc=0.6682; Entropy={'forw': '8.3786'}; Entropy_Test=\n",
      "\n",
      "7 1.1846607 1.2060845\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:55epoch=269; Loss Pred=1.2240; Val Loss=1.2061; Val Acc=0.6836; Loss Att={'forw': '0.1933'}; Train Acc=0.671; Test Acc=0.6676; Entropy={'forw': '8.4002'}; Entropy_Test=\n",
      "\n",
      "8 1.1846607 1.2018254\n",
      "Logged Successfully: \n",
      "2018-05-17 19:22:58epoch=270; Loss Pred=1.2244; Val Loss=1.2018; Val Acc=0.6829; Loss Att={'forw': '0.1927'}; Train Acc=0.671; Test Acc=0.6701; Entropy={'forw': '8.3299'}; Entropy_Test=\n",
      "\n",
      "9 1.1846607 1.1953768\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:00epoch=271; Loss Pred=1.2239; Val Loss=1.1954; Val Acc=0.6846; Loss Att={'forw': '0.1921'}; Train Acc=0.672; Test Acc=0.6693; Entropy={'forw': '8.3186'}; Entropy_Test=\n",
      "\n",
      "10 1.1846607 1.1886612\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:02epoch=272; Loss Pred=1.2248; Val Loss=1.1887; Val Acc=0.6884; Loss Att={'forw': '0.1915'}; Train Acc=0.672; Test Acc=0.6682; Entropy={'forw': '8.2738'}; Entropy_Test=\n",
      "\n",
      "11 1.1846607 1.1909966\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:04epoch=273; Loss Pred=1.2262; Val Loss=1.1910; Val Acc=0.6874; Loss Att={'forw': '0.1915'}; Train Acc=0.672; Test Acc=0.6702; Entropy={'forw': '8.2410'}; Entropy_Test=\n",
      "\n",
      "12 1.1846607 1.1876694\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:06epoch=274; Loss Pred=1.2282; Val Loss=1.1877; Val Acc=0.6836; Loss Att={'forw': '0.1901'}; Train Acc=0.671; Test Acc=0.6686; Entropy={'forw': '8.2090'}; Entropy_Test=\n",
      "\n",
      "13 1.1846607 1.1909765\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:09epoch=275; Loss Pred=1.2287; Val Loss=1.1910; Val Acc=0.6838; Loss Att={'forw': '0.1898'}; Train Acc=0.671; Test Acc=0.6687; Entropy={'forw': '8.1453'}; Entropy_Test=\n",
      "\n",
      "14 1.1846607 1.1922532\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:11epoch=276; Loss Pred=1.2307; Val Loss=1.1923; Val Acc=0.6847; Loss Att={'forw': '0.1897'}; Train Acc=0.671; Test Acc=0.6694; Entropy={'forw': '8.0613'}; Entropy_Test=\n",
      "\n",
      "15 1.1846607 1.1883085\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:13epoch=277; Loss Pred=1.2282; Val Loss=1.1883; Val Acc=0.6896; Loss Att={'forw': '0.1888'}; Train Acc=0.672; Test Acc=0.6663; Entropy={'forw': '8.0864'}; Entropy_Test=\n",
      "\n",
      "16 1.1846607 1.203114\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:15epoch=278; Loss Pred=1.2296; Val Loss=1.2031; Val Acc=0.6845; Loss Att={'forw': '0.1886'}; Train Acc=0.671; Test Acc=0.6708; Entropy={'forw': '8.0844'}; Entropy_Test=\n",
      "\n",
      "17 1.1846607 1.195999\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:17epoch=279; Loss Pred=1.2287; Val Loss=1.1960; Val Acc=0.6856; Loss Att={'forw': '0.1882'}; Train Acc=0.671; Test Acc=0.6682; Entropy={'forw': '8.0477'}; Entropy_Test=\n",
      "\n",
      "18 1.1846607 1.1985925\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:19epoch=280; Loss Pred=1.2313; Val Loss=1.1986; Val Acc=0.6865; Loss Att={'forw': '0.1877'}; Train Acc=0.671; Test Acc=0.6681; Entropy={'forw': '7.9915'}; Entropy_Test=\n",
      "\n",
      "19 1.1846607 1.2110044\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:22epoch=281; Loss Pred=1.2323; Val Loss=1.2110; Val Acc=0.6881; Loss Att={'forw': '0.1877'}; Train Acc=0.670; Test Acc=0.6685; Entropy={'forw': '8.1047'}; Entropy_Test=\n",
      "\n",
      "20 1.1846607 1.2118945\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 283\n",
      "Optimization Finished!\n",
      "Saved Results Successfully\n",
      "L2 reg-n\n",
      "********** replication  0  **********\n",
      "msnbc\n",
      "(7305, 40) (7305, 40) (74, 40) (74, 40)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tGRU bidir(False), task: msnbc\n",
      "    hid: \t\t\t50,\n",
      "    h_hid: \t\t\t100\n",
      "    n_attractor_iterations: \t0,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t7305/0, SEQ_LEN: 40\n",
      "Logged Successfully: \n",
      "0 10000000000.0 2.89237\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:28epoch=0; Loss Pred=2.8849; Val Loss=2.8924; Val Acc=0.0900; Loss Att={'forw': '1.0000'}; Train Acc=0.122; Test Acc=0.1215; Entropy={'forw': '6.7804'}; Entropy_Test=\n",
      "\n",
      "0 2.89237 2.8250062\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:29epoch=1; Loss Pred=2.8179; Val Loss=2.8250; Val Acc=0.1501; Loss Att={'forw': '1.0000'}; Train Acc=0.181; Test Acc=0.1782; Entropy={'forw': '6.8176'}; Entropy_Test=\n",
      "\n",
      "0 2.8250062 2.8233404\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:31epoch=2; Loss Pred=2.8179; Val Loss=2.8233; Val Acc=0.1498; Loss Att={'forw': '1.0000'}; Train Acc=0.182; Test Acc=0.1769; Entropy={'forw': '6.8176'}; Entropy_Test=\n",
      "\n",
      "0 2.8233404 2.7547174\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:32epoch=3; Loss Pred=2.7524; Val Loss=2.7547; Val Acc=0.1894; Loss Att={'forw': '1.0000'}; Train Acc=0.214; Test Acc=0.2109; Entropy={'forw': '6.8562'}; Entropy_Test=\n",
      "\n",
      "0 2.7547174 2.7550201\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:34epoch=4; Loss Pred=2.7523; Val Loss=2.7550; Val Acc=0.1897; Loss Att={'forw': '1.0000'}; Train Acc=0.215; Test Acc=0.2119; Entropy={'forw': '6.8562'}; Entropy_Test=\n",
      "\n",
      "1 2.7547174 2.6810613\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:35epoch=5; Loss Pred=2.6833; Val Loss=2.6811; Val Acc=0.2203; Loss Att={'forw': '1.0000'}; Train Acc=0.239; Test Acc=0.2365; Entropy={'forw': '7.1137'}; Entropy_Test=\n",
      "\n",
      "0 2.6810613 2.681228\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:37epoch=6; Loss Pred=2.6832; Val Loss=2.6812; Val Acc=0.2161; Loss Att={'forw': '1.0000'}; Train Acc=0.239; Test Acc=0.2351; Entropy={'forw': '7.1137'}; Entropy_Test=\n",
      "\n",
      "1 2.6810613 2.603401\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:38epoch=7; Loss Pred=2.6076; Val Loss=2.6034; Val Acc=0.2499; Loss Att={'forw': '1.0000'}; Train Acc=0.262; Test Acc=0.2554; Entropy={'forw': '7.2801'}; Entropy_Test=\n",
      "\n",
      "0 2.603401 2.60178\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:40epoch=8; Loss Pred=2.6077; Val Loss=2.6018; Val Acc=0.2491; Loss Att={'forw': '1.0000'}; Train Acc=0.263; Test Acc=0.2553; Entropy={'forw': '7.2801'}; Entropy_Test=\n",
      "\n",
      "0 2.60178 2.5132565\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:41epoch=9; Loss Pred=2.5224; Val Loss=2.5133; Val Acc=0.2713; Loss Att={'forw': '1.0000'}; Train Acc=0.283; Test Acc=0.2786; Entropy={'forw': '7.5106'}; Entropy_Test=\n",
      "\n",
      "0 2.5132565 2.5146706\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:42epoch=10; Loss Pred=2.5226; Val Loss=2.5147; Val Acc=0.2738; Loss Att={'forw': '1.0000'}; Train Acc=0.283; Test Acc=0.2743; Entropy={'forw': '7.5106'}; Entropy_Test=\n",
      "\n",
      "1 2.5132565 2.4140093\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:44epoch=11; Loss Pred=2.4273; Val Loss=2.4140; Val Acc=0.2743; Loss Att={'forw': '1.0000'}; Train Acc=0.287; Test Acc=0.2817; Entropy={'forw': '7.8468'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.4140093 2.4121416\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:45epoch=12; Loss Pred=2.4269; Val Loss=2.4121; Val Acc=0.2729; Loss Att={'forw': '1.0000'}; Train Acc=0.287; Test Acc=0.2806; Entropy={'forw': '7.8468'}; Entropy_Test=\n",
      "\n",
      "0 2.4121416 2.317695\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:47epoch=13; Loss Pred=2.3352; Val Loss=2.3177; Val Acc=0.2566; Loss Att={'forw': '1.0000'}; Train Acc=0.276; Test Acc=0.2697; Entropy={'forw': '8.3363'}; Entropy_Test=\n",
      "\n",
      "0 2.317695 2.3175666\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:48epoch=14; Loss Pred=2.3352; Val Loss=2.3176; Val Acc=0.2599; Loss Att={'forw': '1.0000'}; Train Acc=0.276; Test Acc=0.2695; Entropy={'forw': '8.3363'}; Entropy_Test=\n",
      "\n",
      "0 2.3175666 2.2711031\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:49epoch=15; Loss Pred=2.2899; Val Loss=2.2711; Val Acc=0.2508; Loss Att={'forw': '1.0000'}; Train Acc=0.274; Test Acc=0.2681; Entropy={'forw': '8.6055'}; Entropy_Test=\n",
      "\n",
      "0 2.2711031 2.2741215\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:50epoch=16; Loss Pred=2.2898; Val Loss=2.2741; Val Acc=0.2503; Loss Att={'forw': '1.0000'}; Train Acc=0.275; Test Acc=0.2685; Entropy={'forw': '8.6055'}; Entropy_Test=\n",
      "\n",
      "1 2.2711031 2.197465\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:52epoch=17; Loss Pred=2.2210; Val Loss=2.1975; Val Acc=0.2846; Loss Att={'forw': '1.0000'}; Train Acc=0.305; Test Acc=0.2990; Entropy={'forw': '8.6569'}; Entropy_Test=\n",
      "\n",
      "0 2.197465 2.1946418\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:53epoch=18; Loss Pred=2.2214; Val Loss=2.1946; Val Acc=0.2956; Loss Att={'forw': '1.0000'}; Train Acc=0.305; Test Acc=0.2948; Entropy={'forw': '8.6569'}; Entropy_Test=\n",
      "\n",
      "0 2.1946418 2.1053338\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:54epoch=19; Loss Pred=2.1290; Val Loss=2.1053; Val Acc=0.3839; Loss Att={'forw': '1.0000'}; Train Acc=0.382; Test Acc=0.3770; Entropy={'forw': '8.8204'}; Entropy_Test=\n",
      "\n",
      "0 2.1053338 2.1040764\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:56epoch=20; Loss Pred=2.1297; Val Loss=2.1041; Val Acc=0.3776; Loss Att={'forw': '1.0000'}; Train Acc=0.381; Test Acc=0.3732; Entropy={'forw': '8.8204'}; Entropy_Test=\n",
      "\n",
      "0 2.1040764 2.0249264\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:57epoch=21; Loss Pred=2.0562; Val Loss=2.0249; Val Acc=0.4673; Loss Att={'forw': '1.0000'}; Train Acc=0.453; Test Acc=0.4471; Entropy={'forw': '8.9500'}; Entropy_Test=\n",
      "\n",
      "0 2.0249264 2.0234947\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:58epoch=22; Loss Pred=2.0557; Val Loss=2.0235; Val Acc=0.4688; Loss Att={'forw': '1.0000'}; Train Acc=0.454; Test Acc=0.4475; Entropy={'forw': '8.9500'}; Entropy_Test=\n",
      "\n",
      "0 2.0234947 1.9612229\n",
      "Logged Successfully: \n",
      "2018-05-17 19:23:59epoch=23; Loss Pred=1.9973; Val Loss=1.9612; Val Acc=0.5286; Loss Att={'forw': '1.0000'}; Train Acc=0.498; Test Acc=0.4883; Entropy={'forw': '8.9603'}; Entropy_Test=\n",
      "\n",
      "0 1.9612229 1.9580276\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:01epoch=24; Loss Pred=1.9968; Val Loss=1.9580; Val Acc=0.5278; Loss Att={'forw': '1.0000'}; Train Acc=0.498; Test Acc=0.4934; Entropy={'forw': '8.9603'}; Entropy_Test=\n",
      "\n",
      "0 1.9580276 1.9071331\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:02epoch=25; Loss Pred=1.9425; Val Loss=1.9071; Val Acc=0.5521; Loss Att={'forw': '1.0000'}; Train Acc=0.522; Test Acc=0.5162; Entropy={'forw': '8.8700'}; Entropy_Test=\n",
      "\n",
      "0 1.9071331 1.9105833\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:03epoch=26; Loss Pred=1.9429; Val Loss=1.9106; Val Acc=0.5514; Loss Att={'forw': '1.0000'}; Train Acc=0.522; Test Acc=0.5143; Entropy={'forw': '8.8700'}; Entropy_Test=\n",
      "\n",
      "1 1.9071331 1.8448235\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:04epoch=27; Loss Pred=1.8801; Val Loss=1.8448; Val Acc=0.5678; Loss Att={'forw': '1.0000'}; Train Acc=0.545; Test Acc=0.5392; Entropy={'forw': '8.8881'}; Entropy_Test=\n",
      "\n",
      "0 1.8448235 1.8425968\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:06epoch=28; Loss Pred=1.8800; Val Loss=1.8426; Val Acc=0.5724; Loss Att={'forw': '1.0000'}; Train Acc=0.546; Test Acc=0.5377; Entropy={'forw': '8.8881'}; Entropy_Test=\n",
      "\n",
      "0 1.8425968 1.7935085\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:07epoch=29; Loss Pred=1.8148; Val Loss=1.7935; Val Acc=0.5831; Loss Att={'forw': '1.0000'}; Train Acc=0.567; Test Acc=0.5603; Entropy={'forw': '8.9415'}; Entropy_Test=\n",
      "\n",
      "0 1.7935085 1.786225\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:08epoch=30; Loss Pred=1.8142; Val Loss=1.7862; Val Acc=0.5850; Loss Att={'forw': '1.0000'}; Train Acc=0.567; Test Acc=0.5591; Entropy={'forw': '8.9415'}; Entropy_Test=\n",
      "\n",
      "0 1.786225 1.7184395\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:10epoch=31; Loss Pred=1.7552; Val Loss=1.7184; Val Acc=0.6014; Loss Att={'forw': '1.0000'}; Train Acc=0.585; Test Acc=0.5789; Entropy={'forw': '8.9522'}; Entropy_Test=\n",
      "\n",
      "0 1.7184395 1.7337308\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:11epoch=32; Loss Pred=1.7546; Val Loss=1.7337; Val Acc=0.5983; Loss Att={'forw': '1.0000'}; Train Acc=0.584; Test Acc=0.5761; Entropy={'forw': '8.9522'}; Entropy_Test=\n",
      "\n",
      "1 1.7184395 1.6727647\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:12epoch=33; Loss Pred=1.6996; Val Loss=1.6728; Val Acc=0.6013; Loss Att={'forw': '1.0000'}; Train Acc=0.593; Test Acc=0.5845; Entropy={'forw': '8.9106'}; Entropy_Test=\n",
      "\n",
      "0 1.6727647 1.6779135\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:13epoch=34; Loss Pred=1.6999; Val Loss=1.6779; Val Acc=0.5943; Loss Att={'forw': '1.0000'}; Train Acc=0.593; Test Acc=0.5868; Entropy={'forw': '8.9106'}; Entropy_Test=\n",
      "\n",
      "1 1.6727647 1.6322229\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:15epoch=35; Loss Pred=1.6488; Val Loss=1.6322; Val Acc=0.5998; Loss Att={'forw': '1.0000'}; Train Acc=0.597; Test Acc=0.5904; Entropy={'forw': '8.8406'}; Entropy_Test=\n",
      "\n",
      "0 1.6322229 1.6316501\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:16epoch=36; Loss Pred=1.6487; Val Loss=1.6317; Val Acc=0.6030; Loss Att={'forw': '1.0000'}; Train Acc=0.597; Test Acc=0.5912; Entropy={'forw': '8.8406'}; Entropy_Test=\n",
      "\n",
      "0 1.6316501 1.5926892\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:17epoch=37; Loss Pred=1.6044; Val Loss=1.5927; Val Acc=0.6135; Loss Att={'forw': '1.0000'}; Train Acc=0.600; Test Acc=0.5930; Entropy={'forw': '9.0327'}; Entropy_Test=\n",
      "\n",
      "0 1.5926892 1.5943935\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:18epoch=38; Loss Pred=1.6044; Val Loss=1.5944; Val Acc=0.6035; Loss Att={'forw': '1.0000'}; Train Acc=0.600; Test Acc=0.5942; Entropy={'forw': '9.0327'}; Entropy_Test=\n",
      "\n",
      "1 1.5926892 1.557042\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:19epoch=39; Loss Pred=1.5715; Val Loss=1.5570; Val Acc=0.6160; Loss Att={'forw': '1.0000'}; Train Acc=0.603; Test Acc=0.5952; Entropy={'forw': '8.9973'}; Entropy_Test=\n",
      "\n",
      "0 1.557042 1.5561188\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:21epoch=40; Loss Pred=1.5717; Val Loss=1.5561; Val Acc=0.6141; Loss Att={'forw': '1.0000'}; Train Acc=0.603; Test Acc=0.5958; Entropy={'forw': '8.9973'}; Entropy_Test=\n",
      "\n",
      "0 1.5561188 1.5317324\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:22epoch=41; Loss Pred=1.5457; Val Loss=1.5317; Val Acc=0.6054; Loss Att={'forw': '1.0000'}; Train Acc=0.607; Test Acc=0.6035; Entropy={'forw': '8.9917'}; Entropy_Test=\n",
      "\n",
      "0 1.5317324 1.529339\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:23epoch=42; Loss Pred=1.5457; Val Loss=1.5293; Val Acc=0.6050; Loss Att={'forw': '1.0000'}; Train Acc=0.606; Test Acc=0.6008; Entropy={'forw': '8.9917'}; Entropy_Test=\n",
      "\n",
      "0 1.529339 1.5027062\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:24epoch=43; Loss Pred=1.5189; Val Loss=1.5027; Val Acc=0.6130; Loss Att={'forw': '1.0000'}; Train Acc=0.613; Test Acc=0.6061; Entropy={'forw': '8.9411'}; Entropy_Test=\n",
      "\n",
      "0 1.5027062 1.5037926\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:25epoch=44; Loss Pred=1.5192; Val Loss=1.5038; Val Acc=0.6115; Loss Att={'forw': '1.0000'}; Train Acc=0.612; Test Acc=0.6072; Entropy={'forw': '8.9411'}; Entropy_Test=\n",
      "\n",
      "1 1.5027062 1.4826814\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:26epoch=45; Loss Pred=1.4915; Val Loss=1.4827; Val Acc=0.6208; Loss Att={'forw': '1.0000'}; Train Acc=0.619; Test Acc=0.6145; Entropy={'forw': '8.9764'}; Entropy_Test=\n",
      "\n",
      "0 1.4826814 1.4736766\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:28epoch=46; Loss Pred=1.4920; Val Loss=1.4737; Val Acc=0.6186; Loss Att={'forw': '1.0000'}; Train Acc=0.618; Test Acc=0.6151; Entropy={'forw': '8.9764'}; Entropy_Test=\n",
      "\n",
      "0 1.4736766 1.4474546\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:29epoch=47; Loss Pred=1.4661; Val Loss=1.4475; Val Acc=0.6257; Loss Att={'forw': '1.0000'}; Train Acc=0.625; Test Acc=0.6216; Entropy={'forw': '8.8591'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.4474546 1.4536229\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:30epoch=48; Loss Pred=1.4662; Val Loss=1.4536; Val Acc=0.6224; Loss Att={'forw': '1.0000'}; Train Acc=0.625; Test Acc=0.6209; Entropy={'forw': '8.8591'}; Entropy_Test=\n",
      "\n",
      "1 1.4474546 1.419526\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:31epoch=49; Loss Pred=1.4404; Val Loss=1.4195; Val Acc=0.6315; Loss Att={'forw': '1.0000'}; Train Acc=0.632; Test Acc=0.6277; Entropy={'forw': '8.9071'}; Entropy_Test=\n",
      "\n",
      "0 1.419526 1.4117947\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:33epoch=50; Loss Pred=1.4403; Val Loss=1.4118; Val Acc=0.6280; Loss Att={'forw': '1.0000'}; Train Acc=0.631; Test Acc=0.6279; Entropy={'forw': '8.9071'}; Entropy_Test=\n",
      "\n",
      "0 1.4117947 1.3893272\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:34epoch=51; Loss Pred=1.4197; Val Loss=1.3893; Val Acc=0.6377; Loss Att={'forw': '1.0000'}; Train Acc=0.637; Test Acc=0.6337; Entropy={'forw': '8.8179'}; Entropy_Test=\n",
      "\n",
      "0 1.3893272 1.3965511\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:35epoch=52; Loss Pred=1.4200; Val Loss=1.3966; Val Acc=0.6419; Loss Att={'forw': '1.0000'}; Train Acc=0.636; Test Acc=0.6347; Entropy={'forw': '8.8179'}; Entropy_Test=\n",
      "\n",
      "1 1.3893272 1.3746047\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:36epoch=53; Loss Pred=1.3994; Val Loss=1.3746; Val Acc=0.6453; Loss Att={'forw': '1.0000'}; Train Acc=0.642; Test Acc=0.6367; Entropy={'forw': '8.7385'}; Entropy_Test=\n",
      "\n",
      "0 1.3746047 1.363014\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:38epoch=54; Loss Pred=1.3993; Val Loss=1.3630; Val Acc=0.6438; Loss Att={'forw': '1.0000'}; Train Acc=0.642; Test Acc=0.6381; Entropy={'forw': '8.7385'}; Entropy_Test=\n",
      "\n",
      "0 1.363014 1.3507206\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:39epoch=55; Loss Pred=1.3823; Val Loss=1.3507; Val Acc=0.6516; Loss Att={'forw': '1.0000'}; Train Acc=0.646; Test Acc=0.6399; Entropy={'forw': '8.7311'}; Entropy_Test=\n",
      "\n",
      "0 1.3507206 1.3644269\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:40epoch=56; Loss Pred=1.3821; Val Loss=1.3644; Val Acc=0.6432; Loss Att={'forw': '1.0000'}; Train Acc=0.646; Test Acc=0.6441; Entropy={'forw': '8.7311'}; Entropy_Test=\n",
      "\n",
      "1 1.3507206 1.3439229\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:41epoch=57; Loss Pred=1.3685; Val Loss=1.3439; Val Acc=0.6550; Loss Att={'forw': '1.0000'}; Train Acc=0.649; Test Acc=0.6451; Entropy={'forw': '8.6938'}; Entropy_Test=\n",
      "\n",
      "0 1.3439229 1.335542\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:43epoch=58; Loss Pred=1.3697; Val Loss=1.3355; Val Acc=0.6551; Loss Att={'forw': '1.0000'}; Train Acc=0.649; Test Acc=0.6443; Entropy={'forw': '8.6938'}; Entropy_Test=\n",
      "\n",
      "0 1.335542 1.3275914\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:44epoch=59; Loss Pred=1.3567; Val Loss=1.3276; Val Acc=0.6557; Loss Att={'forw': '1.0000'}; Train Acc=0.651; Test Acc=0.6464; Entropy={'forw': '8.6058'}; Entropy_Test=\n",
      "\n",
      "0 1.3275914 1.3294834\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:45epoch=60; Loss Pred=1.3563; Val Loss=1.3295; Val Acc=0.6637; Loss Att={'forw': '1.0000'}; Train Acc=0.651; Test Acc=0.6479; Entropy={'forw': '8.6058'}; Entropy_Test=\n",
      "\n",
      "1 1.3275914 1.3164072\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:47epoch=61; Loss Pred=1.3444; Val Loss=1.3164; Val Acc=0.6602; Loss Att={'forw': '1.0000'}; Train Acc=0.654; Test Acc=0.6496; Entropy={'forw': '8.5826'}; Entropy_Test=\n",
      "\n",
      "0 1.3164072 1.309507\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:48epoch=62; Loss Pred=1.3443; Val Loss=1.3095; Val Acc=0.6606; Loss Att={'forw': '1.0000'}; Train Acc=0.654; Test Acc=0.6507; Entropy={'forw': '8.5826'}; Entropy_Test=\n",
      "\n",
      "0 1.309507 1.3066863\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:49epoch=63; Loss Pred=1.3356; Val Loss=1.3067; Val Acc=0.6663; Loss Att={'forw': '1.0000'}; Train Acc=0.656; Test Acc=0.6535; Entropy={'forw': '8.5189'}; Entropy_Test=\n",
      "\n",
      "0 1.3066863 1.3142873\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:50epoch=64; Loss Pred=1.3343; Val Loss=1.3143; Val Acc=0.6647; Loss Att={'forw': '1.0000'}; Train Acc=0.656; Test Acc=0.6518; Entropy={'forw': '8.5189'}; Entropy_Test=\n",
      "\n",
      "1 1.3066863 1.3005579\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:52epoch=65; Loss Pred=1.3246; Val Loss=1.3006; Val Acc=0.6675; Loss Att={'forw': '1.0000'}; Train Acc=0.657; Test Acc=0.6555; Entropy={'forw': '8.4897'}; Entropy_Test=\n",
      "\n",
      "0 1.3005579 1.3008206\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:53epoch=66; Loss Pred=1.3244; Val Loss=1.3008; Val Acc=0.6631; Loss Att={'forw': '1.0000'}; Train Acc=0.658; Test Acc=0.6560; Entropy={'forw': '8.4897'}; Entropy_Test=\n",
      "\n",
      "1 1.3005579 1.3006048\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:54epoch=67; Loss Pred=1.3156; Val Loss=1.3006; Val Acc=0.6690; Loss Att={'forw': '1.0000'}; Train Acc=0.659; Test Acc=0.6554; Entropy={'forw': '8.4064'}; Entropy_Test=\n",
      "\n",
      "2 1.3005579 1.2997979\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:55epoch=68; Loss Pred=1.3158; Val Loss=1.2998; Val Acc=0.6687; Loss Att={'forw': '1.0000'}; Train Acc=0.658; Test Acc=0.6555; Entropy={'forw': '8.4064'}; Entropy_Test=\n",
      "\n",
      "0 1.2997979 1.2961048\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:57epoch=69; Loss Pred=1.3093; Val Loss=1.2961; Val Acc=0.6632; Loss Att={'forw': '1.0000'}; Train Acc=0.660; Test Acc=0.6572; Entropy={'forw': '8.4441'}; Entropy_Test=\n",
      "\n",
      "0 1.2961048 1.2798604\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:58epoch=70; Loss Pred=1.3085; Val Loss=1.2799; Val Acc=0.6694; Loss Att={'forw': '1.0000'}; Train Acc=0.660; Test Acc=0.6582; Entropy={'forw': '8.4441'}; Entropy_Test=\n",
      "\n",
      "0 1.2798604 1.2770596\n",
      "Logged Successfully: \n",
      "2018-05-17 19:24:59epoch=71; Loss Pred=1.3010; Val Loss=1.2771; Val Acc=0.6648; Loss Att={'forw': '1.0000'}; Train Acc=0.661; Test Acc=0.6556; Entropy={'forw': '8.3531'}; Entropy_Test=\n",
      "\n",
      "0 1.2770596 1.2782592\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:00epoch=72; Loss Pred=1.3030; Val Loss=1.2783; Val Acc=0.6651; Loss Att={'forw': '1.0000'}; Train Acc=0.660; Test Acc=0.6579; Entropy={'forw': '8.3531'}; Entropy_Test=\n",
      "\n",
      "1 1.2770596 1.2870605\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:02epoch=73; Loss Pred=1.2968; Val Loss=1.2871; Val Acc=0.6651; Loss Att={'forw': '1.0000'}; Train Acc=0.662; Test Acc=0.6598; Entropy={'forw': '8.3262'}; Entropy_Test=\n",
      "\n",
      "2 1.2770596 1.2865077\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:03epoch=74; Loss Pred=1.2969; Val Loss=1.2865; Val Acc=0.6600; Loss Att={'forw': '1.0000'}; Train Acc=0.661; Test Acc=0.6589; Entropy={'forw': '8.3262'}; Entropy_Test=\n",
      "\n",
      "3 1.2770596 1.2570571\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:04epoch=75; Loss Pred=1.2904; Val Loss=1.2571; Val Acc=0.6703; Loss Att={'forw': '1.0000'}; Train Acc=0.663; Test Acc=0.6613; Entropy={'forw': '8.3112'}; Entropy_Test=\n",
      "\n",
      "0 1.2570571 1.2651194\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:05epoch=76; Loss Pred=1.2914; Val Loss=1.2651; Val Acc=0.6674; Loss Att={'forw': '1.0000'}; Train Acc=0.663; Test Acc=0.6624; Entropy={'forw': '8.3112'}; Entropy_Test=\n",
      "\n",
      "1 1.2570571 1.2633383\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:07epoch=77; Loss Pred=1.2864; Val Loss=1.2633; Val Acc=0.6757; Loss Att={'forw': '1.0000'}; Train Acc=0.663; Test Acc=0.6625; Entropy={'forw': '8.3816'}; Entropy_Test=\n",
      "\n",
      "2 1.2570571 1.2789189\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:08epoch=78; Loss Pred=1.2870; Val Loss=1.2789; Val Acc=0.6689; Loss Att={'forw': '1.0000'}; Train Acc=0.663; Test Acc=0.6609; Entropy={'forw': '8.3816'}; Entropy_Test=\n",
      "\n",
      "3 1.2570571 1.2708405\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:09epoch=79; Loss Pred=1.2815; Val Loss=1.2708; Val Acc=0.6680; Loss Att={'forw': '1.0000'}; Train Acc=0.664; Test Acc=0.6607; Entropy={'forw': '8.4258'}; Entropy_Test=\n",
      "\n",
      "4 1.2570571 1.256437\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:10epoch=80; Loss Pred=1.2813; Val Loss=1.2564; Val Acc=0.6699; Loss Att={'forw': '1.0000'}; Train Acc=0.664; Test Acc=0.6600; Entropy={'forw': '8.4258'}; Entropy_Test=\n",
      "\n",
      "0 1.256437 1.2639276\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:12epoch=81; Loss Pred=1.2776; Val Loss=1.2639; Val Acc=0.6735; Loss Att={'forw': '1.0000'}; Train Acc=0.664; Test Acc=0.6623; Entropy={'forw': '8.3889'}; Entropy_Test=\n",
      "\n",
      "1 1.256437 1.2566677\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:13epoch=82; Loss Pred=1.2780; Val Loss=1.2567; Val Acc=0.6744; Loss Att={'forw': '1.0000'}; Train Acc=0.664; Test Acc=0.6636; Entropy={'forw': '8.3889'}; Entropy_Test=\n",
      "\n",
      "2 1.256437 1.2638487\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:14epoch=83; Loss Pred=1.2754; Val Loss=1.2638; Val Acc=0.6680; Loss Att={'forw': '1.0000'}; Train Acc=0.665; Test Acc=0.6630; Entropy={'forw': '8.3715'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1.256437 1.2559024\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:15epoch=84; Loss Pred=1.2741; Val Loss=1.2559; Val Acc=0.6783; Loss Att={'forw': '1.0000'}; Train Acc=0.665; Test Acc=0.6632; Entropy={'forw': '8.3715'}; Entropy_Test=\n",
      "\n",
      "0 1.2559024 1.2574518\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:17epoch=85; Loss Pred=1.2694; Val Loss=1.2575; Val Acc=0.6767; Loss Att={'forw': '1.0000'}; Train Acc=0.665; Test Acc=0.6661; Entropy={'forw': '8.4042'}; Entropy_Test=\n",
      "\n",
      "1 1.2559024 1.2515104\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:18epoch=86; Loss Pred=1.2707; Val Loss=1.2515; Val Acc=0.6796; Loss Att={'forw': '1.0000'}; Train Acc=0.665; Test Acc=0.6631; Entropy={'forw': '8.4042'}; Entropy_Test=\n",
      "\n",
      "0 1.2515104 1.253444\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:19epoch=87; Loss Pred=1.2665; Val Loss=1.2534; Val Acc=0.6762; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6629; Entropy={'forw': '8.4277'}; Entropy_Test=\n",
      "\n",
      "1 1.2515104 1.2549784\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:20epoch=88; Loss Pred=1.2670; Val Loss=1.2550; Val Acc=0.6748; Loss Att={'forw': '1.0000'}; Train Acc=0.665; Test Acc=0.6641; Entropy={'forw': '8.4277'}; Entropy_Test=\n",
      "\n",
      "2 1.2515104 1.2508503\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:22epoch=89; Loss Pred=1.2644; Val Loss=1.2509; Val Acc=0.6809; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6629; Entropy={'forw': '8.4188'}; Entropy_Test=\n",
      "\n",
      "0 1.2508503 1.2384444\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:23epoch=90; Loss Pred=1.2636; Val Loss=1.2384; Val Acc=0.6766; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6661; Entropy={'forw': '8.4188'}; Entropy_Test=\n",
      "\n",
      "0 1.2384444 1.2544289\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:24epoch=91; Loss Pred=1.2601; Val Loss=1.2544; Val Acc=0.6778; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6653; Entropy={'forw': '8.4221'}; Entropy_Test=\n",
      "\n",
      "1 1.2384444 1.2412163\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:25epoch=92; Loss Pred=1.2608; Val Loss=1.2412; Val Acc=0.6804; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6647; Entropy={'forw': '8.4221'}; Entropy_Test=\n",
      "\n",
      "2 1.2384444 1.2324622\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:27epoch=93; Loss Pred=1.2561; Val Loss=1.2325; Val Acc=0.6848; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6647; Entropy={'forw': '8.4703'}; Entropy_Test=\n",
      "\n",
      "0 1.2324622 1.2181507\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:28epoch=94; Loss Pred=1.2571; Val Loss=1.2182; Val Acc=0.6820; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6673; Entropy={'forw': '8.4703'}; Entropy_Test=\n",
      "\n",
      "0 1.2181507 1.239775\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:29epoch=95; Loss Pred=1.2535; Val Loss=1.2398; Val Acc=0.6780; Loss Att={'forw': '1.0000'}; Train Acc=0.666; Test Acc=0.6637; Entropy={'forw': '8.4700'}; Entropy_Test=\n",
      "\n",
      "1 1.2181507 1.2316822\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:30epoch=96; Loss Pred=1.2532; Val Loss=1.2317; Val Acc=0.6760; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6651; Entropy={'forw': '8.4700'}; Entropy_Test=\n",
      "\n",
      "2 1.2181507 1.2385786\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:32epoch=97; Loss Pred=1.2516; Val Loss=1.2386; Val Acc=0.6786; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6673; Entropy={'forw': '8.4773'}; Entropy_Test=\n",
      "\n",
      "3 1.2181507 1.2346191\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:33epoch=98; Loss Pred=1.2513; Val Loss=1.2346; Val Acc=0.6776; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6665; Entropy={'forw': '8.4773'}; Entropy_Test=\n",
      "\n",
      "4 1.2181507 1.2332045\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:34epoch=99; Loss Pred=1.2486; Val Loss=1.2332; Val Acc=0.6826; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6669; Entropy={'forw': '8.5242'}; Entropy_Test=\n",
      "\n",
      "5 1.2181507 1.2347628\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:35epoch=100; Loss Pred=1.2489; Val Loss=1.2348; Val Acc=0.6761; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6657; Entropy={'forw': '8.5242'}; Entropy_Test=\n",
      "\n",
      "6 1.2181507 1.2176934\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:36epoch=101; Loss Pred=1.2457; Val Loss=1.2177; Val Acc=0.6836; Loss Att={'forw': '1.0000'}; Train Acc=0.667; Test Acc=0.6674; Entropy={'forw': '8.5457'}; Entropy_Test=\n",
      "\n",
      "0 1.2176934 1.2329272\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:38epoch=102; Loss Pred=1.2454; Val Loss=1.2329; Val Acc=0.6794; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6652; Entropy={'forw': '8.5457'}; Entropy_Test=\n",
      "\n",
      "1 1.2176934 1.2215912\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:39epoch=103; Loss Pred=1.2436; Val Loss=1.2216; Val Acc=0.6852; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6667; Entropy={'forw': '8.5435'}; Entropy_Test=\n",
      "\n",
      "2 1.2176934 1.2272046\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:40epoch=104; Loss Pred=1.2426; Val Loss=1.2272; Val Acc=0.6856; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6668; Entropy={'forw': '8.5435'}; Entropy_Test=\n",
      "\n",
      "3 1.2176934 1.2311219\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:42epoch=105; Loss Pred=1.2418; Val Loss=1.2311; Val Acc=0.6836; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6657; Entropy={'forw': '8.5893'}; Entropy_Test=\n",
      "\n",
      "4 1.2176934 1.211255\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:43epoch=106; Loss Pred=1.2404; Val Loss=1.2113; Val Acc=0.6858; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6670; Entropy={'forw': '8.5893'}; Entropy_Test=\n",
      "\n",
      "0 1.211255 1.2195897\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:44epoch=107; Loss Pred=1.2389; Val Loss=1.2196; Val Acc=0.6857; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6672; Entropy={'forw': '8.6270'}; Entropy_Test=\n",
      "\n",
      "1 1.211255 1.2178895\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:45epoch=108; Loss Pred=1.2386; Val Loss=1.2179; Val Acc=0.6829; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6667; Entropy={'forw': '8.6270'}; Entropy_Test=\n",
      "\n",
      "2 1.211255 1.2245785\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:46epoch=109; Loss Pred=1.2362; Val Loss=1.2246; Val Acc=0.6843; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6665; Entropy={'forw': '8.7012'}; Entropy_Test=\n",
      "\n",
      "3 1.211255 1.2226901\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:48epoch=110; Loss Pred=1.2355; Val Loss=1.2227; Val Acc=0.6818; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6663; Entropy={'forw': '8.7012'}; Entropy_Test=\n",
      "\n",
      "4 1.211255 1.2168081\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:49epoch=111; Loss Pred=1.2331; Val Loss=1.2168; Val Acc=0.6864; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6684; Entropy={'forw': '8.7757'}; Entropy_Test=\n",
      "\n",
      "5 1.211255 1.2110517\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:50epoch=112; Loss Pred=1.2334; Val Loss=1.2111; Val Acc=0.6828; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6690; Entropy={'forw': '8.7757'}; Entropy_Test=\n",
      "\n",
      "0 1.2110517 1.2062036\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:51epoch=113; Loss Pred=1.2321; Val Loss=1.2062; Val Acc=0.6821; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6662; Entropy={'forw': '8.7688'}; Entropy_Test=\n",
      "\n",
      "0 1.2062036 1.2042824\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:53epoch=114; Loss Pred=1.2311; Val Loss=1.2043; Val Acc=0.6837; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6688; Entropy={'forw': '8.7688'}; Entropy_Test=\n",
      "\n",
      "0 1.2042824 1.1965779\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:54epoch=115; Loss Pred=1.2292; Val Loss=1.1966; Val Acc=0.6854; Loss Att={'forw': '1.0000'}; Train Acc=0.668; Test Acc=0.6665; Entropy={'forw': '8.8298'}; Entropy_Test=\n",
      "\n",
      "0 1.1965779 1.2140903\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:55epoch=116; Loss Pred=1.2296; Val Loss=1.2141; Val Acc=0.6861; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6676; Entropy={'forw': '8.8298'}; Entropy_Test=\n",
      "\n",
      "1 1.1965779 1.212118\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:56epoch=117; Loss Pred=1.2267; Val Loss=1.2121; Val Acc=0.6842; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6658; Entropy={'forw': '8.8289'}; Entropy_Test=\n",
      "\n",
      "2 1.1965779 1.2190814\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:57epoch=118; Loss Pred=1.2268; Val Loss=1.2191; Val Acc=0.6843; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6690; Entropy={'forw': '8.8289'}; Entropy_Test=\n",
      "\n",
      "3 1.1965779 1.2031112\n",
      "Logged Successfully: \n",
      "2018-05-17 19:25:59epoch=119; Loss Pred=1.2248; Val Loss=1.2031; Val Acc=0.6873; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6675; Entropy={'forw': '8.8211'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1.1965779 1.2080276\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:00epoch=120; Loss Pred=1.2257; Val Loss=1.2080; Val Acc=0.6861; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6678; Entropy={'forw': '8.8211'}; Entropy_Test=\n",
      "\n",
      "5 1.1965779 1.2096974\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:01epoch=121; Loss Pred=1.2238; Val Loss=1.2097; Val Acc=0.6859; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6644; Entropy={'forw': '8.8654'}; Entropy_Test=\n",
      "\n",
      "6 1.1965779 1.1913226\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:02epoch=122; Loss Pred=1.2228; Val Loss=1.1913; Val Acc=0.6827; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6675; Entropy={'forw': '8.8654'}; Entropy_Test=\n",
      "\n",
      "0 1.1913226 1.196545\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:04epoch=123; Loss Pred=1.2211; Val Loss=1.1965; Val Acc=0.6854; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6656; Entropy={'forw': '8.9132'}; Entropy_Test=\n",
      "\n",
      "1 1.1913226 1.2027333\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:05epoch=124; Loss Pred=1.2212; Val Loss=1.2027; Val Acc=0.6828; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6702; Entropy={'forw': '8.9132'}; Entropy_Test=\n",
      "\n",
      "2 1.1913226 1.1923447\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:06epoch=125; Loss Pred=1.2195; Val Loss=1.1923; Val Acc=0.6830; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6674; Entropy={'forw': '8.9493'}; Entropy_Test=\n",
      "\n",
      "3 1.1913226 1.1862597\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:07epoch=126; Loss Pred=1.2201; Val Loss=1.1863; Val Acc=0.6842; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6663; Entropy={'forw': '8.9493'}; Entropy_Test=\n",
      "\n",
      "0 1.1862597 1.1928912\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:08epoch=127; Loss Pred=1.2182; Val Loss=1.1929; Val Acc=0.6856; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6658; Entropy={'forw': '8.9471'}; Entropy_Test=\n",
      "\n",
      "1 1.1862597 1.182524\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:10epoch=128; Loss Pred=1.2176; Val Loss=1.1825; Val Acc=0.6862; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6665; Entropy={'forw': '8.9471'}; Entropy_Test=\n",
      "\n",
      "0 1.182524 1.1911837\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:11epoch=129; Loss Pred=1.2160; Val Loss=1.1912; Val Acc=0.6853; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6651; Entropy={'forw': '8.9623'}; Entropy_Test=\n",
      "\n",
      "1 1.182524 1.1846235\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:12epoch=130; Loss Pred=1.2157; Val Loss=1.1846; Val Acc=0.6847; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6668; Entropy={'forw': '8.9623'}; Entropy_Test=\n",
      "\n",
      "2 1.182524 1.1817302\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:13epoch=131; Loss Pred=1.2141; Val Loss=1.1817; Val Acc=0.6846; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6653; Entropy={'forw': '8.9883'}; Entropy_Test=\n",
      "\n",
      "0 1.1817302 1.1965492\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:14epoch=132; Loss Pred=1.2151; Val Loss=1.1965; Val Acc=0.6915; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6670; Entropy={'forw': '8.9883'}; Entropy_Test=\n",
      "\n",
      "1 1.1817302 1.1754993\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:16epoch=133; Loss Pred=1.2136; Val Loss=1.1755; Val Acc=0.6861; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6678; Entropy={'forw': '9.0225'}; Entropy_Test=\n",
      "\n",
      "0 1.1754993 1.1757874\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:17epoch=134; Loss Pred=1.2143; Val Loss=1.1758; Val Acc=0.6838; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6669; Entropy={'forw': '9.0225'}; Entropy_Test=\n",
      "\n",
      "1 1.1754993 1.1758173\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:18epoch=135; Loss Pred=1.2111; Val Loss=1.1758; Val Acc=0.6833; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6663; Entropy={'forw': '9.0472'}; Entropy_Test=\n",
      "\n",
      "2 1.1754993 1.1759326\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:19epoch=136; Loss Pred=1.2121; Val Loss=1.1759; Val Acc=0.6847; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6687; Entropy={'forw': '9.0472'}; Entropy_Test=\n",
      "\n",
      "3 1.1754993 1.1876551\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:20epoch=137; Loss Pred=1.2111; Val Loss=1.1877; Val Acc=0.6863; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6679; Entropy={'forw': '9.1261'}; Entropy_Test=\n",
      "\n",
      "4 1.1754993 1.1889149\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:21epoch=138; Loss Pred=1.2102; Val Loss=1.1889; Val Acc=0.6830; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6692; Entropy={'forw': '9.1261'}; Entropy_Test=\n",
      "\n",
      "5 1.1754993 1.1940062\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:23epoch=139; Loss Pred=1.2077; Val Loss=1.1940; Val Acc=0.6877; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6676; Entropy={'forw': '9.1359'}; Entropy_Test=\n",
      "\n",
      "6 1.1754993 1.174715\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:24epoch=140; Loss Pred=1.2103; Val Loss=1.1747; Val Acc=0.6882; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6649; Entropy={'forw': '9.1359'}; Entropy_Test=\n",
      "\n",
      "0 1.174715 1.1790195\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:25epoch=141; Loss Pred=1.2073; Val Loss=1.1790; Val Acc=0.6847; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6658; Entropy={'forw': '9.1440'}; Entropy_Test=\n",
      "\n",
      "1 1.174715 1.183231\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:26epoch=142; Loss Pred=1.2081; Val Loss=1.1832; Val Acc=0.6832; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6694; Entropy={'forw': '9.1440'}; Entropy_Test=\n",
      "\n",
      "2 1.174715 1.1827732\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:27epoch=143; Loss Pred=1.2066; Val Loss=1.1828; Val Acc=0.6858; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6678; Entropy={'forw': '9.1682'}; Entropy_Test=\n",
      "\n",
      "3 1.174715 1.1750526\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:28epoch=144; Loss Pred=1.2072; Val Loss=1.1751; Val Acc=0.6835; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6665; Entropy={'forw': '9.1682'}; Entropy_Test=\n",
      "\n",
      "4 1.174715 1.1743538\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:30epoch=145; Loss Pred=1.2059; Val Loss=1.1744; Val Acc=0.6848; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6689; Entropy={'forw': '9.1708'}; Entropy_Test=\n",
      "\n",
      "0 1.1743538 1.1729594\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:31epoch=146; Loss Pred=1.2053; Val Loss=1.1730; Val Acc=0.6775; Loss Att={'forw': '1.0000'}; Train Acc=0.669; Test Acc=0.6683; Entropy={'forw': '9.1708'}; Entropy_Test=\n",
      "\n",
      "0 1.1729594 1.168791\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:32epoch=147; Loss Pred=1.2037; Val Loss=1.1688; Val Acc=0.6843; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6691; Entropy={'forw': '9.1730'}; Entropy_Test=\n",
      "\n",
      "0 1.168791 1.1918583\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:33epoch=148; Loss Pred=1.2041; Val Loss=1.1919; Val Acc=0.6853; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6659; Entropy={'forw': '9.1730'}; Entropy_Test=\n",
      "\n",
      "1 1.168791 1.1852558\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:34epoch=149; Loss Pred=1.2030; Val Loss=1.1853; Val Acc=0.6840; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6688; Entropy={'forw': '9.2085'}; Entropy_Test=\n",
      "\n",
      "2 1.168791 1.1883415\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:35epoch=150; Loss Pred=1.2038; Val Loss=1.1883; Val Acc=0.6818; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6675; Entropy={'forw': '9.2085'}; Entropy_Test=\n",
      "\n",
      "3 1.168791 1.174352\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:37epoch=151; Loss Pred=1.2017; Val Loss=1.1744; Val Acc=0.6847; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6675; Entropy={'forw': '9.2375'}; Entropy_Test=\n",
      "\n",
      "4 1.168791 1.1774074\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:38epoch=152; Loss Pred=1.2023; Val Loss=1.1774; Val Acc=0.6883; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6707; Entropy={'forw': '9.2375'}; Entropy_Test=\n",
      "\n",
      "5 1.168791 1.1677967\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:39epoch=153; Loss Pred=1.2012; Val Loss=1.1678; Val Acc=0.6835; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6696; Entropy={'forw': '9.2666'}; Entropy_Test=\n",
      "\n",
      "0 1.1677967 1.1714951\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:40epoch=154; Loss Pred=1.2001; Val Loss=1.1715; Val Acc=0.6767; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6674; Entropy={'forw': '9.2666'}; Entropy_Test=\n",
      "\n",
      "1 1.1677967 1.1745535\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:41epoch=155; Loss Pred=1.2007; Val Loss=1.1746; Val Acc=0.6912; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6680; Entropy={'forw': '9.2709'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1.1677967 1.1733073\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:42epoch=156; Loss Pred=1.1995; Val Loss=1.1733; Val Acc=0.6847; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6679; Entropy={'forw': '9.2709'}; Entropy_Test=\n",
      "\n",
      "3 1.1677967 1.1613163\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:44epoch=157; Loss Pred=1.1991; Val Loss=1.1613; Val Acc=0.6858; Loss Att={'forw': '1.0000'}; Train Acc=0.670; Test Acc=0.6681; Entropy={'forw': '9.3022'}; Entropy_Test=\n",
      "\n",
      "0 1.1613163 1.184357\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:45epoch=158; Loss Pred=1.1987; Val Loss=1.1844; Val Acc=0.6840; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6658; Entropy={'forw': '9.3022'}; Entropy_Test=\n",
      "\n",
      "1 1.1613163 1.1782836\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:46epoch=159; Loss Pred=1.1986; Val Loss=1.1783; Val Acc=0.6882; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6691; Entropy={'forw': '9.3510'}; Entropy_Test=\n",
      "\n",
      "2 1.1613163 1.1694119\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:47epoch=160; Loss Pred=1.1975; Val Loss=1.1694; Val Acc=0.6901; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6668; Entropy={'forw': '9.3510'}; Entropy_Test=\n",
      "\n",
      "3 1.1613163 1.1711744\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:48epoch=161; Loss Pred=1.1965; Val Loss=1.1712; Val Acc=0.6836; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6686; Entropy={'forw': '9.3588'}; Entropy_Test=\n",
      "\n",
      "4 1.1613163 1.1832672\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:49epoch=162; Loss Pred=1.1967; Val Loss=1.1833; Val Acc=0.6802; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6684; Entropy={'forw': '9.3588'}; Entropy_Test=\n",
      "\n",
      "5 1.1613163 1.1755031\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:51epoch=163; Loss Pred=1.1963; Val Loss=1.1755; Val Acc=0.6835; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6685; Entropy={'forw': '9.3264'}; Entropy_Test=\n",
      "\n",
      "6 1.1613163 1.1747136\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:52epoch=164; Loss Pred=1.1956; Val Loss=1.1747; Val Acc=0.6801; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6686; Entropy={'forw': '9.3264'}; Entropy_Test=\n",
      "\n",
      "7 1.1613163 1.1665504\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:53epoch=165; Loss Pred=1.1957; Val Loss=1.1666; Val Acc=0.6867; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6685; Entropy={'forw': '9.3302'}; Entropy_Test=\n",
      "\n",
      "8 1.1613163 1.1714957\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:54epoch=166; Loss Pred=1.1965; Val Loss=1.1715; Val Acc=0.6853; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6688; Entropy={'forw': '9.3302'}; Entropy_Test=\n",
      "\n",
      "9 1.1613163 1.152394\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:55epoch=167; Loss Pred=1.1948; Val Loss=1.1524; Val Acc=0.6861; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6682; Entropy={'forw': '9.3321'}; Entropy_Test=\n",
      "\n",
      "0 1.152394 1.1678356\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:56epoch=168; Loss Pred=1.1935; Val Loss=1.1678; Val Acc=0.6858; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6685; Entropy={'forw': '9.3321'}; Entropy_Test=\n",
      "\n",
      "1 1.152394 1.1716068\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:57epoch=169; Loss Pred=1.1935; Val Loss=1.1716; Val Acc=0.6850; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6681; Entropy={'forw': '9.3348'}; Entropy_Test=\n",
      "\n",
      "2 1.152394 1.1726865\n",
      "Logged Successfully: \n",
      "2018-05-17 19:26:59epoch=170; Loss Pred=1.1935; Val Loss=1.1727; Val Acc=0.6842; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6676; Entropy={'forw': '9.3348'}; Entropy_Test=\n",
      "\n",
      "3 1.152394 1.1633304\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:00epoch=171; Loss Pred=1.1922; Val Loss=1.1633; Val Acc=0.6855; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6683; Entropy={'forw': '9.3369'}; Entropy_Test=\n",
      "\n",
      "4 1.152394 1.170358\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:01epoch=172; Loss Pred=1.1921; Val Loss=1.1704; Val Acc=0.6828; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6709; Entropy={'forw': '9.3369'}; Entropy_Test=\n",
      "\n",
      "5 1.152394 1.1626089\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:02epoch=173; Loss Pred=1.1918; Val Loss=1.1626; Val Acc=0.6820; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6685; Entropy={'forw': '9.3519'}; Entropy_Test=\n",
      "\n",
      "6 1.152394 1.1657342\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:03epoch=174; Loss Pred=1.1917; Val Loss=1.1657; Val Acc=0.6940; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6708; Entropy={'forw': '9.3519'}; Entropy_Test=\n",
      "\n",
      "7 1.152394 1.1638653\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:04epoch=175; Loss Pred=1.1903; Val Loss=1.1639; Val Acc=0.6750; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6684; Entropy={'forw': '9.3577'}; Entropy_Test=\n",
      "\n",
      "8 1.152394 1.1660459\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:06epoch=176; Loss Pred=1.1914; Val Loss=1.1660; Val Acc=0.6843; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6676; Entropy={'forw': '9.3577'}; Entropy_Test=\n",
      "\n",
      "9 1.152394 1.1724526\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:07epoch=177; Loss Pred=1.1904; Val Loss=1.1725; Val Acc=0.6785; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6662; Entropy={'forw': '9.3546'}; Entropy_Test=\n",
      "\n",
      "10 1.152394 1.165775\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:08epoch=178; Loss Pred=1.1895; Val Loss=1.1658; Val Acc=0.6884; Loss Att={'forw': '1.0000'}; Train Acc=0.671; Test Acc=0.6684; Entropy={'forw': '9.3546'}; Entropy_Test=\n",
      "\n",
      "11 1.152394 1.1612278\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:09epoch=179; Loss Pred=1.1885; Val Loss=1.1612; Val Acc=0.6887; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6706; Entropy={'forw': '9.3688'}; Entropy_Test=\n",
      "\n",
      "12 1.152394 1.1668442\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:10epoch=180; Loss Pred=1.1888; Val Loss=1.1668; Val Acc=0.6871; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6709; Entropy={'forw': '9.3688'}; Entropy_Test=\n",
      "\n",
      "13 1.152394 1.1578999\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:11epoch=181; Loss Pred=1.1869; Val Loss=1.1579; Val Acc=0.6853; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6691; Entropy={'forw': '9.3846'}; Entropy_Test=\n",
      "\n",
      "14 1.152394 1.1642193\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:12epoch=182; Loss Pred=1.1880; Val Loss=1.1642; Val Acc=0.6821; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6685; Entropy={'forw': '9.3846'}; Entropy_Test=\n",
      "\n",
      "15 1.152394 1.1662301\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:14epoch=183; Loss Pred=1.1877; Val Loss=1.1662; Val Acc=0.6822; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6694; Entropy={'forw': '9.3959'}; Entropy_Test=\n",
      "\n",
      "16 1.152394 1.1614443\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:15epoch=184; Loss Pred=1.1871; Val Loss=1.1614; Val Acc=0.6794; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6677; Entropy={'forw': '9.3959'}; Entropy_Test=\n",
      "\n",
      "17 1.152394 1.1755563\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:16epoch=185; Loss Pred=1.1856; Val Loss=1.1756; Val Acc=0.6842; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6684; Entropy={'forw': '9.4050'}; Entropy_Test=\n",
      "\n",
      "18 1.152394 1.1578679\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:17epoch=186; Loss Pred=1.1859; Val Loss=1.1579; Val Acc=0.6927; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6695; Entropy={'forw': '9.4050'}; Entropy_Test=\n",
      "\n",
      "19 1.152394 1.1668133\n",
      "Logged Successfully: \n",
      "2018-05-17 19:27:18epoch=187; Loss Pred=1.1850; Val Loss=1.1668; Val Acc=0.6854; Loss Att={'forw': '1.0000'}; Train Acc=0.672; Test Acc=0.6691; Entropy={'forw': '9.4146'}; Entropy_Test=\n",
      "\n",
      "20 1.152394 1.159214\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 189\n",
      "Optimization Finished!\n",
      "Saved Results Successfully\n"
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python\n",
    "\n",
    "# This version of the code trains the attractor connections with a separate\n",
    "# objective function than the objective function used to train all other weights\n",
    "# in the network (on the prediction task).\n",
    "\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload\n",
    "\n",
    "from tensorflow_helpers import *\n",
    "from data_generator import generate_examples, pick_task\n",
    "\n",
    "from helper_functions import get_batches, load_pretrained_embeddings, \\\n",
    "    get_model_type_str, translate_ids_to_words, \\\n",
    "    save_results, print_into_log, print_some_translated_sentences, \\\n",
    "    get_training_progress_comment\n",
    "from information_trackers import compute_entropy_fullvec\n",
    "from graph_init import GRU_attractor\n",
    "\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, patience_max, disp_epoch, min_delta = 0.00):\n",
    "        self.best = 1e10\n",
    "        self.patience = 0  # our patience\n",
    "        self.patience_max = patience_max\n",
    "        self.display_epoch = disp_epoch\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def update(self, current):\n",
    "        if self.best > current:\n",
    "            self.best = current\n",
    "            self.patience = 0\n",
    "        elif abs(self.best - current) > self.min_delta:\n",
    "            self.patience += 1\n",
    "\n",
    "    def patience_ran_out(self):\n",
    "        if self.patience*self.display_epoch > self.patience_max:\n",
    "            return True\n",
    "        else:\n",
    "            False\n",
    "            \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "ops = {\n",
    "    'model_type': \"GRU\",  # OPTIONS: vanilla, LSTM_raw, LSTM_tensorflow, LSTM_attractor\n",
    "    'hid': 50,\n",
    "    'in': None,  # TBD\n",
    "    'out': 1,\n",
    "    #         'batch_size':n_examples, #since the sequences are 1-dimensional it's easier to just run them all at once\n",
    "    'n_attractor_iterations': 15,\n",
    "    'attractor_dynamics': \"projection2\",  # OPTIONS:  \"\" (for no attractor dynamics),\n",
    "    #           \"direct\" (simple attractor weights applied to hidden states directly, trained with noise addition)\n",
    "    #           \"projection\" (project the hidden state into a separate space via weights, do attraction, project back)\n",
    "    #           \"helper_hidden\" (hidden-hidden neurons) - IMPORTANT: don't forget to add h_hid number\n",
    "    'h_hid': 100,  # helper hidden for \"helper hidden\" \"attractory_dynamics\" mode\n",
    "    'attractor_noise_level': 0.5,\n",
    "    'attractor_noise_type': \"bernoilli\",  # OPTIONS: \"gaussian\", \"dropout\", \"random_drop\"\n",
    "\n",
    "    'training_mode': \"\",  # 'attractor_on_both',\n",
    "\n",
    "    'attractor_regularization': \"l2_regularization\",  # OPTIONS: \"l2_regularization\", \"l2_norm\"\n",
    "    'attractor_regularization_lambda': 0.0,\n",
    "\n",
    "    'record_mutual_information': True,\n",
    "    'problem_type': \"msnbc\",  # OPTIONS: parity, parity_length, majority, reber, kazakov, pos_brown, ner_german, sentiment_imdb, topic_classification, video_classification\n",
    "    'masking': True,#\"seq\", \"final\"\n",
    "    'prediction_type': 'seq', #'seq', 'final', 'final_class'\n",
    "    'seq_len': None,\n",
    "\n",
    "    'save_best_model': True,\n",
    "    'reshuffle_data_each_replication': False,  # relevant for POS datasets (since they are loaded from files)\n",
    "    'test_partition': 0.3,\n",
    "    'lrate': 0.001,  # was 0.008\n",
    "\n",
    "    # NLP related (pos_brown task)\n",
    "    'bidirectional': False,\n",
    "    'embedding_size': 100,\n",
    "    'load_word_embeddings': False,\n",
    "    'train_word_embeddings': False,\n",
    "    'trainable_logic_symbols': 0, #make first *N* embeddings trainable(Pad, unknown, start symbols make it a separate matrix and trainable)\n",
    "    'input_type': \"embed\",  # embed&prior, embed, prior\n",
    "    'dropout': 0.2  # in range(0,1)\n",
    "}\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# SEQ_LEN = 12 # number of bits in input sequence\n",
    "N_HIDDEN = ops['hid']  # number of hidden units\n",
    "N_H_HIDDEN = ops['h_hid']\n",
    "TASK = ops['problem_type']\n",
    "ARCH = ops['model_type']  # hidden layer type: 'GRU' or 'tanh'\n",
    "NOISE_LEVEL = ops['attractor_noise_level']\n",
    "# noise in training attractor net\n",
    "# if >=0, Gaussian with std dev NOISE_LEVEL\n",
    "# if < 0, Bernoulli dropout proportion -NOISE_LEVEL\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "INPUT_NOISE_LEVEL = 0.1\n",
    "ATTRACTOR_TYPE = ops['attractor_dynamics']\n",
    "N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "# number of time steps in attractor dynamics\n",
    "# if = 0, then no attractor net\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# ATTR_WEIGHT_CONSTRAINTS = True\n",
    "# True: make attractor weights symmetric and have zero diag\n",
    "# False: unconstrained\n",
    "TRAIN_ATTR_WEIGHTS_ON_PREDICTION = False\n",
    "# True: train attractor weights on attractor net _and_ prediction\n",
    "REPORT_BEST_TRAIN_PERFORMANCE = True\n",
    "# True: save the train/test perf on the epoch for which train perf was best\n",
    "LOSS_SWITCH_FREQ = 1\n",
    "# how often (in epochs) to switch between attractor\n",
    "# and prediction loss\n",
    "\n",
    "ops, SEQ_LEN, N_INPUT, N_CLASSES, N_TRAIN, N_TEST = pick_task(ops['problem_type'],\n",
    "                                                              ops)  # task (parity, majority, reber, kazakov)\n",
    "\n",
    "# Training Parameters\n",
    "\n",
    "TRAINING_EPOCHS = 500\n",
    "N_REPLICATIONS = 1\n",
    "BATCH_SIZE = 8000\n",
    "DISPLAY_EPOCH = 1\n",
    "EARLY_STOPPING_THRESH = 0.03 # 1e-3 for POS, 0.03 for Sentiment\n",
    "EARLY_STOPPING_PATIENCE = 20  # in epochs\n",
    "EARLY_STOPPING_MINIMUM_EPOCH = 0\n",
    "\n",
    "# NOTEBOOK CODE\n",
    "\n",
    "######### MAIN CODE #############################################################\n",
    "#0.02, 0.05, 0.1, 0.2, 0.35, 0.5, \n",
    "for dataset_part in [0.5, 0.99]:\n",
    "#     for attractor_steps in [15,0]:\n",
    "    attractor_steps = 15\n",
    "    for attractor_steps in [15,0]:\n",
    "        NOISE_LEVEL = ops['attractor_noise_level']\n",
    "\n",
    "\n",
    "#     for att_reg in [0.0]:\n",
    "#         ops['attractor_regularization_lambda'] = att_reg\n",
    "        # the tf seed needs to be within the context of the graph.\n",
    "        tf.reset_default_graph()\n",
    "        np.random.seed(11)\n",
    "        tf.set_random_seed(11)\n",
    "        ops['n_attractor_iterations'] = attractor_steps\n",
    "        N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "\n",
    "        #\n",
    "        # PLACEHOLDERS\n",
    "        #\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            # last dimension is left singular, tensorflow will expect it to be an id number, not 1-hot embed\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN], name='Y')\n",
    "        elif 'msnbc' in ops['problem_type']:\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN], name='Y')\n",
    "        elif ops['problem_type'] == 'sentiment_imdb':\n",
    "             # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            Y = tf.placeholder(\"int64\", [None, N_CLASSES], name='Y')\n",
    "        elif ops['problem_type'] == 'topic_classification':\n",
    "             # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            Y = tf.placeholder(\"int64\", [None, 1], name='Y')\n",
    "        elif ops['problem_type'] == 'ner_german':\n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN])\n",
    "        else:  # single output \n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, 1])\n",
    "        attractor_tgt_net = tf.placeholder(\"float\", [None, N_HIDDEN], name='attractor_tgt')\n",
    "\n",
    "        # Embedding matrix initialization\n",
    "        if 'pos' in ops['problem_type'] or 'sentiment' in ops['problem_type'] or ops['problem_type'] == \"topic_classification\":\n",
    "            [_, _, _, _, _, _, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                         INPUT_NOISE_LEVEL, TASK, ops)\n",
    "\n",
    "            if ops['load_word_embeddings']:\n",
    "                embeddings_loaded, _ = load_pretrained_embeddings('data/glove.6B.{}d.txt'.format(ops['embedding_size']),\n",
    "                                                               maps, ops)\n",
    "                if ops['trainable_logic_symbols'] > 0:\n",
    "                    with tf.variable_scope(\"TASK_WEIGHTS\"):\n",
    "                        symbols_embedding = tf.get_variable(\"symb_embedding\",\n",
    "                                                initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                                shape=[ops['trainable_logic_symbols'], ops['embedding_size']],\n",
    "                                                dtype=tf.float32,\n",
    "                                                trainable=True)\n",
    "                    \n",
    "                word_embedding = tf.get_variable(\"embedding\",\n",
    "                                            initializer=embeddings_loaded,\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=ops['train_word_embeddings'])\n",
    "                if ops['trainable_logic_symbols'] > 0:\n",
    "                    embedding = tf.concat([symbols_embedding, word_embedding], axis=0)\n",
    "                else:\n",
    "                    embedding = word_embedding\n",
    "            else:  # initialize randomly\n",
    "                embedding = tf.get_variable(\"embedding\",\n",
    "                                            initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                            shape=[ops['vocab_size'], ops['embedding_size']],\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=ops['train_word_embeddings'])\n",
    "            embed_lookup = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "            # load priors information\n",
    "            if ops['input_type'] == 'prior' or ops['input_type'] == 'embed&prior':\n",
    "                id2prior = maps['id2prior']\n",
    "                word2id = maps['word2id']\n",
    "                priors = np.zeros([len(id2prior), len(id2prior[0])]).astype(\"float32\")\n",
    "                for id, prior in id2prior.items():\n",
    "                    priors[id] = prior\n",
    "                priors_op = tf.get_variable(\"priors\",\n",
    "                                            initializer=priors,\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=False)\n",
    "                prior_lookup = tf.nn.embedding_lookup(priors_op, X)\n",
    "\n",
    "            if ops['input_type'] == 'embed':\n",
    "                embed = embed_lookup\n",
    "            elif ops['input_type'] == 'prior':\n",
    "                embed = prior_lookup\n",
    "            elif ops['input_type'] == 'embed&prior':\n",
    "                embed = tf.concat([embed_lookup, prior_lookup], axis=2)\n",
    "\n",
    "        # Graph + all the training variables\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            net_inputs = {'X': embed, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        elif ops['problem_type'] == 'sentiment_imdb' or ops['problem_type'] == 'topic_classification':\n",
    "            net_inputs = {'X': embed, 'mask': X, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        elif ops['problem_type'] == 'msnbc':\n",
    "            net_inputs = {'X': tf.one_hot(X, depth=N_CLASSES), 'mask': X, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        else:\n",
    "            net_inputs = {'X': X, 'mask': X, 'attractor_tgt_net': attractor_tgt_net}\n",
    "\n",
    "        if ops['bidirectional']:\n",
    "            G_attractors = {'forw': [], 'back': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            # Backward:\n",
    "            G_back = GRU_attractor(ops, inputs=net_inputs, direction='backward', suffix=names[1])\n",
    "            attr_loss_op_back = G_back.attr_loss_op\n",
    "            attr_train_op_back = G_back.attr_train_op\n",
    "            h_clean_seq_flat_back = G_back.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_back = G_back.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['back'] = {'attr_loss_op': attr_loss_op_back, \"attr_train_op\": attr_train_op_back,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_back, 'h_net_seq_flat': h_net_seq_flat_back}\n",
    "            G_back_output = G_back.output\n",
    "\n",
    "            \n",
    "            \n",
    "            # Merge: [seq_len, batch_size, n_hid*2]\n",
    "            # Note that we reverse the backward cell's output to align with original direction\n",
    "            # note in \"final\" only prediction, one less dimension\n",
    "            if 'final' in ops['prediction_type']:\n",
    "                merge_index = 1\n",
    "            else:\n",
    "                merge_index = 2\n",
    "            output = tf.concat([G_forw_output, tf.reverse(G_back_output, axis=[0])], axis=merge_index)\n",
    "    \n",
    "            if ops['dropout'] > 0.0:\n",
    "                # note keep_prob = 1.0 - drop_probability (not sure why they implemented it this way)\n",
    "                # tensorflow implementation scales by 1/keep_prob automatically\n",
    "                output = tf.nn.dropout(output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output = output\n",
    "\n",
    "            input_size_final_projection = 2 * ops['hid']\n",
    "            Y_ =  project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "            \n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "        else:\n",
    "            G_attractors = {'forw': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            input_size_final_projection = ops['hid']\n",
    "            \n",
    "            if ops['dropout'] > 0.0:\n",
    "                output = tf.nn.dropout(G_forw_output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output = G_forw_output\n",
    "            \n",
    "            Y_ = project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "\n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "\n",
    "            \n",
    "        mask_op = tf.cast(tf.sign(Y), dtype=tf.float32)\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "            # TODO: make a class for all \"best\" quantities (a lot of space)\n",
    "            saved_train_acc = []\n",
    "            saved_test_acc = []\n",
    "            saved_epoch = []\n",
    "            saved_att_loss = []\n",
    "            saved_entropy_final = []\n",
    "            saved_val_acc = []\n",
    "            saved_val_loss = []\n",
    "            saved_traini_loss = []\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # Start training\n",
    "            for replication in range(N_REPLICATIONS):\n",
    "                print(\"********** replication \", replication, \" **********\")\n",
    "                early_stopper = EarlyStopper(EARLY_STOPPING_PATIENCE, DISPLAY_EPOCH)\n",
    "                [X_full_train, Y_full_train, X_test, Y_test, X_val, Y_val, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                                                           INPUT_NOISE_LEVEL, TASK, ops)\n",
    "                # Take Only part of dataset:\n",
    "                all_ids = range(len(X_full_train))\n",
    "                np.random.shuffle(all_ids)\n",
    "                train_part = int(dataset_part * len(X_full_train))\n",
    "                ids_to_take = all_ids[0:train_part]\n",
    "                ids_for_val = all_ids[train_part:int(train_part + 0.2*train_part)]\n",
    "                if len(ids_to_take) > X_full_train.shape[0]:\n",
    "                    ids_to_take = range(X_full_train.shape[0])\n",
    "                X_train = X_full_train[ids_to_take]\n",
    "                Y_train = Y_full_train[ids_to_take]\n",
    "                \n",
    "                if BATCH_SIZE < len(X_train):\n",
    "                    ops['attractor_regularization_lambda'] = ops['attractor_regularization_lambda']/(len(X_train)*1.0/BATCH_SIZE)\n",
    "                    print(ops['attractor_regularization_lambda'])\n",
    "                \n",
    "                X_val, Y_val = X_full_train[ids_for_val], Y_full_train[ids_for_val,:]\n",
    "                \n",
    "                N_TRAIN = len(X_train)\n",
    "                print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)\n",
    "\n",
    "                # Log Path init-n:\n",
    "                COMMENT = 'dataset_starvation_experiment'\n",
    "                MODEL_NAME_FILE = '{}_(att_iter{}__bidir{}__drop{})_{}.txt'.format(ops['problem_type'],\n",
    "                                                                                   ops['n_attractor_iterations'],\n",
    "                                                                                   ops['bidirectional'],\n",
    "                                                                                   ops['dropout'],\n",
    "                                                                                   COMMENT)\n",
    "                LOG_DIRECTORY = 'experiments/logs/{}'.format(MODEL_NAME_FILE)\n",
    "                MODEL_DIRECTORY = 'experiments/logs/{}_{}'.format(datetime.date.today(), MODEL_NAME_FILE)\n",
    "                print_into_log(LOG_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN))\n",
    "                print_into_log(MODEL_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN), supress=True)\n",
    "\n",
    "                sess.run(init)  # Run the initializer\n",
    "\n",
    "                train_prediction_loss = True\n",
    "                best_train_acc = -1000.\n",
    "                best_test_acc = 0\n",
    "                best_entropy = 0.0\n",
    "                best_att_loss = 0\n",
    "                best_train_loss = 0\n",
    "                best_val_loss = 0.0\n",
    "                best_val_acc = 0.0\n",
    "                best_epoch = 0\n",
    "                for epoch in range(1, TRAINING_EPOCHS + 2):\n",
    "                    if (epoch - 1) % DISPLAY_EPOCH == 0:\n",
    "                        # TRAIN set:\n",
    "                        ploss, train_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        # TEST set:\n",
    "                        test_acc = batch_tensor_collect(sess, [accuracy], X, Y, X_test, Y_test, BATCH_SIZE)[0]\n",
    "    \n",
    "                        # Validation set & Early stopping:\n",
    "                        ploss_val, val_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                  X, Y, X_val, Y_val, BATCH_SIZE)\n",
    "            \n",
    "                        # Precistion/Recall:\n",
    "                        if ops['problem_type'] == 'ner_german':\n",
    "                            y_pred, y_true, mask_val = batch_tensor_collect(sess, [Y_, Y, mask_op],\n",
    "                                                                X, Y, X_test, Y_test, BATCH_SIZE)\n",
    "                            y_pred = np.argmax(y_pred, axis=2)\n",
    "                            \n",
    "                            Y_pred_flat = np.extract(mask_val.astype(bool), y_pred)\n",
    "                            Y_test_flat = np.extract(mask_val.astype(bool), y_true)\n",
    "                            print(\"PRECISION:\",compute_f1(Y_pred_flat, Y_test_flat, maps['id2tag']))\n",
    "                            \n",
    "                        print(early_stopper.patience, early_stopper.best, ploss_val)\n",
    "                        early_stopper.update(ploss_val)\n",
    "                        if early_stopper.patience_ran_out():\n",
    "                            print_into_log(LOG_DIRECTORY, \"STOPPED EARLY AT {}\".format(epoch))\n",
    "                            break\n",
    "\n",
    "                        # ATTRACTOR(s) LOSS\n",
    "                        aloss = {}\n",
    "                        entropy = {}\n",
    "                        hid_vals_arr = batch_tensor_collect(sess, [A['h_net_seq_flat'] for att_name, A in\n",
    "                                                                   G_attractors.items()],\n",
    "                                                            X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        h_clean_val_arr = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            A = G_attractors[attractor_name]\n",
    "                            a_loss_val = []\n",
    "                            n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "                            for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "                                a_loss_val.append(\n",
    "                                    sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "                            aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "\n",
    "                            entropy[attractor_name] = \"{:.4f}\".format(\n",
    "                                compute_entropy_fullvec(h_clean_val_arr[i], ops, n_bins=8))\n",
    "\n",
    "                        # Print training information:\n",
    "                        print_into_log(LOG_DIRECTORY, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + get_training_progress_comment(epoch, ploss, aloss, ploss_val, val_acc, train_acc,\n",
    "                                                                     test_acc, entropy))\n",
    "                        # Update the logs:\n",
    "                       \n",
    "                        #                 if ops['record_mutual_information']:\n",
    "                        # #                     h_attractor_val, h_clean_val = sess.run([h_attractor_collection, h_clean_seq_flat],\n",
    "                        # #                                                                    feed_dict={X: X_train, Y: Y_train})\n",
    "                        #                     # TODO: h_attractor_collection reshapeing masking.\n",
    "                        #                     h_attractor_val = None\n",
    "                        #                     h_clean_val = batch_tensor_collect(sess, [h_clean_seq_flat],\n",
    "                        #                                                                         X, Y, X_train, Y_train, BATCH_SIZE)[0]\n",
    "                        #                     MIS.update(ploss, aloss, train_acc, test_acc, np.tanh(hid_vals), h_attractor_val, h_clean_val)\n",
    "\n",
    "                        if (val_acc > best_val_acc):\n",
    "                            best_train_acc = train_acc\n",
    "                            best_test_acc = test_acc\n",
    "                            best_att_loss = aloss\n",
    "                            best_epoch = epoch\n",
    "                            best_val_acc = val_acc\n",
    "\n",
    "                            best_val_loss = ploss_val\n",
    "                            best_train_loss = ploss\n",
    "                            if ops['save_best_model']:\n",
    "                                save_path = saver.save(sess, MODEL_DIRECTORY)\n",
    "                            best_entropy = entropy\n",
    "                        if (1.0 - 1e-15 < 0.0):\n",
    "                            print('reached_peak')\n",
    "                            break\n",
    "\n",
    "                    if epoch > 1 and LOSS_SWITCH_FREQ > 0 \\\n",
    "                            and (epoch - 1) % LOSS_SWITCH_FREQ == 0:\n",
    "                        train_prediction_loss = not train_prediction_loss\n",
    "\n",
    "                    # MODEL TRAINING\n",
    "                    batches = get_batches(BATCH_SIZE, X_train, Y_train)\n",
    "                    for (batch_x, batch_y) in batches:\n",
    "                        if (LOSS_SWITCH_FREQ == 0 or train_prediction_loss):\n",
    "                            # Optimize all parameters except for attractor weights\n",
    "                            _ = sess.run([pred_train_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y})\n",
    "                        # Attractor:\n",
    "                        if (N_ATTRACTOR_STEPS > 0):\n",
    "                            batch_hid_vals = sess.run([A['h_net_seq_flat'] for att_name, A in G_attractors.items()],\n",
    "                                                      feed_dict={X:batch_x,  Y:batch_y})\n",
    "\n",
    "                            for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                A = G_attractors[attractor_name]\n",
    "                                _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: batch_hid_vals[i]})\n",
    "                print(\"Optimization Finished!\")\n",
    "\n",
    "                if (REPORT_BEST_TRAIN_PERFORMANCE):\n",
    "                    saved_train_acc.append(best_train_acc)\n",
    "                    saved_test_acc.append(best_test_acc)\n",
    "                    saved_att_loss.append(best_att_loss)\n",
    "                    saved_entropy_final.append(best_entropy)\n",
    "                    saved_epoch.append(best_epoch)\n",
    "\n",
    "                    saved_val_acc.append(best_val_acc)\n",
    "                    saved_val_loss.append(best_val_loss)\n",
    "                    saved_traini_loss.append(best_train_loss)\n",
    "                else:\n",
    "                    saved_train_acc.append(train_acc)\n",
    "                    saved_test_acc.append(test_acc)\n",
    "                    #             saved_att_loss.append(aloss)\n",
    "\n",
    "            save_results(ops, saved_epoch, saved_train_acc, saved_test_acc, saved_att_loss, saved_entropy_final, saved_val_acc,\n",
    "                 saved_val_loss, saved_traini_loss, N_TRAIN, N_TEST, SEQ_LEN, comment=COMMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
