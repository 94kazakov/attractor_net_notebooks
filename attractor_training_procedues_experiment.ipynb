{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "reber no_att\n",
      "L2 norm\n",
      "********** replication  0  **********\n",
      "reber\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: reber\n",
      "    hid: \t\t\t5,\n",
      "    h_hid: \t\t\t10\n",
      "    n_attractor_iterations: \t0,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.2\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_norm(lambda:0.05)\n",
      "    word_embedding: size\t(100), train(True)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t200/400, SEQ_LEN: 20\n",
      "Logged Successfully: \n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:49epoch=0; Loss Pred=1.0985; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.490; Test Acc=0.4950; Entropy={'forw': '3.8557'}; Entropy_Test={'forw': '3.7953'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:50epoch=200; Loss Pred=0.6585; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.785; Test Acc=0.7100; Entropy={'forw': '4.6206'}; Entropy_Test={'forw': '4.6029'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:51epoch=400; Loss Pred=0.3633; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.885; Test Acc=0.7625; Entropy={'forw': '4.4714'}; Entropy_Test={'forw': '4.4372'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:53epoch=600; Loss Pred=0.3799; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.895; Test Acc=0.7625; Entropy={'forw': '4.5072'}; Entropy_Test={'forw': '4.4673'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:54epoch=800; Loss Pred=0.2252; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.930; Test Acc=0.7900; Entropy={'forw': '4.4292'}; Entropy_Test={'forw': '4.4027'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:55epoch=1000; Loss Pred=0.2733; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.920; Test Acc=0.8050; Entropy={'forw': '4.0342'}; Entropy_Test={'forw': '3.9664'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:56epoch=1200; Loss Pred=0.1863; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.945; Test Acc=0.8100; Entropy={'forw': '4.2288'}; Entropy_Test={'forw': '4.1738'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:57epoch=1400; Loss Pred=0.1497; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.955; Test Acc=0.7900; Entropy={'forw': '4.2420'}; Entropy_Test={'forw': '4.1861'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:58epoch=1600; Loss Pred=1.6969; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.570; Test Acc=0.5525; Entropy={'forw': '4.4026'}; Entropy_Test={'forw': '4.3638'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:59epoch=1800; Loss Pred=0.6278; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.795; Test Acc=0.7150; Entropy={'forw': '4.4272'}; Entropy_Test={'forw': '4.3983'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:07:59epoch=2000; Loss Pred=0.5340; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.835; Test Acc=0.7075; Entropy={'forw': '4.4403'}; Entropy_Test={'forw': '4.3931'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:00epoch=2200; Loss Pred=0.4787; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.840; Test Acc=0.7300; Entropy={'forw': '4.4684'}; Entropy_Test={'forw': '4.4350'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:00epoch=2400; Loss Pred=0.4556; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.865; Test Acc=0.7475; Entropy={'forw': '4.4759'}; Entropy_Test={'forw': '4.4365'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:01epoch=2600; Loss Pred=0.4325; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.875; Test Acc=0.7375; Entropy={'forw': '4.5085'}; Entropy_Test={'forw': '4.4655'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:01epoch=2800; Loss Pred=0.4072; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.885; Test Acc=0.7600; Entropy={'forw': '4.4880'}; Entropy_Test={'forw': '4.4531'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:02epoch=3000; Loss Pred=0.3562; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.910; Test Acc=0.7450; Entropy={'forw': '4.5086'}; Entropy_Test={'forw': '4.4548'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:02epoch=3200; Loss Pred=0.3588; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.915; Test Acc=0.7550; Entropy={'forw': '4.5026'}; Entropy_Test={'forw': '4.4558'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:03epoch=3400; Loss Pred=0.3185; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.920; Test Acc=0.7375; Entropy={'forw': '4.5021'}; Entropy_Test={'forw': '4.4440'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:03epoch=3600; Loss Pred=0.3029; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.930; Test Acc=0.7475; Entropy={'forw': '4.4982'}; Entropy_Test={'forw': '4.4491'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:04epoch=3800; Loss Pred=0.2881; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.935; Test Acc=0.7650; Entropy={'forw': '4.4857'}; Entropy_Test={'forw': '4.4386'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:04epoch=4000; Loss Pred=0.2779; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.935; Test Acc=0.7625; Entropy={'forw': '4.4840'}; Entropy_Test={'forw': '4.4296'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:05epoch=4200; Loss Pred=0.2714; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.935; Test Acc=0.7675; Entropy={'forw': '4.4781'}; Entropy_Test={'forw': '4.4334'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:06epoch=4400; Loss Pred=0.2664; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.935; Test Acc=0.7600; Entropy={'forw': '4.4677'}; Entropy_Test={'forw': '4.4282'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:06epoch=4600; Loss Pred=0.2619; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.935; Test Acc=0.7600; Entropy={'forw': '4.4628'}; Entropy_Test={'forw': '4.4229'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:07epoch=4800; Loss Pred=0.2578; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.940; Test Acc=0.7600; Entropy={'forw': '4.4586'}; Entropy_Test={'forw': '4.4141'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:07epoch=5000; Loss Pred=0.2540; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0048'}; Train Acc=0.940; Test Acc=0.7575; Entropy={'forw': '4.4659'}; Entropy_Test={'forw': '4.4165'}\n",
      "\n",
      "Optimization Finished!\n",
      "********** replication  1  **********\n",
      "reber\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: reber\n",
      "    hid: \t\t\t5,\n",
      "    h_hid: \t\t\t10\n",
      "    n_attractor_iterations: \t0,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.2\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_norm(lambda:0.05)\n",
      "    word_embedding: size\t(100), train(True)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t200/400, SEQ_LEN: 20\n",
      "Logged Successfully: \n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:07epoch=0; Loss Pred=1.0208; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0054'}; Train Acc=0.425; Test Acc=0.4550; Entropy={'forw': '2.6908'}; Entropy_Test={'forw': '2.7018'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:08epoch=200; Loss Pred=0.9367; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0054'}; Train Acc=0.575; Test Acc=0.6025; Entropy={'forw': '2.7942'}; Entropy_Test={'forw': '2.8277'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:09epoch=400; Loss Pred=0.4712; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0054'}; Train Acc=0.860; Test Acc=0.8850; Entropy={'forw': '3.4092'}; Entropy_Test={'forw': '3.4847'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:11epoch=600; Loss Pred=0.2255; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0054'}; Train Acc=0.940; Test Acc=0.9550; Entropy={'forw': '3.3149'}; Entropy_Test={'forw': '3.3473'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:12epoch=800; Loss Pred=0.0924; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0054'}; Train Acc=0.980; Test Acc=0.9650; Entropy={'forw': '3.1393'}; Entropy_Test={'forw': '3.1461'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 23:08:13epoch=1000; Loss Pred=0.0471; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0054'}; Train Acc=0.995; Test Acc=0.9700; Entropy={'forw': '3.1194'}; Entropy_Test={'forw': '3.1284'}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-42ccd166ab61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    414\u001b[0m                             \u001b[0;31m# Optimize all parameters except for attractor weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                             _ = sess.run([pred_train_op],\n\u001b[0;32m--> 416\u001b[0;31m                                          feed_dict={X: batch_x, Y: batch_y})\n\u001b[0m\u001b[1;32m    417\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training_mode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'attractor_on_task'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLOSS_SWITCH_FREQ\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrain_prediction_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python\n",
    "\n",
    "# This version of the code trains the attractor connections with a separate\n",
    "# objective function than the objective function used to train all other weights\n",
    "# in the network (on the prediction task).\n",
    "\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload\n",
    "\n",
    "from tensorflow_helpers import *\n",
    "from data_generator import generate_examples, pick_task\n",
    "from information_trackers import MutInfSaver, WeightSaver, compute_entropy_fullvec, get_mut_inf_for_fullvec, \\\n",
    "    flat_mutual_inf, compute_avg_entropy_vec\n",
    "from helper_functions import get_batches, load_pretrained_embeddings, \\\n",
    "    get_model_type_str, translate_ids_to_words, \\\n",
    "    save_results, print_into_log, print_some_translated_sentences, \\\n",
    "    get_training_progress_comment\n",
    "from graph_init import GRU_attractor, TANH_attractor\n",
    "\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, patience_max, disp_epoch, min_delta = 0.03):\n",
    "        self.best = 1e10\n",
    "        self.patience = 0  # our patience\n",
    "        self.patience_max = patience_max\n",
    "        self.display_epoch = disp_epoch\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def update(self, current):\n",
    "        if self.best > current:\n",
    "            self.best = current\n",
    "            self.patience = 0\n",
    "        elif abs(self.best - current) > self.min_delta:\n",
    "            self.patience += 1\n",
    "\n",
    "    def patience_ran_out(self):\n",
    "        if self.patience*self.display_epoch > self.patience_max:\n",
    "            return True\n",
    "        else:\n",
    "            False\n",
    "            \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "ops = {\n",
    "    'model_type': \"TANH\",  # OPTIONS: GRU, TANH\n",
    "    'hid': 5,\n",
    "    'in': None,  # TBD\n",
    "    'out': 1,\n",
    "    #         'batch_size':n_examples, #since the sequences are 1-dimensional it's easier to just run them all at once\n",
    "    'n_attractor_iterations': 10,\n",
    "    'attractor_dynamics': \"projection2\",  # OPTIONS:  \"\" (for no attractor dynamics),\n",
    "    #           \"direct\" (simple attractor weights applied to hidden states directly, trained with noise addition)\n",
    "    #           \"projection2\" (project the hidden state into a separate space via weights, do attraction, project back)\n",
    "    #           \"helper_hidden\" (hidden-hidden neurons) - IMPORTANT: don't forget to add h_hid number\n",
    "    'h_hid': 10,  # helper hidden for \"helper hidden\" \"attractory_dynamics\" mode\n",
    "    'attractor_noise_level': 0.2,\n",
    "    'attractor_noise_type': \"bernoilli\",  # OPTIONS: \"gaussian\", \"dropout\", \"random_drop\"\n",
    "\n",
    "    'training_mode': \"\",  # 'attractor_on_task',\n",
    "\n",
    "    'attractor_regularization': \"l2_norm\",  # OPTIONS: \"l2_regularization\", \"l2_norm\"\n",
    "    'attractor_regularization_lambda': 0.05,\n",
    "\n",
    "    'record_mutual_information': True,\n",
    "    'problem_type': \"majority\",  # OPTIONS: parity, parity_length, majority, reber, kazakov, pos_brown, ner_german, sentimend_imdb\n",
    "    'masking': False,#\"seq\", \"final\"\n",
    "    'prediction_type': 'final', #'seq', 'final'\n",
    "    'seq_len': 5,\n",
    "\n",
    "    'save_best_model': True,\n",
    "    'reshuffle_data_each_replication': False,  # relevant for POS datasets (since they are loaded from files)\n",
    "    'test_partition': 0.3,\n",
    "    'lrate': 0.008,  # was 0.008\n",
    "\n",
    "    # NLP related (pos_brown task)\n",
    "    'bidirectional': False,\n",
    "    'embedding_size': 100,\n",
    "    'load_word_embeddings': True,\n",
    "    'train_word_embeddings': True,\n",
    "    'input_type': \"embed\",  # embed&prior, embed, prior\n",
    "    'dropout': 0.0  # in range(0,1)\n",
    "}\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# SEQ_LEN = 12 # number of bits in input sequence\n",
    "N_HIDDEN = ops['hid']  # number of hidden units\n",
    "N_H_HIDDEN = ops['h_hid']\n",
    "NOISE_LEVEL = ops['attractor_noise_level']\n",
    "# noise in training attractor net\n",
    "# if >=0, Gaussian with std dev NOISE_LEVEL\n",
    "# if < 0, Bernoulli dropout proportion -NOISE_LEVEL\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "INPUT_NOISE_LEVEL = 0.1\n",
    "# number of time steps in attractor dynamics\n",
    "# if = 0, then no attractor net\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# ATTR_WEIGHT_CONSTRAINTS = True\n",
    "# True: make attractor weights symmetric and have zero diag\n",
    "# False: unconstrained\n",
    "TRAIN_ATTR_WEIGHTS_ON_PREDICTION = False\n",
    "# True: train attractor weights on attractor net _and_ prediction\n",
    "REPORT_BEST_TRAIN_PERFORMANCE = True\n",
    "# True: save the train/test perf on the epoch for which train perf was best\n",
    "LOSS_SWITCH_FREQ = 1\n",
    "# how often (in epochs) to switch between attractor\n",
    "# and prediction loss\n",
    "\n",
    "# Training Parameters\n",
    "\n",
    "TRAINING_EPOCHS = 5000\n",
    "N_REPLICATIONS = 4\n",
    "BATCH_SIZE = 5000\n",
    "DISPLAY_EPOCH = 200\n",
    "EARLY_STOPPING_THRESH = 1e-3\n",
    "EARLY_STOPPING_PATIENCE = 1000  # in epochs\n",
    "\n",
    "# NOTEBOOK CODE\n",
    "WS = WeightSaver()\n",
    "MIS = MutInfSaver()\n",
    "\n",
    "######### MAIN CODE #############################################################\n",
    "#0.02, 0.05, 0.1, 0.2, 0.35, 0.5, \n",
    "\n",
    "for problem in ['majority', 'parity_length', 'reber', 'parity_length_noisy_longer_remainder']: #'parity', 'majority', 'parity_length', 'reber'\n",
    "    ops['problem_type'] = problem\n",
    "    TASK = ops['problem_type']\n",
    "    for training_procedure in ['no_att', 'att', 'att_on_task']: #'no_att',\n",
    "        print(TASK, training_procedure)\n",
    "        if training_procedure == 'no_att':\n",
    "            ops['n_attractor_iterations'] = 0\n",
    "            ops['training_mode'] = ''\n",
    "        elif training_procedure == 'att':\n",
    "            ops['n_attractor_iterations'] = 10\n",
    "            ops['training_mode'] = ''\n",
    "            ops['attractor_regularization_lambda'] = 0.5\n",
    "        elif training_procedure == 'att_on_task':\n",
    "            ops['n_attractor_iterations'] = 10\n",
    "            ops['training_mode'] = 'attractor_on_task'\n",
    "            ops['attractor_regularization_lambda'] = 0.0\n",
    "        ops, SEQ_LEN, N_INPUT, N_CLASSES, N_TRAIN, N_TEST = pick_task(ops['problem_type'],\n",
    "                                                              ops)  # task (parity, majority, reber, kazakov)\n",
    "        N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "        ARCH = ops['model_type']  # hidden layer type: 'GRU' or 'tanh'\n",
    "        ATTRACTOR_TYPE = ops['attractor_dynamics']\n",
    "        # the tf seed needs to be within the context of the graph.\n",
    "        tf.reset_default_graph()\n",
    "    #     np.random.seed(100)\n",
    "    #     tf.set_random_seed(100)\n",
    "    #         ops['n_attractor_iterations'] = attractor_steps\n",
    "    #         N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "        #\n",
    "        # PLACEHOLDERS\n",
    "        #\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            # last dimension is left singular, tensorflow will expect it to be an id number, not 1-hot embed\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN], name='Y')\n",
    "        elif ops['problem_type'] == 'ner_german':\n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN])\n",
    "        else:  # single output\n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"float\", [None, N_CLASSES])\n",
    "            \n",
    "        if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "            X_longer = tf.placeholder(\"float\", [None, SEQ_LEN*3, N_INPUT])\n",
    "        attractor_tgt_net = tf.placeholder(\"float\", [None, N_HIDDEN], name='attractor_tgt')\n",
    "\n",
    "\n",
    "\n",
    "        # Graph + all the training variables\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            net_inputs = {'X': embed, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        else:\n",
    "            net_inputs = {'X': X, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "            \n",
    "        if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "            net_inputs_longer = {'X': X_longer, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "            \n",
    "        if ops['bidirectional']:\n",
    "            G_attractors = {'forw': [], 'back': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            # Backward:\n",
    "            G_back = GRU_attractor(ops, inputs=net_inputs, direction='backward', suffix=names[1])\n",
    "            attr_loss_op_back = G_back.attr_loss_op\n",
    "            attr_train_op_back = G_back.attr_train_op\n",
    "            h_clean_seq_flat_back = G_back.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_back = G_back.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['back'] = {'attr_loss_op': attr_loss_op_back, \"attr_train_op\": attr_train_op_back,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_back, 'h_net_seq_flat': h_net_seq_flat_back}\n",
    "            G_back_output = G_back.output\n",
    "\n",
    "            # Merge: [seq_len, batch_size, n_hid*2]\n",
    "            # Note that we reverse the backward cell's output to align with original direction\n",
    "            output = tf.concat([G_forw_output, tf.reverse(G_back_output, axis=[0])], axis=2)\n",
    "\n",
    "            if ops['dropout'] > 0.0:\n",
    "                # note keep_prob = 1.0 - drop_probability (not sure why they implemented it this way)\n",
    "                # tensorflow implementation scales by 1/keep_prob automatically\n",
    "                output_dropped = tf.nn.dropout(output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output_dropped = output\n",
    "\n",
    "            input_size_final_projection = 2 * ops['hid']\n",
    "            Y_ = project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "\n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "        else:\n",
    "            # TODO: change to map with just ont entry as well.\n",
    "            G_attractors = {'forw': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            if ops['model_type'] == 'GRU':\n",
    "                G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            elif ops['model_type'] == 'TANH':\n",
    "                G_forw = TANH_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "                if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "                    G_forw_longer = TANH_attractor(ops, inputs=net_inputs_longer, direction='forward', suffix=names[0], reuse=True)\n",
    "                \n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            output = G_forw.output\n",
    "            if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "                output_longer = G_forw_longer.output\n",
    "\n",
    "            h_net_seq_flat = G_forw.h_net_seq_flat # pure cell ouptut (before attractor was applied)\n",
    "            h_attractor_collection_flat = G_forw.h_attractor_collection\n",
    "            h_clean_seq_flat = G_forw.h_clean_seq_flat\n",
    "            input_size_final_projection = ops['hid']\n",
    "            Y_ = project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "            if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "                Y__longer = project_into_output(Y, output_longer, input_size_final_projection, ops['out'], ops, reuse=True)\n",
    "\n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            if ops['training_mode'] == 'attractor_on_task':\n",
    "                print_into_log(LOG_DIRECTORY, \"adding attractor params to task training op\")\n",
    "                prediction_parameters += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"ATTRACTOR_WEIGHTS\")\n",
    "                print(prediction_parameters)\n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "            if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "                accuracy_longer = task_accuracy(Y, Y__longer, ops)\n",
    "\n",
    "        mask_op = tf.cast(tf.sign(Y), dtype=tf.float32)\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # TODO: make a class for all \"best\" quantities (a lot of space)\n",
    "            saved_train_acc = []\n",
    "            saved_test_acc = []\n",
    "            saved_epoch = []\n",
    "            saved_att_loss = []\n",
    "            saved_entropy_final = []\n",
    "            saved_entropy_final_test = []\n",
    "            saved_val_acc = []\n",
    "            saved_val_loss = []\n",
    "            saved_traini_loss = []\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # Start training\n",
    "            for replication in range(N_REPLICATIONS):\n",
    "                print(\"********** replication \", replication, \" **********\")\n",
    "                early_stopper = EarlyStopper(EARLY_STOPPING_PATIENCE, DISPLAY_EPOCH)\n",
    "                [X_train, Y_train, X_test, Y_test, X_val, Y_val, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                                                           INPUT_NOISE_LEVEL, TASK, ops)\n",
    "\n",
    "                if len(X_test) ==3 :\n",
    "                    N_TRAIN_print = len(X_train)\n",
    "                    N_TEST_print = ', '.join([str(len(arr)) for arr in X_test])\n",
    "                else:\n",
    "                    N_TRAIN_print = N_TRAIN\n",
    "                    N_TEST_print = N_TEST\n",
    "                # Log Path init-n:\n",
    "                COMMENT = 'training_procedure_test'\n",
    "                MODEL_NAME_FILE = '{}_{}.txt'.format(ops['problem_type'], COMMENT)\n",
    "                LOG_DIRECTORY = 'experiments/logs/{}'.format(MODEL_NAME_FILE)\n",
    "                MODEL_DIRECTORY = 'experiments/logs/{}_{}'.format(datetime.date.today(), MODEL_NAME_FILE)\n",
    "                print_into_log(LOG_DIRECTORY, get_model_type_str(ops, N_TRAIN_print, N_TEST_print, SEQ_LEN) + ops['training_mode'])\n",
    "                print_into_log(MODEL_DIRECTORY, get_model_type_str(ops, N_TRAIN_print, N_TEST_print, SEQ_LEN), supress=True)\n",
    "                sess.run(init)  # Run the initializer\n",
    "\n",
    "                train_prediction_loss = True\n",
    "                best_train_acc = -1000.\n",
    "                best_test_acc = 0\n",
    "                best_entropy = 0.0\n",
    "                best_entropy_test = 0.0\n",
    "                best_att_loss = 0\n",
    "                best_train_loss = 0\n",
    "                best_val_loss = 0.0\n",
    "                best_val_acc = 0.0\n",
    "                best_epoch = 0\n",
    "                for epoch in range(1, TRAINING_EPOCHS + 2):\n",
    "                    if (epoch - 1) % DISPLAY_EPOCH == 0:\n",
    "                        # TRAIN set:\n",
    "                        ploss, train_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        # TEST set:\n",
    "                        if len(X_test) ==3:\n",
    "                            test_acc = \"leftover:{:.3f}, noisy_train:{:.3f}, longer:{:.3f}\".format(batch_tensor_collect(sess, [accuracy], X, Y, X_test[0], Y_test[0], BATCH_SIZE)[0],\n",
    "                                                                                      batch_tensor_collect(sess, [accuracy], X, Y, X_test[1], Y_test[1], BATCH_SIZE)[0],\n",
    "                                                                                      batch_tensor_collect(sess, [accuracy_longer], X_longer, Y, X_test[2], Y_test[2], BATCH_SIZE)[0],)\n",
    "                        else:\n",
    "                            test_acc = batch_tensor_collect(sess, [accuracy], X, Y, X_test, Y_test, BATCH_SIZE)[0]\n",
    "\n",
    "    #                         # Validation set & Early stopping:\n",
    "    #                         ploss_val, val_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "    #                                                                   X, Y, X_val, Y_val, BATCH_SIZE)\n",
    "\n",
    "                       \n",
    "\n",
    "                        # ATTRACTOR(s) LOSS\n",
    "                        aloss = {}\n",
    "                        entropy = {}\n",
    "                        entropy_test = {}\n",
    "                        hid_vals_arr = batch_tensor_collect(sess, [A['h_net_seq_flat'] for att_name, A in\n",
    "                                                                   G_attractors.items()],\n",
    "                                                            X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        h_clean_val_arr = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        if len(X_test) ==3:\n",
    "                            h_clean_val_arr_test = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_test[0], Y_test[0], BATCH_SIZE)\n",
    "                        else:\n",
    "                            h_clean_val_arr_test = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_test, Y_test, BATCH_SIZE)\n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            A = G_attractors[attractor_name]\n",
    "                            a_loss_val = []\n",
    "                            n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "                            for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "                                a_loss_val.append(\n",
    "                                    sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "                            aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "\n",
    "                            entropy[attractor_name] = \"{:.4f}\".format(\n",
    "                                compute_entropy_fullvec(h_clean_val_arr[i], ops, n_bins=8))\n",
    "                            \n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            entropy_test[attractor_name] = \"{:.4f}\".format(\n",
    "                                compute_entropy_fullvec(h_clean_val_arr_test[i], ops, n_bins=8))    \n",
    "\n",
    "                        # Print training information:\n",
    "                        print_into_log(LOG_DIRECTORY, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + get_training_progress_comment(epoch, ploss, aloss, 0.0, 0.0, train_acc,\n",
    "                                                                     test_acc, entropy, entropy_test))\n",
    "                        # Update the logs:\n",
    "                        WS.update_conservative(epoch_number=epoch, loss_att=aloss,\n",
    "                                               loss_task=ploss, acc=test_acc, entropy=entropy)\n",
    "                        if ops['record_mutual_information']:\n",
    "                            h_attractor_val, h_clean_val = batch_tensor_collect(sess, [h_attractor_collection_flat, h_clean_seq_flat],\n",
    "                                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                            MIS.update(ploss, aloss, train_acc, test_acc, np.tanh(hid_vals_arr[0]), h_attractor_val, h_clean_val)\n",
    "\n",
    "                        if (train_acc > best_train_acc):\n",
    "                            best_train_acc = train_acc\n",
    "                            best_test_acc = test_acc\n",
    "                            best_att_loss = aloss\n",
    "                            best_epoch = epoch\n",
    "    #                         best_val_acc = val_acc\n",
    "\n",
    "    #                         best_val_loss = ploss_val\n",
    "    #                         best_train_loss = ploss\n",
    "                            if ops['save_best_model']:\n",
    "                                save_path = saver.save(sess, MODEL_DIRECTORY)\n",
    "                            best_entropy = entropy\n",
    "                            best_entropy_test = entropy_test\n",
    "                        if (train_acc == 1.0):\n",
    "                            print(\"Reached Peak!\")\n",
    "                            break\n",
    "                    if epoch > 1 and LOSS_SWITCH_FREQ > 0 \\\n",
    "                            and (epoch - 1) % LOSS_SWITCH_FREQ == 0:\n",
    "                        train_prediction_loss = not train_prediction_loss\n",
    "\n",
    "                    # MODEL TRAINING\n",
    "                    batches = get_batches(BATCH_SIZE, X_train, Y_train)\n",
    "                    for (batch_x, batch_y) in batches:\n",
    "                        if (LOSS_SWITCH_FREQ == 0 or train_prediction_loss):\n",
    "                            # Optimize all parameters except for attractor weights\n",
    "                            _ = sess.run([pred_train_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y})\n",
    "                        if ops['training_mode'] != 'attractor_on_task':\n",
    "                            if (LOSS_SWITCH_FREQ == 0 or not train_prediction_loss):\n",
    "                                if (N_ATTRACTOR_STEPS > 0):\n",
    "                                    # ATTRACTOR(s) training\n",
    "                                    for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                        A = G_attractors[attractor_name]\n",
    "                                        _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: hid_vals_arr[i]})\n",
    "\n",
    "                print(\"Optimization Finished!\")\n",
    "\n",
    "                if (REPORT_BEST_TRAIN_PERFORMANCE):\n",
    "                    saved_train_acc.append(best_train_acc)\n",
    "                    saved_test_acc.append(best_test_acc)\n",
    "                    saved_att_loss.append(best_att_loss)\n",
    "                    saved_entropy_final.append(best_entropy)\n",
    "                    saved_entropy_final_test.append(best_entropy_test)\n",
    "                    saved_epoch.append(best_epoch)\n",
    "\n",
    "    #                 saved_val_acc.append(best_val_acc)\n",
    "    #                 saved_val_loss.append(best_val_loss)\n",
    "                    saved_traini_loss.append(best_train_loss)\n",
    "                else:\n",
    "                    saved_train_acc.append(train_acc)\n",
    "                    saved_test_acc.append(test_acc)\n",
    "                    #             saved_att_loss.append(aloss)\n",
    "\n",
    "            if ops['training_mode'] == 'attractor_on_task':\n",
    "                COMMENT += \"<attractor on task loss>\"\n",
    "            save_results(ops, saved_epoch, saved_train_acc, saved_test_acc, saved_att_loss, saved_entropy_final, saved_val_acc,\n",
    "                 saved_val_loss, saved_traini_loss, N_TRAIN_print, N_TEST_print, SEQ_LEN, COMMENT, saved_entropy_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
