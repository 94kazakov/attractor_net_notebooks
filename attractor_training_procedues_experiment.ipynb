{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "parity att\n",
      "L2 norm\n",
      "Logged Successfully: \n",
      "adding attractor params to task training op\n",
      "[<tf.Variable 'TASK_WEIGHTS/forw/W_in:0' shape=(1, 5) dtype=float32_ref>, <tf.Variable 'TASK_WEIGHTS/forw/W_rec:0' shape=(5, 5) dtype=float32_ref>, <tf.Variable 'TASK_WEIGHTS/forw/b_rec:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'TASK_WEIGHTS/b_out:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'TASK_WEIGHTS/W_out:0' shape=(5, 1) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_W_in:0' shape=(5, 10) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_Wout:0' shape=(10, 5) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_b_out:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_W:0' shape=(10, 10) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_b:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_scale:0' shape=(1,) dtype=float32_ref>]\n",
      "********** replication  0  **********\n",
      "parity\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: parity\n",
      "    hid: \t\t\t5,\n",
      "    h_hid: \t\t\t10\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_norm(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t256/768, SEQ_LEN: 10\n",
      "Logged Successfully: \n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:22epoch=0; Loss Pred=1.1040; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0368'}; Train Acc=0.484; Test Acc=0.5137; Entropy={'forw': '2.5494'}; Entropy_Test={'forw': '3.5266'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:24epoch=200; Loss Pred=0.9519; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.9239'}; Train Acc=0.578; Test Acc=0.4954; Entropy={'forw': '3.1198'}; Entropy_Test={'forw': '3.6900'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:27epoch=400; Loss Pred=0.9358; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6631'}; Train Acc=0.598; Test Acc=0.5117; Entropy={'forw': '2.7868'}; Entropy_Test={'forw': '3.2981'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:30epoch=600; Loss Pred=0.7941; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0641'}; Train Acc=0.691; Test Acc=0.6178; Entropy={'forw': '3.3359'}; Entropy_Test={'forw': '3.8104'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:32epoch=800; Loss Pred=0.7314; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.8040'}; Train Acc=0.727; Test Acc=0.7012; Entropy={'forw': '3.1170'}; Entropy_Test={'forw': '3.7239'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:35epoch=1000; Loss Pred=0.7263; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.8620'}; Train Acc=0.727; Test Acc=0.7292; Entropy={'forw': '2.8721'}; Entropy_Test={'forw': '3.2281'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:37epoch=1200; Loss Pred=0.7211; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.8571'}; Train Acc=0.684; Test Acc=0.6523; Entropy={'forw': '3.2882'}; Entropy_Test={'forw': '3.6531'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:39epoch=1400; Loss Pred=0.7567; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.8271'}; Train Acc=0.711; Test Acc=0.6439; Entropy={'forw': '3.2898'}; Entropy_Test={'forw': '3.6441'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:42epoch=1600; Loss Pred=0.7940; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0907'}; Train Acc=0.684; Test Acc=0.6100; Entropy={'forw': '3.2501'}; Entropy_Test={'forw': '3.7197'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:44epoch=1800; Loss Pred=0.7833; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5843'}; Train Acc=0.703; Test Acc=0.6497; Entropy={'forw': '3.2698'}; Entropy_Test={'forw': '3.7394'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:46epoch=2000; Loss Pred=0.7136; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6688'}; Train Acc=0.695; Test Acc=0.6751; Entropy={'forw': '3.1183'}; Entropy_Test={'forw': '3.6690'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:49epoch=2200; Loss Pred=0.6870; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4731'}; Train Acc=0.758; Test Acc=0.7422; Entropy={'forw': '3.1472'}; Entropy_Test={'forw': '3.6062'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:51epoch=2400; Loss Pred=0.6355; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4189'}; Train Acc=0.777; Test Acc=0.7715; Entropy={'forw': '2.9722'}; Entropy_Test={'forw': '3.3713'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:54epoch=2600; Loss Pred=0.9995; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6059'}; Train Acc=0.520; Test Acc=0.5059; Entropy={'forw': '2.4184'}; Entropy_Test={'forw': '2.6033'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:56epoch=2800; Loss Pred=0.9859; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3072'}; Train Acc=0.512; Test Acc=0.5195; Entropy={'forw': '2.2153'}; Entropy_Test={'forw': '2.3564'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:54:59epoch=3000; Loss Pred=0.9872; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2447'}; Train Acc=0.520; Test Acc=0.4648; Entropy={'forw': '2.0204'}; Entropy_Test={'forw': '2.2361'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:01epoch=3200; Loss Pred=0.9819; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2464'}; Train Acc=0.559; Test Acc=0.5124; Entropy={'forw': '2.0284'}; Entropy_Test={'forw': '2.2002'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:04epoch=3400; Loss Pred=0.9867; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2502'}; Train Acc=0.559; Test Acc=0.4707; Entropy={'forw': '2.0925'}; Entropy_Test={'forw': '2.1760'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:06epoch=3600; Loss Pred=0.9917; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2359'}; Train Acc=0.516; Test Acc=0.4655; Entropy={'forw': '1.4249'}; Entropy_Test={'forw': '1.6402'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:08epoch=3800; Loss Pred=0.9922; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1495'}; Train Acc=0.547; Test Acc=0.5065; Entropy={'forw': '1.3516'}; Entropy_Test={'forw': '1.4637'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:11epoch=4000; Loss Pred=0.9922; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1369'}; Train Acc=0.539; Test Acc=0.5130; Entropy={'forw': '1.3892'}; Entropy_Test={'forw': '1.3948'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:13epoch=4200; Loss Pred=0.9922; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1207'}; Train Acc=0.516; Test Acc=0.4701; Entropy={'forw': '1.3932'}; Entropy_Test={'forw': '1.3940'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:16epoch=4400; Loss Pred=0.9926; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1436'}; Train Acc=0.516; Test Acc=0.4714; Entropy={'forw': '1.3375'}; Entropy_Test={'forw': '1.4717'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:18epoch=4600; Loss Pred=0.9946; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2689'}; Train Acc=0.516; Test Acc=0.4635; Entropy={'forw': '1.6787'}; Entropy_Test={'forw': '1.8090'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:20epoch=4800; Loss Pred=1.0008; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5455'}; Train Acc=0.516; Test Acc=0.4681; Entropy={'forw': '1.3416'}; Entropy_Test={'forw': '1.4670'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:23epoch=5000; Loss Pred=0.9961; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1745'}; Train Acc=0.516; Test Acc=0.4622; Entropy={'forw': '1.3977'}; Entropy_Test={'forw': '1.4463'}\n",
      "\n",
      "Optimization Finished!\n",
      "********** replication  1  **********\n",
      "parity\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: parity\n",
      "    hid: \t\t\t5,\n",
      "    h_hid: \t\t\t10\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_norm(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t256/768, SEQ_LEN: 10\n",
      "Logged Successfully: \n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:23epoch=0; Loss Pred=1.1459; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0998'}; Train Acc=0.461; Test Acc=0.4688; Entropy={'forw': '1.5288'}; Entropy_Test={'forw': '2.1877'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:26epoch=200; Loss Pred=0.9553; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '2.0017'}; Train Acc=0.582; Test Acc=0.5111; Entropy={'forw': '3.1538'}; Entropy_Test={'forw': '3.8381'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Successfully: \n",
      "2018-07-16 12:55:28epoch=400; Loss Pred=0.9026; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.2056'}; Train Acc=0.621; Test Acc=0.5091; Entropy={'forw': '3.6311'}; Entropy_Test={'forw': '4.1394'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:31epoch=600; Loss Pred=0.8672; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.9325'}; Train Acc=0.648; Test Acc=0.5013; Entropy={'forw': '3.8498'}; Entropy_Test={'forw': '4.3530'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:33epoch=800; Loss Pred=0.8655; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.8479'}; Train Acc=0.660; Test Acc=0.4941; Entropy={'forw': '3.9949'}; Entropy_Test={'forw': '4.4787'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:36epoch=1000; Loss Pred=0.8728; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6336'}; Train Acc=0.656; Test Acc=0.4902; Entropy={'forw': '4.0442'}; Entropy_Test={'forw': '4.5473'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:38epoch=1200; Loss Pred=0.8547; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6604'}; Train Acc=0.648; Test Acc=0.5039; Entropy={'forw': '4.0793'}; Entropy_Test={'forw': '4.5383'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:41epoch=1400; Loss Pred=0.8761; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6514'}; Train Acc=0.633; Test Acc=0.4967; Entropy={'forw': '4.2075'}; Entropy_Test={'forw': '4.5600'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:43epoch=1600; Loss Pred=0.8859; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6824'}; Train Acc=0.613; Test Acc=0.5104; Entropy={'forw': '4.3196'}; Entropy_Test={'forw': '4.7370'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:45epoch=1800; Loss Pred=0.8590; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6265'}; Train Acc=0.645; Test Acc=0.4850; Entropy={'forw': '4.2568'}; Entropy_Test={'forw': '4.6326'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:48epoch=2000; Loss Pred=0.8631; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.7245'}; Train Acc=0.625; Test Acc=0.5137; Entropy={'forw': '4.2804'}; Entropy_Test={'forw': '4.7277'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:50epoch=2200; Loss Pred=0.8616; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5604'}; Train Acc=0.660; Test Acc=0.5033; Entropy={'forw': '4.1400'}; Entropy_Test={'forw': '4.5366'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:53epoch=2400; Loss Pred=0.9390; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0215'}; Train Acc=0.609; Test Acc=0.5026; Entropy={'forw': '3.9450'}; Entropy_Test={'forw': '4.3544'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:55epoch=2600; Loss Pred=0.9809; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5546'}; Train Acc=0.566; Test Acc=0.5156; Entropy={'forw': '3.5844'}; Entropy_Test={'forw': '4.0299'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:55:57epoch=2800; Loss Pred=0.9546; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4469'}; Train Acc=0.578; Test Acc=0.5176; Entropy={'forw': '3.6068'}; Entropy_Test={'forw': '3.9168'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:00epoch=3000; Loss Pred=0.9372; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4087'}; Train Acc=0.602; Test Acc=0.5286; Entropy={'forw': '3.6954'}; Entropy_Test={'forw': '4.0182'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:02epoch=3200; Loss Pred=0.9319; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4224'}; Train Acc=0.617; Test Acc=0.5156; Entropy={'forw': '3.5553'}; Entropy_Test={'forw': '3.9269'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:05epoch=3400; Loss Pred=0.9260; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4739'}; Train Acc=0.617; Test Acc=0.5260; Entropy={'forw': '3.6567'}; Entropy_Test={'forw': '3.9367'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:07epoch=3600; Loss Pred=0.9866; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5975'}; Train Acc=0.559; Test Acc=0.4922; Entropy={'forw': '3.2281'}; Entropy_Test={'forw': '3.4754'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:10epoch=3800; Loss Pred=0.9145; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.7394'}; Train Acc=0.613; Test Acc=0.5098; Entropy={'forw': '3.8306'}; Entropy_Test={'forw': '4.1243'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:12epoch=4000; Loss Pred=0.9167; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4853'}; Train Acc=0.625; Test Acc=0.5260; Entropy={'forw': '3.7218'}; Entropy_Test={'forw': '4.0291'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:14epoch=4200; Loss Pred=0.9245; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4265'}; Train Acc=0.609; Test Acc=0.5254; Entropy={'forw': '3.3924'}; Entropy_Test={'forw': '3.7449'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:17epoch=4400; Loss Pred=0.9017; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3972'}; Train Acc=0.652; Test Acc=0.5176; Entropy={'forw': '3.3842'}; Entropy_Test={'forw': '3.6538'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:19epoch=4600; Loss Pred=0.9172; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4765'}; Train Acc=0.625; Test Acc=0.5280; Entropy={'forw': '3.4284'}; Entropy_Test={'forw': '3.7501'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:22epoch=4800; Loss Pred=0.9213; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5688'}; Train Acc=0.648; Test Acc=0.5215; Entropy={'forw': '3.5783'}; Entropy_Test={'forw': '3.7743'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:24epoch=5000; Loss Pred=0.9163; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.7813'}; Train Acc=0.641; Test Acc=0.5345; Entropy={'forw': '3.3625'}; Entropy_Test={'forw': '3.7822'}\n",
      "\n",
      "Optimization Finished!\n",
      "********** replication  2  **********\n",
      "parity\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: parity\n",
      "    hid: \t\t\t5,\n",
      "    h_hid: \t\t\t10\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_norm(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t256/768, SEQ_LEN: 10\n",
      "Logged Successfully: \n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:24epoch=0; Loss Pred=1.0034; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.9795'}; Train Acc=0.512; Test Acc=0.5130; Entropy={'forw': '1.9654'}; Entropy_Test={'forw': '2.7097'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:27epoch=200; Loss Pred=0.9738; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0459'}; Train Acc=0.547; Test Acc=0.4987; Entropy={'forw': '2.0330'}; Entropy_Test={'forw': '2.6159'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:29epoch=400; Loss Pred=0.8857; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.2918'}; Train Acc=0.652; Test Acc=0.5013; Entropy={'forw': '3.5745'}; Entropy_Test={'forw': '4.2401'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:32epoch=600; Loss Pred=0.9394; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.7702'}; Train Acc=0.602; Test Acc=0.4883; Entropy={'forw': '4.0244'}; Entropy_Test={'forw': '4.5616'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:34epoch=800; Loss Pred=0.8868; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6918'}; Train Acc=0.648; Test Acc=0.4954; Entropy={'forw': '4.0960'}; Entropy_Test={'forw': '4.5440'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:37epoch=1000; Loss Pred=0.9024; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.7826'}; Train Acc=0.660; Test Acc=0.4967; Entropy={'forw': '4.1189'}; Entropy_Test={'forw': '4.7548'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:39epoch=1200; Loss Pred=0.9159; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6168'}; Train Acc=0.641; Test Acc=0.4967; Entropy={'forw': '3.9958'}; Entropy_Test={'forw': '4.5899'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:42epoch=1400; Loss Pred=0.9203; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6813'}; Train Acc=0.637; Test Acc=0.4954; Entropy={'forw': '3.6898'}; Entropy_Test={'forw': '4.1818'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:44epoch=1600; Loss Pred=0.8760; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6386'}; Train Acc=0.645; Test Acc=0.5020; Entropy={'forw': '3.7973'}; Entropy_Test={'forw': '4.1498'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:47epoch=1800; Loss Pred=0.9196; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5168'}; Train Acc=0.641; Test Acc=0.5000; Entropy={'forw': '3.6694'}; Entropy_Test={'forw': '4.1374'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Successfully: \n",
      "2018-07-16 12:56:49epoch=2000; Loss Pred=0.9057; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5867'}; Train Acc=0.648; Test Acc=0.4987; Entropy={'forw': '3.8556'}; Entropy_Test={'forw': '4.2821'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:52epoch=2200; Loss Pred=0.9127; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5666'}; Train Acc=0.648; Test Acc=0.4909; Entropy={'forw': '3.8430'}; Entropy_Test={'forw': '4.4223'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:54epoch=2400; Loss Pred=0.8757; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5498'}; Train Acc=0.668; Test Acc=0.4987; Entropy={'forw': '3.6551'}; Entropy_Test={'forw': '4.3283'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:57epoch=2600; Loss Pred=0.9180; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6738'}; Train Acc=0.637; Test Acc=0.4928; Entropy={'forw': '3.7421'}; Entropy_Test={'forw': '4.2253'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:56:59epoch=2800; Loss Pred=0.9018; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5629'}; Train Acc=0.652; Test Acc=0.4980; Entropy={'forw': '3.6439'}; Entropy_Test={'forw': '4.1820'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:57:01epoch=3000; Loss Pred=0.8829; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5541'}; Train Acc=0.656; Test Acc=0.5059; Entropy={'forw': '3.9601'}; Entropy_Test={'forw': '4.4247'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-07-16 12:57:04epoch=3200; Loss Pred=0.8600; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4957'}; Train Acc=0.680; Test Acc=0.4948; Entropy={'forw': '3.8414'}; Entropy_Test={'forw': '4.3440'}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-74cd0df610c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    426\u001b[0m                                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattractor_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_attractors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                                         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_attractors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattractor_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                                         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attr_train_op'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mattractor_tgt_net\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhid_vals_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimization Finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# This version of the code trains the attractor connections with a separate\n",
    "# objective function than the objective function used to train all other weights\n",
    "# in the network (on the prediction task).\n",
    "\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload\n",
    "\n",
    "from tensorflow_helpers import *\n",
    "from data_generator import generate_examples, pick_task\n",
    "from information_trackers import MutInfSaver, WeightSaver, compute_entropy_fullvec, get_mut_inf_for_fullvec, \\\n",
    "    flat_mutual_inf, compute_avg_entropy_vec\n",
    "from helper_functions import get_batches, load_pretrained_embeddings, \\\n",
    "    get_model_type_str, translate_ids_to_words, \\\n",
    "    save_results, print_into_log, print_some_translated_sentences, \\\n",
    "    get_training_progress_comment\n",
    "from graph_init import GRU_attractor, TANH_attractor\n",
    "\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, patience_max, disp_epoch, min_delta = 0.03):\n",
    "        self.best = 1e10\n",
    "        self.patience = 0  # our patience\n",
    "        self.patience_max = patience_max\n",
    "        self.display_epoch = disp_epoch\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def update(self, current):\n",
    "        if self.best > current:\n",
    "            self.best = current\n",
    "            self.patience = 0\n",
    "        elif abs(self.best - current) > self.min_delta:\n",
    "            self.patience += 1\n",
    "\n",
    "    def patience_ran_out(self):\n",
    "        if self.patience*self.display_epoch > self.patience_max:\n",
    "            return True\n",
    "        else:\n",
    "            False\n",
    "            \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "ops = {\n",
    "    'model_type': \"TANH\",  # OPTIONS: GRU, TANH\n",
    "    'hid': 5,\n",
    "    'in': None,  # TBD\n",
    "    'out': 1,\n",
    "    #         'batch_size':n_examples, #since the sequences are 1-dimensional it's easier to just run them all at once\n",
    "    'n_attractor_iterations': 15,\n",
    "    'attractor_dynamics': \"projection2\",  # OPTIONS:  \"\" (for no attractor dynamics),\n",
    "    #           \"direct\" (simple attractor weights applied to hidden states directly, trained with noise addition)\n",
    "    #           \"projection2\" (project the hidden state into a separate space via weights, do attraction, project back)\n",
    "    #           \"helper_hidden\" (hidden-hidden neurons) - IMPORTANT: don't forget to add h_hid number\n",
    "    'h_hid': 10,  # helper hidden for \"helper hidden\" \"attractory_dynamics\" mode\n",
    "    'attractor_noise_level': 0.5,\n",
    "    'attractor_noise_type': \"bernoilli\",  # OPTIONS: \"gaussian\", \"dropout\", \"random_drop\"\n",
    "\n",
    "    'train_attr_weights_on_pred': True, \n",
    "\n",
    "    'attractor_regularization': \"l2_norm\",  # OPTIONS: \"l2_regularization\", \"l2_norm\"\n",
    "    'attractor_regularization_lambda': 0.0,\n",
    "\n",
    "    'record_mutual_information': True,\n",
    "    'problem_type': \"majority\",  # OPTIONS: parity, parity_length, majority, reber, kazakov, pos_brown, ner_german, sentimend_imdb\n",
    "    'masking': False,#\"seq\", \"final\"\n",
    "    'prediction_type': 'final', #'seq', 'final'\n",
    "    'seq_len': 5,\n",
    "\n",
    "    'save_best_model': True,\n",
    "    'reshuffle_data_each_replication': False,  # relevant for POS datasets (since they are loaded from files)\n",
    "    'test_partition': 0.3,\n",
    "    'lrate': 0.008,  # was 0.008\n",
    "\n",
    "    # NLP related (pos_brown task)\n",
    "    'bidirectional': False,\n",
    "    'embedding_size': 100,\n",
    "    'load_word_embeddings': False,\n",
    "    'train_word_embeddings': False,\n",
    "    'input_type': \"embed\",  # embed&prior, embed, prior\n",
    "    'dropout': 0.0  # in range(0,1)\n",
    "}\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# SEQ_LEN = 12 # number of bits in input sequence\n",
    "N_HIDDEN = ops['hid']  # number of hidden units\n",
    "N_H_HIDDEN = ops['h_hid']\n",
    "NOISE_LEVEL = ops['attractor_noise_level']\n",
    "# noise in training attractor net\n",
    "# if >=0, Gaussian with std dev NOISE_LEVEL\n",
    "# if < 0, Bernoulli dropout proportion -NOISE_LEVEL\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "INPUT_NOISE_LEVEL = 0.1\n",
    "# number of time steps in attractor dynamics\n",
    "# if = 0, then no attractor net\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# ATTR_WEIGHT_CONSTRAINTS = True\n",
    "# True: make attractor weights symmetric and have zero diag\n",
    "# False: unconstrained\n",
    "TRAIN_ATTR_WEIGHTS_ON_PREDICTION = False\n",
    "# True: train attractor weights on attractor net _and_ prediction\n",
    "REPORT_BEST_TRAIN_PERFORMANCE = True\n",
    "# True: save the train/test perf on the epoch for which train perf was best\n",
    "LOSS_SWITCH_FREQ = 1\n",
    "# how often (in epochs) to switch between attractor\n",
    "# and prediction loss\n",
    "\n",
    "# Training Parameters\n",
    "\n",
    "TRAINING_EPOCHS = 5000\n",
    "N_REPLICATIONS = 4\n",
    "BATCH_SIZE = 5000\n",
    "DISPLAY_EPOCH = 200\n",
    "EARLY_STOPPING_THRESH = 1e-3\n",
    "EARLY_STOPPING_PATIENCE = 1000  # in epochs\n",
    "\n",
    "\n",
    "LOG_DIRECTORY = 'experiments/logs/{}'.format('file.txt')\n",
    "# NOTEBOOK CODE\n",
    "WS = WeightSaver()\n",
    "MIS = MutInfSaver()\n",
    "\n",
    "######### MAIN CODE #############################################################\n",
    "#0.02, 0.05, 0.1, 0.2, 0.35, 0.5, \n",
    "\n",
    "for problem in ['parity', 'parity_length', 'majority', 'reber', 'parity_length_noisy_longer_remainder']: #'parity', 'majority', 'parity_length', 'reber'\n",
    "    ops['problem_type'] = problem\n",
    "    TASK = ops['problem_type']\n",
    "    for training_procedure in ['att', 'no_att']: #'no_att', 'att_on_task'\n",
    "        print(TASK, training_procedure)\n",
    "        if training_procedure == 'no_att':\n",
    "            ops['n_attractor_iterations'] = 0\n",
    "            ops['training_mode'] = ''\n",
    "        elif training_procedure == 'att':\n",
    "            ops['n_attractor_iterations'] = 15\n",
    "            ops['training_mode'] = ''\n",
    "            ops['attractor_regularization_lambda'] = 0.0\n",
    "        elif training_procedure == 'att_on_task':\n",
    "            ops['n_attractor_iterations'] = 10\n",
    "            ops['training_mode'] = 'attractor_on_task'\n",
    "            ops['attractor_regularization_lambda'] = 0.0\n",
    "        ops, SEQ_LEN, N_INPUT, N_CLASSES, N_TRAIN, N_TEST = pick_task(ops['problem_type'],\n",
    "                                                              ops)  # task (parity, majority, reber, kazakov)\n",
    "        N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "        ARCH = ops['model_type']  # hidden layer type: 'GRU' or 'tanh'\n",
    "        ATTRACTOR_TYPE = ops['attractor_dynamics']\n",
    "        # the tf seed needs to be within the context of the graph.\n",
    "        tf.reset_default_graph()\n",
    "    #     np.random.seed(100)\n",
    "    #     tf.set_random_seed(100)\n",
    "    #         ops['n_attractor_iterations'] = attractor_steps\n",
    "    #         N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "        #\n",
    "        # PLACEHOLDERS\n",
    "        #\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            # last dimension is left singular, tensorflow will expect it to be an id number, not 1-hot embed\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN], name='Y')\n",
    "        elif ops['problem_type'] == 'ner_german':\n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN])\n",
    "        else:  # single output\n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"float\", [None, N_CLASSES])\n",
    "            \n",
    "        if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "            X_longer = tf.placeholder(\"float\", [None, SEQ_LEN*3, N_INPUT])\n",
    "        attractor_tgt_net = tf.placeholder(\"float\", [None, N_HIDDEN], name='attractor_tgt')\n",
    "\n",
    "\n",
    "\n",
    "        # Graph + all the training variables\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            net_inputs = {'X': embed, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        else:\n",
    "            net_inputs = {'X': X, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "            \n",
    "        if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "            net_inputs_longer = {'X': X_longer, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "            \n",
    "        if ops['bidirectional']:\n",
    "            G_attractors = {'forw': [], 'back': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            # Backward:\n",
    "            G_back = GRU_attractor(ops, inputs=net_inputs, direction='backward', suffix=names[1])\n",
    "            attr_loss_op_back = G_back.attr_loss_op\n",
    "            attr_train_op_back = G_back.attr_train_op\n",
    "            h_clean_seq_flat_back = G_back.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_back = G_back.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['back'] = {'attr_loss_op': attr_loss_op_back, \"attr_train_op\": attr_train_op_back,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_back, 'h_net_seq_flat': h_net_seq_flat_back}\n",
    "            G_back_output = G_back.output\n",
    "\n",
    "            # Merge: [seq_len, batch_size, n_hid*2]\n",
    "            # Note that we reverse the backward cell's output to align with original direction\n",
    "            output = tf.concat([G_forw_output, tf.reverse(G_back_output, axis=[0])], axis=2)\n",
    "\n",
    "            if ops['dropout'] > 0.0:\n",
    "                # note keep_prob = 1.0 - drop_probability (not sure why they implemented it this way)\n",
    "                # tensorflow implementation scales by 1/keep_prob automatically\n",
    "                output_dropped = tf.nn.dropout(output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output_dropped = output\n",
    "\n",
    "            input_size_final_projection = 2 * ops['hid']\n",
    "            Y_ = project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "\n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "        else:\n",
    "            # TODO: change to map with just ont entry as well.\n",
    "            G_attractors = {'forw': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            if ops['model_type'] == 'GRU':\n",
    "                G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            elif ops['model_type'] == 'TANH':\n",
    "                G_forw = TANH_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "                if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "                    G_forw_longer = TANH_attractor(ops, inputs=net_inputs_longer, direction='forward', suffix=names[0], reuse=True)\n",
    "                \n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            output = G_forw.output\n",
    "            if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "                output_longer = G_forw_longer.output\n",
    "\n",
    "            h_net_seq_flat = G_forw.h_net_seq_flat # pure cell ouptut (before attractor was applied)\n",
    "            h_attractor_collection_flat = G_forw.h_attractor_collection\n",
    "            h_clean_seq_flat = G_forw.h_clean_seq_flat\n",
    "            input_size_final_projection = ops['hid']\n",
    "            Y_ = project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "            if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "                Y__longer = project_into_output(Y, output_longer, input_size_final_projection, ops['out'], ops, reuse=True)\n",
    "\n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            \n",
    "            if ops['train_attr_weights_on_pred']:\n",
    "                prediction_parameters += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"ATTRACTOR_WEIGHTS\")\n",
    "                print_into_log(LOG_DIRECTORY, \"adding attractor params to task training op\")\n",
    "                print(prediction_parameters)\n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "            if ops['problem_type'] == 'parity_length_noisy_longer_remainder':\n",
    "                accuracy_longer = task_accuracy(Y, Y__longer, ops)\n",
    "\n",
    "        mask_op = tf.cast(tf.sign(Y), dtype=tf.float32)\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # TODO: make a class for all \"best\" quantities (a lot of space)\n",
    "            saved_train_acc = []\n",
    "            saved_test_acc = []\n",
    "            saved_epoch = []\n",
    "            saved_att_loss = []\n",
    "            saved_entropy_final = []\n",
    "            saved_entropy_final_test = []\n",
    "            saved_val_acc = []\n",
    "            saved_val_loss = []\n",
    "            saved_traini_loss = []\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # Start training\n",
    "            for replication in range(N_REPLICATIONS):\n",
    "                print(\"********** replication \", replication, \" **********\")\n",
    "                early_stopper = EarlyStopper(EARLY_STOPPING_PATIENCE, DISPLAY_EPOCH)\n",
    "                [X_train, Y_train, X_test, Y_test, X_val, Y_val, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                                                           INPUT_NOISE_LEVEL, TASK, ops)\n",
    "\n",
    "                if len(X_test) ==3 :\n",
    "                    N_TRAIN_print = len(X_train)\n",
    "                    N_TEST_print = ', '.join([str(len(arr)) for arr in X_test])\n",
    "                else:\n",
    "                    N_TRAIN_print = N_TRAIN\n",
    "                    N_TEST_print = N_TEST\n",
    "                # Log Path init-n:\n",
    "                COMMENT = 'training_procedure_test'\n",
    "                MODEL_NAME_FILE = '{}_{}.txt'.format(ops['problem_type'], COMMENT)\n",
    "                LOG_DIRECTORY = 'experiments/logs/{}'.format(MODEL_NAME_FILE)\n",
    "                MODEL_DIRECTORY = 'experiments/logs/{}_{}'.format(datetime.date.today(), MODEL_NAME_FILE)\n",
    "                print_into_log(LOG_DIRECTORY, get_model_type_str(ops, N_TRAIN_print, N_TEST_print, SEQ_LEN) + ops['training_mode'])\n",
    "                print_into_log(MODEL_DIRECTORY, get_model_type_str(ops, N_TRAIN_print, N_TEST_print, SEQ_LEN), supress=True)\n",
    "                sess.run(init)  # Run the initializer\n",
    "\n",
    "                train_prediction_loss = True\n",
    "                best_train_acc = -1000.\n",
    "                best_test_acc = 0\n",
    "                best_entropy = 0.0\n",
    "                best_entropy_test = 0.0\n",
    "                best_att_loss = 0\n",
    "                best_train_loss = 0\n",
    "                best_val_loss = 0.0\n",
    "                best_val_acc = 0.0\n",
    "                best_epoch = 0\n",
    "                for epoch in range(1, TRAINING_EPOCHS + 2):\n",
    "                    if (epoch - 1) % DISPLAY_EPOCH == 0:\n",
    "                        # TRAIN set:\n",
    "                        ploss, train_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        # TEST set:\n",
    "                        if len(X_test) ==3:\n",
    "                            test_acc = \"leftover:{:.3f}, noisy_train:{:.3f}, longer:{:.3f}\".format(batch_tensor_collect(sess, [accuracy], X, Y, X_test[0], Y_test[0], BATCH_SIZE)[0],\n",
    "                                                                                      batch_tensor_collect(sess, [accuracy], X, Y, X_test[1], Y_test[1], BATCH_SIZE)[0],\n",
    "                                                                                      batch_tensor_collect(sess, [accuracy_longer], X_longer, Y, X_test[2], Y_test[2], BATCH_SIZE)[0],)\n",
    "                        else:\n",
    "                            test_acc = batch_tensor_collect(sess, [accuracy], X, Y, X_test, Y_test, BATCH_SIZE)[0]\n",
    "\n",
    "    #                         # Validation set & Early stopping:\n",
    "    #                         ploss_val, val_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "    #                                                                   X, Y, X_val, Y_val, BATCH_SIZE)\n",
    "\n",
    "                       \n",
    "\n",
    "                        # ATTRACTOR(s) LOSS\n",
    "                        aloss = {}\n",
    "                        entropy = {}\n",
    "                        entropy_test = {}\n",
    "                        hid_vals_arr = batch_tensor_collect(sess, [A['h_net_seq_flat'] for att_name, A in\n",
    "                                                                   G_attractors.items()],\n",
    "                                                            X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        h_clean_val_arr = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        if len(X_test) ==3:\n",
    "                            h_clean_val_arr_test = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_test[0], Y_test[0], BATCH_SIZE)\n",
    "                        else:\n",
    "                            h_clean_val_arr_test = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_test, Y_test, BATCH_SIZE)\n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            A = G_attractors[attractor_name]\n",
    "                            a_loss_val = []\n",
    "                            n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "                            for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "                                a_loss_val.append(\n",
    "                                    sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "                            aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "\n",
    "                            entropy[attractor_name] = \"{:.4f}\".format(\n",
    "                                compute_entropy_fullvec(h_clean_val_arr[i], ops, n_bins=8))\n",
    "                            \n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            entropy_test[attractor_name] = \"{:.4f}\".format(\n",
    "                                compute_entropy_fullvec(h_clean_val_arr_test[i], ops, n_bins=8))    \n",
    "\n",
    "                        # Print training information:\n",
    "                        print_into_log(LOG_DIRECTORY, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + get_training_progress_comment(epoch, ploss, aloss, 0.0, 0.0, train_acc,\n",
    "                                                                     test_acc, entropy, entropy_test))\n",
    "                        # Update the logs:\n",
    "                        WS.update_conservative(epoch_number=epoch, loss_att=aloss,\n",
    "                                               loss_task=ploss, acc=test_acc, entropy=entropy)\n",
    "                        if ops['record_mutual_information']:\n",
    "                            h_attractor_val, h_clean_val = batch_tensor_collect(sess, [h_attractor_collection_flat, h_clean_seq_flat],\n",
    "                                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                            MIS.update(ploss, aloss, train_acc, test_acc, np.tanh(hid_vals_arr[0]), h_attractor_val, h_clean_val)\n",
    "\n",
    "                        if (train_acc > best_train_acc):\n",
    "                            best_train_acc = train_acc\n",
    "                            best_test_acc = test_acc\n",
    "                            best_att_loss = aloss\n",
    "                            best_epoch = epoch\n",
    "    #                         best_val_acc = val_acc\n",
    "\n",
    "    #                         best_val_loss = ploss_val\n",
    "    #                         best_train_loss = ploss\n",
    "                            if ops['save_best_model']:\n",
    "                                save_path = saver.save(sess, MODEL_DIRECTORY)\n",
    "                            best_entropy = entropy\n",
    "                            best_entropy_test = entropy_test\n",
    "                        if (train_acc == 1.0):\n",
    "                            print(\"Reached Peak!\")\n",
    "                            break\n",
    "                    if epoch > 1 and LOSS_SWITCH_FREQ > 0 \\\n",
    "                            and (epoch - 1) % LOSS_SWITCH_FREQ == 0:\n",
    "                        train_prediction_loss = not train_prediction_loss\n",
    "\n",
    "                    # MODEL TRAINING\n",
    "                    batches = get_batches(BATCH_SIZE, X_train, Y_train)\n",
    "                    for (batch_x, batch_y) in batches:\n",
    "                        if (LOSS_SWITCH_FREQ == 0 or train_prediction_loss):\n",
    "                            # Optimize all parameters except for attractor weights\n",
    "                            _ = sess.run([pred_train_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y})\n",
    "                        if ops['training_mode'] != 'attractor_on_task':\n",
    "                            if (LOSS_SWITCH_FREQ == 0 or not train_prediction_loss):\n",
    "                                if (N_ATTRACTOR_STEPS > 0):\n",
    "                                    # ATTRACTOR(s) training\n",
    "                                    for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                        A = G_attractors[attractor_name]\n",
    "                                        _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: hid_vals_arr[i]})\n",
    "\n",
    "                print(\"Optimization Finished!\")\n",
    "\n",
    "                if (REPORT_BEST_TRAIN_PERFORMANCE):\n",
    "                    saved_train_acc.append(best_train_acc)\n",
    "                    saved_test_acc.append(best_test_acc)\n",
    "                    saved_att_loss.append(best_att_loss)\n",
    "                    saved_entropy_final.append(best_entropy)\n",
    "                    saved_entropy_final_test.append(best_entropy_test)\n",
    "                    saved_epoch.append(best_epoch)\n",
    "\n",
    "    #                 saved_val_acc.append(best_val_acc)\n",
    "    #                 saved_val_loss.append(best_val_loss)\n",
    "                    saved_traini_loss.append(best_train_loss)\n",
    "                else:\n",
    "                    saved_train_acc.append(train_acc)\n",
    "                    saved_test_acc.append(test_acc)\n",
    "                    #             saved_att_loss.append(aloss)\n",
    "\n",
    "            if ops['training_mode'] == 'attractor_on_task':\n",
    "                COMMENT += \"<attractor on task loss>\"\n",
    "            save_results(ops, saved_epoch, saved_train_acc, saved_test_acc, saved_att_loss, saved_entropy_final, saved_val_acc,\n",
    "                 saved_val_loss, saved_traini_loss, N_TRAIN_print, N_TEST_print, SEQ_LEN, COMMENT, saved_entropy_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
