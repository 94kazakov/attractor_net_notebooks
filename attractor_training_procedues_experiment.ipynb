{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "parity no_att\n",
      "L2 norm\n",
      "********** replication  0  **********\n",
      "parity\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tGRU bidir(False), task: parity\n",
      "    hid: \t\t\t5,\n",
      "    h_hid: \t\t\t10\n",
      "    n_attractor_iterations: \t0,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.2\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_norm(lambda:0.05)\n",
      "    word_embedding: size\t(100), train(True)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t32/32, SEQ_LEN: 5\n",
      "Logged Successfully: \n",
      "Logged Successfully: \n",
      "2018-04-16 15:37:38epoch=0; Loss Pred=1.0781; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0054'}; Train Acc=0.500; Test Acc=0.4531; Entropy={'forw': '2.0409'}; Entropy_Test={'forw': '2.6641'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:37:40epoch=200; Loss Pred=0.9291; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0054'}; Train Acc=0.469; Test Acc=0.4531; Entropy={'forw': '2.9966'}; Entropy_Test={'forw': '3.6713'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:37:40epoch=400; Loss Pred=0.1036; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0054'}; Train Acc=1.000; Test Acc=0.9844; Entropy={'forw': '3.3360'}; Entropy_Test={'forw': '4.0658'}\n",
      "\n",
      "Reached Peak!\n",
      "Optimization Finished!\n",
      "Saved Results Successfully\n",
      "parity att\n",
      "L2 norm\n",
      "********** replication  0  **********\n",
      "parity\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tGRU bidir(False), task: parity\n",
      "    hid: \t\t\t5,\n",
      "    h_hid: \t\t\t10\n",
      "    n_attractor_iterations: \t10,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.2\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_norm(lambda:0.05)\n",
      "    word_embedding: size\t(100), train(True)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t32/32, SEQ_LEN: 5\n",
      "Logged Successfully: \n",
      "Logged Successfully: \n",
      "2018-04-16 15:37:45epoch=0; Loss Pred=1.0450; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.4419'}; Train Acc=0.500; Test Acc=0.5000; Entropy={'forw': '2.1640'}; Entropy_Test={'forw': '2.5478'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:37:48epoch=200; Loss Pred=0.9989; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.5745'}; Train Acc=0.469; Test Acc=0.4531; Entropy={'forw': '1.5531'}; Entropy_Test={'forw': '1.8324'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:37:49epoch=400; Loss Pred=0.9647; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.5003'}; Train Acc=0.500; Test Acc=0.5156; Entropy={'forw': '1.8287'}; Entropy_Test={'forw': '2.1845'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:37:50epoch=600; Loss Pred=0.4535; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '4.8248'}; Train Acc=0.875; Test Acc=0.8750; Entropy={'forw': '3.2154'}; Entropy_Test={'forw': '3.7532'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:37:53epoch=800; Loss Pred=0.0232; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.7911'}; Train Acc=1.000; Test Acc=1.0000; Entropy={'forw': '3.1465'}; Entropy_Test={'forw': '3.6840'}\n",
      "\n",
      "Reached Peak!\n",
      "Optimization Finished!\n",
      "Saved Results Successfully\n",
      "parity att_on_task\n",
      "L2 norm\n",
      "adding attractor params to task training op\n",
      "********** replication  0  **********\n",
      "parity\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tGRU bidir(False), task: parity\n",
      "    hid: \t\t\t5,\n",
      "    h_hid: \t\t\t10\n",
      "    n_attractor_iterations: \t10,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.2\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_norm(lambda:0.05)\n",
      "    word_embedding: size\t(100), train(True)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t32/32, SEQ_LEN: 5attractor_on_task\n",
      "Logged Successfully: \n",
      "Logged Successfully: \n",
      "2018-04-16 15:37:59epoch=0; Loss Pred=1.0781; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.1195'}; Train Acc=0.500; Test Acc=0.5156; Entropy={'forw': '2.6728'}; Entropy_Test={'forw': '3.1623'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:02epoch=200; Loss Pred=0.4991; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '16.9245'}; Train Acc=0.781; Test Acc=0.7344; Entropy={'forw': '3.3279'}; Entropy_Test={'forw': '4.0879'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:05epoch=400; Loss Pred=0.2437; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '14.4947'}; Train Acc=0.938; Test Acc=0.8906; Entropy={'forw': '3.5226'}; Entropy_Test={'forw': '4.5315'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:07epoch=600; Loss Pred=1.3223; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '51.3598'}; Train Acc=0.594; Test Acc=0.5469; Entropy={'forw': '3.0855'}; Entropy_Test={'forw': '3.5979'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:08epoch=800; Loss Pred=0.9790; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '50.5092'}; Train Acc=0.531; Test Acc=0.5000; Entropy={'forw': '2.8356'}; Entropy_Test={'forw': '3.3336'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:09epoch=1000; Loss Pred=0.8453; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '53.5924'}; Train Acc=0.719; Test Acc=0.6250; Entropy={'forw': '3.0841'}; Entropy_Test={'forw': '3.5375'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:10epoch=1200; Loss Pred=0.5064; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '60.9071'}; Train Acc=0.812; Test Acc=0.7500; Entropy={'forw': '2.9592'}; Entropy_Test={'forw': '3.0899'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:11epoch=1400; Loss Pred=0.4559; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '74.0096'}; Train Acc=0.812; Test Acc=0.7500; Entropy={'forw': '2.6860'}; Entropy_Test={'forw': '2.9993'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:11epoch=1600; Loss Pred=0.4295; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '64.1171'}; Train Acc=0.812; Test Acc=0.7812; Entropy={'forw': '2.7782'}; Entropy_Test={'forw': '3.0404'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:12epoch=1800; Loss Pred=0.4132; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '72.1988'}; Train Acc=0.812; Test Acc=0.7656; Entropy={'forw': '2.8389'}; Entropy_Test={'forw': '3.1230'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:13epoch=2000; Loss Pred=0.4094; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '67.7378'}; Train Acc=0.812; Test Acc=0.7656; Entropy={'forw': '2.8637'}; Entropy_Test={'forw': '3.1392'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:13epoch=2200; Loss Pred=0.4199; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '59.0484'}; Train Acc=0.844; Test Acc=0.7656; Entropy={'forw': '2.9096'}; Entropy_Test={'forw': '3.1822'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:14epoch=2400; Loss Pred=0.4737; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '73.7572'}; Train Acc=0.781; Test Acc=0.7656; Entropy={'forw': '2.9480'}; Entropy_Test={'forw': '3.3834'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:15epoch=2600; Loss Pred=0.4450; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '68.0879'}; Train Acc=0.781; Test Acc=0.7500; Entropy={'forw': '2.8605'}; Entropy_Test={'forw': '3.3584'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:16epoch=2800; Loss Pred=0.4421; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '63.4540'}; Train Acc=0.781; Test Acc=0.7500; Entropy={'forw': '2.8048'}; Entropy_Test={'forw': '3.3201'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:17epoch=3000; Loss Pred=0.4408; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '60.5417'}; Train Acc=0.781; Test Acc=0.7656; Entropy={'forw': '2.8340'}; Entropy_Test={'forw': '3.3035'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:18epoch=3200; Loss Pred=0.4400; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '70.5209'}; Train Acc=0.781; Test Acc=0.7656; Entropy={'forw': '2.8204'}; Entropy_Test={'forw': '3.3259'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:18epoch=3400; Loss Pred=0.4395; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '69.6144'}; Train Acc=0.781; Test Acc=0.7656; Entropy={'forw': '2.8870'}; Entropy_Test={'forw': '3.2992'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:19epoch=3600; Loss Pred=0.4391; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '65.5668'}; Train Acc=0.781; Test Acc=0.7500; Entropy={'forw': '2.8102'}; Entropy_Test={'forw': '3.3340'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:20epoch=3800; Loss Pred=0.4388; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '74.3459'}; Train Acc=0.781; Test Acc=0.7500; Entropy={'forw': '2.8659'}; Entropy_Test={'forw': '3.3477'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:21epoch=4000; Loss Pred=0.4385; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '56.6721'}; Train Acc=0.781; Test Acc=0.7500; Entropy={'forw': '2.8414'}; Entropy_Test={'forw': '3.3332'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:21epoch=4200; Loss Pred=0.4382; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '73.7236'}; Train Acc=0.812; Test Acc=0.7656; Entropy={'forw': '2.8907'}; Entropy_Test={'forw': '3.3862'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:22epoch=4400; Loss Pred=0.4373; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '69.4578'}; Train Acc=0.812; Test Acc=0.7812; Entropy={'forw': '2.8684'}; Entropy_Test={'forw': '3.4261'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:23epoch=4600; Loss Pred=0.4271; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '73.0735'}; Train Acc=0.812; Test Acc=0.7188; Entropy={'forw': '3.0752'}; Entropy_Test={'forw': '3.5095'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:24epoch=4800; Loss Pred=0.3846; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '75.9893'}; Train Acc=0.812; Test Acc=0.6562; Entropy={'forw': '2.9444'}; Entropy_Test={'forw': '3.6476'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:24epoch=5000; Loss Pred=0.3797; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '66.5228'}; Train Acc=0.812; Test Acc=0.6562; Entropy={'forw': '3.0113'}; Entropy_Test={'forw': '3.6685'}\n",
      "\n",
      "Optimization Finished!\n",
      "Saved Results Successfully\n",
      "parity_length no_att\n",
      "L2 norm\n",
      "********** replication  0  **********\n",
      "parity_length\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tGRU bidir(False), task: parity_length\n",
      "    hid: \t\t\t5,\n",
      "    h_hid: \t\t\t10\n",
      "    n_attractor_iterations: \t0,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.2\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_norm(lambda:0.05)\n",
      "    word_embedding: size\t(100), train(True)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t1000/1000, SEQ_LEN: 12\n",
      "Logged Successfully: \n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:26epoch=0; Loss Pred=1.0743; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.517; Test Acc=0.5100; Entropy={'forw': '2.2311'}; Entropy_Test={'forw': '2.8804'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:30epoch=200; Loss Pred=0.9950; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.533; Test Acc=0.4990; Entropy={'forw': '2.5759'}; Entropy_Test={'forw': '3.1743'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:32epoch=400; Loss Pred=0.9885; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.541; Test Acc=0.5195; Entropy={'forw': '3.5375'}; Entropy_Test={'forw': '4.0537'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:35epoch=600; Loss Pred=0.9646; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.575; Test Acc=0.5205; Entropy={'forw': '4.5550'}; Entropy_Test={'forw': '4.9419'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:37epoch=800; Loss Pred=0.9281; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.604; Test Acc=0.5370; Entropy={'forw': '5.0247'}; Entropy_Test={'forw': '5.5210'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:39epoch=1000; Loss Pred=0.8873; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.622; Test Acc=0.5335; Entropy={'forw': '5.2658'}; Entropy_Test={'forw': '5.8096'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:42epoch=1200; Loss Pred=0.8631; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.652; Test Acc=0.5300; Entropy={'forw': '5.3279'}; Entropy_Test={'forw': '5.8918'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:45epoch=1400; Loss Pred=0.8510; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.654; Test Acc=0.5240; Entropy={'forw': '5.4308'}; Entropy_Test={'forw': '5.9935'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:47epoch=1600; Loss Pred=0.8396; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.662; Test Acc=0.5235; Entropy={'forw': '5.4291'}; Entropy_Test={'forw': '6.0046'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:49epoch=1800; Loss Pred=0.8315; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.672; Test Acc=0.5165; Entropy={'forw': '5.4420'}; Entropy_Test={'forw': '6.0196'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:52epoch=2000; Loss Pred=0.8277; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.673; Test Acc=0.5120; Entropy={'forw': '5.4484'}; Entropy_Test={'forw': '6.0216'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:54epoch=2200; Loss Pred=0.8227; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.673; Test Acc=0.5115; Entropy={'forw': '5.4623'}; Entropy_Test={'forw': '6.0337'}\n",
      "\n",
      "Logged Successfully: \n",
      "2018-04-16 15:38:55epoch=2400; Loss Pred=0.8178; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0053'}; Train Acc=0.677; Test Acc=0.5105; Entropy={'forw': '5.4636'}; Entropy_Test={'forw': '6.0377'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python\n",
    "\n",
    "# This version of the code trains the attractor connections with a separate\n",
    "# objective function than the objective function used to train all other weights\n",
    "# in the network (on the prediction task).\n",
    "\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload\n",
    "\n",
    "from tensorflow_helpers import *\n",
    "from data_generator import generate_examples, pick_task\n",
    "from information_trackers import MutInfSaver, WeightSaver, compute_entropy_fullvec, get_mut_inf_for_fullvec, \\\n",
    "    flat_mutual_inf, compute_avg_entropy_vec\n",
    "from helper_functions import get_batches, load_pretrained_embeddings, \\\n",
    "    get_model_type_str, translate_ids_to_words, \\\n",
    "    save_results, print_into_log, print_some_translated_sentences, \\\n",
    "    get_training_progress_comment\n",
    "from graph_init import GRU_attractor, TANH_attractor\n",
    "\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, patience_max, disp_epoch, min_delta = 0.03):\n",
    "        self.best = 1e10\n",
    "        self.patience = 0  # our patience\n",
    "        self.patience_max = patience_max\n",
    "        self.display_epoch = disp_epoch\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def update(self, current):\n",
    "        if self.best > current:\n",
    "            self.best = current\n",
    "            self.patience = 0\n",
    "        elif abs(self.best - current) > self.min_delta:\n",
    "            self.patience += 1\n",
    "\n",
    "    def patience_ran_out(self):\n",
    "        if self.patience*self.display_epoch > self.patience_max:\n",
    "            return True\n",
    "        else:\n",
    "            False\n",
    "            \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "ops = {\n",
    "    'model_type': \"GRU\",  # OPTIONS: GRU, TANH\n",
    "    'hid': 5,\n",
    "    'in': None,  # TBD\n",
    "    'out': 1,\n",
    "    #         'batch_size':n_examples, #since the sequences are 1-dimensional it's easier to just run them all at once\n",
    "    'n_attractor_iterations': 0,\n",
    "    'attractor_dynamics': \"projection2\",  # OPTIONS:  \"\" (for no attractor dynamics),\n",
    "    #           \"direct\" (simple attractor weights applied to hidden states directly, trained with noise addition)\n",
    "    #           \"projection\" (project the hidden state into a separate space via weights, do attraction, project back)\n",
    "    #           \"helper_hidden\" (hidden-hidden neurons) - IMPORTANT: don't forget to add h_hid number\n",
    "    'h_hid': 10,  # helper hidden for \"helper hidden\" \"attractory_dynamics\" mode\n",
    "    'attractor_noise_level': 0.2,\n",
    "    'attractor_noise_type': \"bernoilli\",  # OPTIONS: \"gaussian\", \"dropout\", \"random_drop\"\n",
    "\n",
    "    'training_mode': \"\",  # 'attractor_on_task',\n",
    "\n",
    "    'attractor_regularization': \"l2_norm\",  # OPTIONS: \"l2_regularization\", \"l2_norm\"\n",
    "    'attractor_regularization_lambda': 0.05,\n",
    "\n",
    "    'record_mutual_information': True,\n",
    "    'problem_type': \"majority\",  # OPTIONS: parity, parity_length, majority, reber, kazakov, pos_brown, ner_german, sentimend_imdb\n",
    "    'masking': False,#\"seq\", \"final\"\n",
    "    'prediction_type': 'final', #'seq', 'final'\n",
    "    'seq_len': 5,\n",
    "\n",
    "    'save_best_model': True,\n",
    "    'reshuffle_data_each_replication': False,  # relevant for POS datasets (since they are loaded from files)\n",
    "    'test_partition': 0.3,\n",
    "    'lrate': 0.003,  # was 0.008\n",
    "\n",
    "    # NLP related (pos_brown task)\n",
    "    'bidirectional': False,\n",
    "    'embedding_size': 100,\n",
    "    'load_word_embeddings': True,\n",
    "    'train_word_embeddings': True,\n",
    "    'input_type': \"embed\",  # embed&prior, embed, prior\n",
    "    'dropout': 0.0  # in range(0,1)\n",
    "}\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# SEQ_LEN = 12 # number of bits in input sequence\n",
    "N_HIDDEN = ops['hid']  # number of hidden units\n",
    "N_H_HIDDEN = ops['h_hid']\n",
    "NOISE_LEVEL = ops['attractor_noise_level']\n",
    "# noise in training attractor net\n",
    "# if >=0, Gaussian with std dev NOISE_LEVEL\n",
    "# if < 0, Bernoulli dropout proportion -NOISE_LEVEL\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "INPUT_NOISE_LEVEL = 0.1\n",
    "# number of time steps in attractor dynamics\n",
    "# if = 0, then no attractor net\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# ATTR_WEIGHT_CONSTRAINTS = True\n",
    "# True: make attractor weights symmetric and have zero diag\n",
    "# False: unconstrained\n",
    "TRAIN_ATTR_WEIGHTS_ON_PREDICTION = False\n",
    "# True: train attractor weights on attractor net _and_ prediction\n",
    "REPORT_BEST_TRAIN_PERFORMANCE = True\n",
    "# True: save the train/test perf on the epoch for which train perf was best\n",
    "LOSS_SWITCH_FREQ = 1\n",
    "# how often (in epochs) to switch between attractor\n",
    "# and prediction loss\n",
    "\n",
    "# Training Parameters\n",
    "\n",
    "TRAINING_EPOCHS = 5000\n",
    "N_REPLICATIONS = 1\n",
    "BATCH_SIZE = 5000\n",
    "DISPLAY_EPOCH = 200\n",
    "EARLY_STOPPING_THRESH = 1e-3\n",
    "EARLY_STOPPING_PATIENCE = 1000  # in epochs\n",
    "\n",
    "# NOTEBOOK CODE\n",
    "WS = WeightSaver()\n",
    "MIS = MutInfSaver()\n",
    "\n",
    "######### MAIN CODE #############################################################\n",
    "#0.02, 0.05, 0.1, 0.2, 0.35, 0.5, \n",
    "\n",
    "for problem in ['parity', 'parity_length', 'majority', 'reber']:\n",
    "    ops['problem_type'] = problem\n",
    "    TASK = ops['problem_type']\n",
    "    for training_procedure in ['no_att', 'att', 'att_on_task']:\n",
    "        print(TASK, training_procedure)\n",
    "        if training_procedure == 'no_att':\n",
    "            ops['n_attractor_iterations'] = 0\n",
    "            ops['training_mode'] = ''\n",
    "        elif training_procedure == 'att':\n",
    "            ops['n_attractor_iterations'] = 10\n",
    "            ops['training_mode'] = ''\n",
    "        elif training_procedure == 'att_on_task':\n",
    "            ops['n_attractor_iterations'] = 10\n",
    "            ops['training_mode'] = 'attractor_on_task'\n",
    "        ops, SEQ_LEN, N_INPUT, N_CLASSES, N_TRAIN, N_TEST = pick_task(ops['problem_type'],\n",
    "                                                              ops)  # task (parity, majority, reber, kazakov)\n",
    "        N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "        ARCH = ops['model_type']  # hidden layer type: 'GRU' or 'tanh'\n",
    "        ATTRACTOR_TYPE = ops['attractor_dynamics']\n",
    "        # the tf seed needs to be within the context of the graph.\n",
    "        tf.reset_default_graph()\n",
    "    #     np.random.seed(100)\n",
    "    #     tf.set_random_seed(100)\n",
    "    #         ops['n_attractor_iterations'] = attractor_steps\n",
    "    #         N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "        #\n",
    "        # PLACEHOLDERS\n",
    "        #\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            # last dimension is left singular, tensorflow will expect it to be an id number, not 1-hot embed\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN], name='Y')\n",
    "        elif ops['problem_type'] == 'ner_german':\n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN])\n",
    "        else:  # single output\n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"float\", [None, N_CLASSES])\n",
    "        attractor_tgt_net = tf.placeholder(\"float\", [None, N_HIDDEN], name='attractor_tgt')\n",
    "\n",
    "        # Embedding matrix initialization\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            [_, _, _, _, _, _, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                         INPUT_NOISE_LEVEL, TASK, ops)\n",
    "\n",
    "            if ops['load_word_embeddings']:\n",
    "                embeddings_loaded = load_pretrained_embeddings('data/glove.6B.{}d.txt'.format(ops['embedding_size']),\n",
    "                                                               maps, ops)\n",
    "                embedding = tf.get_variable(\"embedding\",\n",
    "                                            initializer=embeddings_loaded,\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=ops['train_word_embeddings'])\n",
    "            else:  # initialize randomly\n",
    "                embedding = tf.get_variable(\"embedding\",\n",
    "                                            initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                            shape=[ops['vocab_size'], ops['embedding_size']],\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=ops['train_word_embeddings'])\n",
    "            embed_lookup = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "            # load priors information\n",
    "            if ops['input_type'] == 'prior' or ops['input_type'] == 'embed&prior':\n",
    "                id2prior = maps['id2prior']\n",
    "                word2id = maps['word2id']\n",
    "                priors = np.zeros([len(id2prior), len(id2prior[0])]).astype(\"float32\")\n",
    "                for id, prior in id2prior.items():\n",
    "                    priors[id] = prior\n",
    "                priors_op = tf.get_variable(\"priors\",\n",
    "                                            initializer=priors,\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=False)\n",
    "                prior_lookup = tf.nn.embedding_lookup(priors_op, X)\n",
    "\n",
    "            if ops['input_type'] == 'embed':\n",
    "                embed = embed_lookup\n",
    "            elif ops['input_type'] == 'prior':\n",
    "                embed = prior_lookup\n",
    "            elif ops['input_type'] == 'embed&prior':\n",
    "                embed = tf.concat([embed_lookup, prior_lookup], axis=2)\n",
    "\n",
    "        # Graph + all the training variables\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            net_inputs = {'X': embed, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        else:\n",
    "            net_inputs = {'X': X, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "\n",
    "        if ops['bidirectional']:\n",
    "            G_attractors = {'forw': [], 'back': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            # Backward:\n",
    "            G_back = GRU_attractor(ops, inputs=net_inputs, direction='backward', suffix=names[1])\n",
    "            attr_loss_op_back = G_back.attr_loss_op\n",
    "            attr_train_op_back = G_back.attr_train_op\n",
    "            h_clean_seq_flat_back = G_back.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_back = G_back.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['back'] = {'attr_loss_op': attr_loss_op_back, \"attr_train_op\": attr_train_op_back,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_back, 'h_net_seq_flat': h_net_seq_flat_back}\n",
    "            G_back_output = G_back.output\n",
    "\n",
    "            # Merge: [seq_len, batch_size, n_hid*2]\n",
    "            # Note that we reverse the backward cell's output to align with original direction\n",
    "            output = tf.concat([G_forw_output, tf.reverse(G_back_output, axis=[0])], axis=2)\n",
    "\n",
    "            if ops['dropout'] > 0.0:\n",
    "                # note keep_prob = 1.0 - drop_probability (not sure why they implemented it this way)\n",
    "                # tensorflow implementation scales by 1/keep_prob automatically\n",
    "                output_dropped = tf.nn.dropout(output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output_dropped = output\n",
    "\n",
    "            input_size_final_projection = 2 * ops['hid']\n",
    "            Y_ = project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "\n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "        else:\n",
    "            # TODO: change to map with just ont entry as well.\n",
    "            G_attractors = {'forw': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            if ops['model_type'] == 'GRU':\n",
    "                G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            elif ops['model_type'] == 'TANH':\n",
    "                G_forw = TANH_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            output = G_forw.output\n",
    "\n",
    "            h_net_seq_flat = G_forw.h_net_seq_flat # pure cell ouptut (before attractor was applied)\n",
    "            h_attractor_collection_flat = G_forw.h_attractor_collection\n",
    "            h_clean_seq_flat = G_forw.h_clean_seq_flat\n",
    "            input_size_final_projection = ops['hid']\n",
    "            Y_ = project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "\n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            if ops['training_mode'] == 'attractor_on_task':\n",
    "                print(\"adding attractor params to task training op\")\n",
    "                prediction_parameters += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"ATTRACTOR_WEIGHTS\")\n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "\n",
    "\n",
    "        mask_op = tf.cast(tf.sign(Y), dtype=tf.float32)\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # TODO: make a class for all \"best\" quantities (a lot of space)\n",
    "            saved_train_acc = []\n",
    "            saved_test_acc = []\n",
    "            saved_epoch = []\n",
    "            saved_att_loss = []\n",
    "            saved_entropy_final = []\n",
    "            saved_entropy_final_test = []\n",
    "            saved_val_acc = []\n",
    "            saved_val_loss = []\n",
    "            saved_traini_loss = []\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # Start training\n",
    "            for replication in range(N_REPLICATIONS):\n",
    "                print(\"********** replication \", replication, \" **********\")\n",
    "                early_stopper = EarlyStopper(EARLY_STOPPING_PATIENCE, DISPLAY_EPOCH)\n",
    "                [X_train, Y_train, X_test, Y_test, X_val, Y_val, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                                                           INPUT_NOISE_LEVEL, TASK, ops)\n",
    "\n",
    "\n",
    "                # Log Path init-n:\n",
    "                COMMENT = 'training_procedure_test'\n",
    "                MODEL_NAME_FILE = '{}_{}.txt'.format(ops['problem_type'],\n",
    "                                                                                   COMMENT)\n",
    "                LOG_DIRECTORY = 'experiments/logs/{}'.format(MODEL_NAME_FILE)\n",
    "                MODEL_DIRECTORY = 'experiments/logs/{}_{}'.format(datetime.date.today(), MODEL_NAME_FILE)\n",
    "                print_into_log(LOG_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN) + ops['training_mode'])\n",
    "                print_into_log(MODEL_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN), supress=True)\n",
    "\n",
    "                sess.run(init)  # Run the initializer\n",
    "\n",
    "                train_prediction_loss = True\n",
    "                best_train_acc = -1000.\n",
    "                best_test_acc = 0\n",
    "                best_entropy = 0.0\n",
    "                best_entropy_test = 0.0\n",
    "                best_att_loss = 0\n",
    "                best_train_loss = 0\n",
    "                best_val_loss = 0.0\n",
    "                best_val_acc = 0.0\n",
    "                best_epoch = 0\n",
    "                for epoch in range(1, TRAINING_EPOCHS + 2):\n",
    "                    if (epoch - 1) % DISPLAY_EPOCH == 0:\n",
    "                        # TRAIN set:\n",
    "                        ploss, train_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        # TEST set:\n",
    "                        test_acc = batch_tensor_collect(sess, [accuracy], X, Y, X_test, Y_test, BATCH_SIZE)[0]\n",
    "\n",
    "    #                         # Validation set & Early stopping:\n",
    "    #                         ploss_val, val_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "    #                                                                   X, Y, X_val, Y_val, BATCH_SIZE)\n",
    "\n",
    "                        # Precistion/Recall:\n",
    "                        def _get_metrics(X_data, Y_data):\n",
    "                            y_pred, y_true, mask_val = batch_tensor_collect(sess, [Y_, Y, mask_op],\n",
    "                                                                X, Y, X_data, Y_data, BATCH_SIZE)\n",
    "                            y_pred = np.argmax(y_pred, axis=2)\n",
    "\n",
    "                            Y_pred_flat = np.extract(mask_val.astype(bool), y_pred)\n",
    "                            Y_test_flat = np.extract(mask_val.astype(bool), y_true)\n",
    "                            print(\"PRECISION:\",compute_f1(Y_pred_flat, Y_test_flat, maps['id2tag']))\n",
    "\n",
    "                        if ops['problem_type'] == 'ner_german':\n",
    "                            _get_metrics(X_test, Y_test)\n",
    "                            _get_metrics(X_train, Y_train)\n",
    "\n",
    "    # #                         print(early_stopper.patience, early_stopper.best, ploss_val)\n",
    "    #                         early_stopper.update(ploss_val)\n",
    "    #                         if (epoch > 100) and early_stopper.patience_ran_out():\n",
    "    #                             print_into_log(LOG_DIRECTORY, \"STOPPED EARLY AT {}\".format(epoch))\n",
    "    #                             break\n",
    "\n",
    "                        # ATTRACTOR(s) LOSS\n",
    "                        aloss = {}\n",
    "                        entropy = {}\n",
    "                        entropy_test = {}\n",
    "                        hid_vals_arr = batch_tensor_collect(sess, [A['h_net_seq_flat'] for att_name, A in\n",
    "                                                                   G_attractors.items()],\n",
    "                                                            X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        h_clean_val_arr = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        h_clean_val_arr_test = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_test, Y_test, BATCH_SIZE)\n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            A = G_attractors[attractor_name]\n",
    "                            a_loss_val = []\n",
    "                            n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "                            for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "                                a_loss_val.append(\n",
    "                                    sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "                            aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "\n",
    "                            entropy[attractor_name] = \"{:.4f}\".format(\n",
    "                                compute_entropy_fullvec(h_clean_val_arr[i], ops, n_bins=8))\n",
    "                            \n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            entropy_test[attractor_name] = \"{:.4f}\".format(\n",
    "                                compute_entropy_fullvec(h_clean_val_arr_test[i], ops, n_bins=8))    \n",
    "\n",
    "                        # Print training information:\n",
    "                        print_into_log(LOG_DIRECTORY, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + get_training_progress_comment(epoch, ploss, aloss, 0.0, 0.0, train_acc,\n",
    "                                                                     test_acc, entropy, entropy_test))\n",
    "                        # Update the logs:\n",
    "                        WS.update_conservative(epoch_number=epoch, loss_att=aloss,\n",
    "                                               loss_task=ploss, acc=test_acc, entropy=entropy)\n",
    "                        if ops['record_mutual_information']:\n",
    "                            h_attractor_val, h_clean_val = batch_tensor_collect(sess, [h_attractor_collection_flat, h_clean_seq_flat],\n",
    "                                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                            MIS.update(ploss, aloss, train_acc, test_acc, np.tanh(hid_vals_arr[0]), h_attractor_val, h_clean_val)\n",
    "\n",
    "                        if (train_acc > best_train_acc):\n",
    "                            best_train_acc = train_acc\n",
    "                            best_test_acc = test_acc\n",
    "                            best_att_loss = aloss\n",
    "                            best_epoch = epoch\n",
    "    #                         best_val_acc = val_acc\n",
    "\n",
    "    #                         best_val_loss = ploss_val\n",
    "    #                         best_train_loss = ploss\n",
    "                            if ops['save_best_model']:\n",
    "                                save_path = saver.save(sess, MODEL_DIRECTORY)\n",
    "                            best_entropy = entropy\n",
    "                            best_entropy_test = entropy_test\n",
    "                        if (train_acc == 1.0):\n",
    "                            print(\"Reached Peak!\")\n",
    "                            break\n",
    "                    if epoch > 1 and LOSS_SWITCH_FREQ > 0 \\\n",
    "                            and (epoch - 1) % LOSS_SWITCH_FREQ == 0:\n",
    "                        train_prediction_loss = not train_prediction_loss\n",
    "\n",
    "                    # MODEL TRAINING\n",
    "                    batches = get_batches(BATCH_SIZE, X_train, Y_train)\n",
    "                    for (batch_x, batch_y) in batches:\n",
    "                        if (LOSS_SWITCH_FREQ == 0 or train_prediction_loss):\n",
    "                            # Optimize all parameters except for attractor weights\n",
    "                            _ = sess.run([pred_train_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y})\n",
    "                        if ops['training_mode'] != 'attractor_on_task':\n",
    "                            if (LOSS_SWITCH_FREQ == 0 or not train_prediction_loss):\n",
    "                                if (N_ATTRACTOR_STEPS > 0):\n",
    "                                    # ATTRACTOR(s) training\n",
    "                                    for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                        A = G_attractors[attractor_name]\n",
    "                                        _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: hid_vals_arr[i]})\n",
    "\n",
    "                print(\"Optimization Finished!\")\n",
    "\n",
    "                if (REPORT_BEST_TRAIN_PERFORMANCE):\n",
    "                    saved_train_acc.append(best_train_acc)\n",
    "                    saved_test_acc.append(best_test_acc)\n",
    "                    saved_att_loss.append(best_att_loss)\n",
    "                    saved_entropy_final.append(best_entropy)\n",
    "                    saved_entropy_final_test.append(best_entropy_test)\n",
    "                    saved_epoch.append(best_epoch)\n",
    "\n",
    "    #                 saved_val_acc.append(best_val_acc)\n",
    "    #                 saved_val_loss.append(best_val_loss)\n",
    "                    saved_traini_loss.append(best_train_loss)\n",
    "                else:\n",
    "                    saved_train_acc.append(train_acc)\n",
    "                    saved_test_acc.append(test_acc)\n",
    "                    #             saved_att_loss.append(aloss)\n",
    "\n",
    "            save_results(ops, saved_epoch, saved_train_acc, saved_test_acc, saved_att_loss, saved_entropy_final, saved_val_acc,\n",
    "                 saved_val_loss, saved_traini_loss, N_TRAIN, N_TEST, SEQ_LEN, COMMENT, saved_entropy_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
