{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "topic_classification\n",
      "((11228, 300), (11228, 1))\n",
      "Loading embeddings...\n",
      "(400000, ' words loaded!')\n",
      "10579\n",
      "3 words not found in pretrained embeddings: ['<PAD>', '<START>', \"may's\"]\n",
      "L2 reg-n\n",
      "********** replication  0  **********\n",
      "topic_classification\n",
      "((11228, 300), (11228, 1))\n",
      "0.0\n",
      "(1965, 300) (1965, 1) (393, 300) (393, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: topic_classification\n",
      "    hid: \t\t\t150,\n",
      "    h_hid: \t\t\t300\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t1965/2245, SEQ_LEN: 300\n",
      "Logged Successfully: \n",
      "0 10000000000.0 3.8673987\n",
      "Logged Successfully: \n",
      "2018-07-16 13:52:29epoch=0; Loss Pred=3.8723; Val Loss=3.8674; Val Acc=0.0051; Loss Att={'forw': '1.1015'}; Train Acc=0.006; Test Acc=0.0073; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 3.8673987 2.4646444\n",
      "Logged Successfully: \n",
      "2018-07-16 13:52:35epoch=1; Loss Pred=2.5368; Val Loss=2.4646; Val Acc=0.3384; Loss Att={'forw': '1.2243'}; Train Acc=0.292; Test Acc=0.3186; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4646444 2.4668708\n",
      "Logged Successfully: \n",
      "2018-07-16 13:52:39epoch=2; Loss Pred=2.5322; Val Loss=2.4669; Val Acc=0.3257; Loss Att={'forw': '0.9017'}; Train Acc=0.307; Test Acc=0.3189; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4646444 2.4493613\n",
      "Logged Successfully: \n",
      "2018-07-16 13:52:44epoch=3; Loss Pred=2.4930; Val Loss=2.4494; Val Acc=0.3817; Loss Att={'forw': '1.1215'}; Train Acc=0.340; Test Acc=0.3568; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4493613 2.4517748\n",
      "Logged Successfully: \n",
      "2018-07-16 13:52:48epoch=4; Loss Pred=2.4891; Val Loss=2.4518; Val Acc=0.3817; Loss Att={'forw': '0.8375'}; Train Acc=0.339; Test Acc=0.3565; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4493613 2.4004757\n",
      "Logged Successfully: \n",
      "2018-07-16 13:52:53epoch=5; Loss Pred=2.4449; Val Loss=2.4005; Val Acc=0.3791; Loss Att={'forw': '0.8485'}; Train Acc=0.337; Test Acc=0.3545; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4004757 2.4010203\n",
      "Logged Successfully: \n",
      "2018-07-16 13:52:57epoch=6; Loss Pred=2.4434; Val Loss=2.4010; Val Acc=0.3868; Loss Att={'forw': '0.8740'}; Train Acc=0.338; Test Acc=0.3504; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4004757 2.382227\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:02epoch=7; Loss Pred=2.4250; Val Loss=2.3822; Val Acc=0.3842; Loss Att={'forw': '0.9098'}; Train Acc=0.341; Test Acc=0.3563; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.382227 2.3830595\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:06epoch=8; Loss Pred=2.4315; Val Loss=2.3831; Val Acc=0.3817; Loss Att={'forw': '0.9049'}; Train Acc=0.340; Test Acc=0.3570; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.382227 2.4235008\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:12epoch=9; Loss Pred=2.4205; Val Loss=2.4235; Val Acc=0.3817; Loss Att={'forw': '0.9301'}; Train Acc=0.337; Test Acc=0.3597; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.382227 2.414847\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:16epoch=10; Loss Pred=2.4246; Val Loss=2.4148; Val Acc=0.3817; Loss Att={'forw': '0.8900'}; Train Acc=0.342; Test Acc=0.3564; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.382227 2.405501\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:21epoch=11; Loss Pred=2.4125; Val Loss=2.4055; Val Acc=0.3766; Loss Att={'forw': '0.8821'}; Train Acc=0.339; Test Acc=0.3595; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.382227 2.407375\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:25epoch=12; Loss Pred=2.4074; Val Loss=2.4074; Val Acc=0.3791; Loss Att={'forw': '0.8643'}; Train Acc=0.339; Test Acc=0.3602; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.382227 2.3838549\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:31epoch=13; Loss Pred=2.4022; Val Loss=2.3839; Val Acc=0.3715; Loss Att={'forw': '0.8444'}; Train Acc=0.348; Test Acc=0.3614; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.382227 2.377348\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:35epoch=14; Loss Pred=2.4036; Val Loss=2.3773; Val Acc=0.3842; Loss Att={'forw': '0.8433'}; Train Acc=0.341; Test Acc=0.3634; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.377348 2.3852746\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:40epoch=15; Loss Pred=2.4062; Val Loss=2.3853; Val Acc=0.3766; Loss Att={'forw': '0.9540'}; Train Acc=0.341; Test Acc=0.3600; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.377348 2.3671155\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:45epoch=16; Loss Pred=2.4028; Val Loss=2.3671; Val Acc=0.3791; Loss Att={'forw': '0.8641'}; Train Acc=0.343; Test Acc=0.3588; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3671155 2.3621132\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:50epoch=17; Loss Pred=2.3473; Val Loss=2.3621; Val Acc=0.3995; Loss Att={'forw': '0.8556'}; Train Acc=0.369; Test Acc=0.3705; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3621132 2.362193\n",
      "Logged Successfully: \n",
      "2018-07-16 13:53:54epoch=18; Loss Pred=2.3375; Val Loss=2.3622; Val Acc=0.3893; Loss Att={'forw': '0.8613'}; Train Acc=0.371; Test Acc=0.3761; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3621132 2.3502176\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:00epoch=19; Loss Pred=2.3710; Val Loss=2.3502; Val Acc=0.3613; Loss Att={'forw': '0.8533'}; Train Acc=0.354; Test Acc=0.3489; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3502176 2.3600826\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:04epoch=20; Loss Pred=2.3604; Val Loss=2.3601; Val Acc=0.3740; Loss Att={'forw': '0.8306'}; Train Acc=0.361; Test Acc=0.3506; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3502176 2.3016872\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:09epoch=21; Loss Pred=2.3000; Val Loss=2.3017; Val Acc=0.3893; Loss Att={'forw': '0.8409'}; Train Acc=0.390; Test Acc=0.3695; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3016872 2.3032424\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:13epoch=22; Loss Pred=2.3012; Val Loss=2.3032; Val Acc=0.3944; Loss Att={'forw': '0.8332'}; Train Acc=0.384; Test Acc=0.3711; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3016872 2.2544281\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:19epoch=23; Loss Pred=2.2827; Val Loss=2.2544; Val Acc=0.3995; Loss Att={'forw': '0.8693'}; Train Acc=0.382; Test Acc=0.3767; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2544281 2.2573922\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:23epoch=24; Loss Pred=2.2838; Val Loss=2.2574; Val Acc=0.3919; Loss Att={'forw': '0.8551'}; Train Acc=0.374; Test Acc=0.3743; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2544281 2.2846103\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:28epoch=25; Loss Pred=2.2668; Val Loss=2.2846; Val Acc=0.3944; Loss Att={'forw': '0.8239'}; Train Acc=0.382; Test Acc=0.3894; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2544281 2.2834208\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:32epoch=26; Loss Pred=2.2644; Val Loss=2.2834; Val Acc=0.3944; Loss Att={'forw': '0.8488'}; Train Acc=0.388; Test Acc=0.3781; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2544281 2.2728121\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:38epoch=27; Loss Pred=2.2361; Val Loss=2.2728; Val Acc=0.4071; Loss Att={'forw': '0.9113'}; Train Acc=0.389; Test Acc=0.3880; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2544281 2.2653377\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:42epoch=28; Loss Pred=2.2406; Val Loss=2.2653; Val Acc=0.4173; Loss Att={'forw': '0.8526'}; Train Acc=0.393; Test Acc=0.3883; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2544281 2.2630002\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:47epoch=29; Loss Pred=2.2474; Val Loss=2.2630; Val Acc=0.3995; Loss Att={'forw': '0.8212'}; Train Acc=0.384; Test Acc=0.3817; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.2544281 2.2650533\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:52epoch=30; Loss Pred=2.2438; Val Loss=2.2651; Val Acc=0.3919; Loss Att={'forw': '0.8103'}; Train Acc=0.380; Test Acc=0.3841; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.2544281 2.2396839\n",
      "Logged Successfully: \n",
      "2018-07-16 13:54:57epoch=31; Loss Pred=2.2213; Val Loss=2.2397; Val Acc=0.4122; Loss Att={'forw': '0.8326'}; Train Acc=0.396; Test Acc=0.3876; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2396839 2.2267013\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:01epoch=32; Loss Pred=2.2163; Val Loss=2.2267; Val Acc=0.4046; Loss Att={'forw': '0.8274'}; Train Acc=0.394; Test Acc=0.3929; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2267013 2.220088\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:07epoch=33; Loss Pred=2.1998; Val Loss=2.2201; Val Acc=0.4351; Loss Att={'forw': '0.8455'}; Train Acc=0.401; Test Acc=0.3922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.220088 2.2320087\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:11epoch=34; Loss Pred=2.2019; Val Loss=2.2320; Val Acc=0.4198; Loss Att={'forw': '0.8286'}; Train Acc=0.403; Test Acc=0.3997; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.220088 2.2385256\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:17epoch=35; Loss Pred=2.2170; Val Loss=2.2385; Val Acc=0.3740; Loss Att={'forw': '0.8226'}; Train Acc=0.403; Test Acc=0.3760; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.220088 2.2542531\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:21epoch=36; Loss Pred=2.2060; Val Loss=2.2543; Val Acc=0.3715; Loss Att={'forw': '0.8001'}; Train Acc=0.407; Test Acc=0.3820; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.220088 2.2075071\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:26epoch=37; Loss Pred=2.1908; Val Loss=2.2075; Val Acc=0.4071; Loss Att={'forw': '0.8360'}; Train Acc=0.405; Test Acc=0.3941; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2075071 2.22924\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:30epoch=38; Loss Pred=2.1791; Val Loss=2.2292; Val Acc=0.4198; Loss Att={'forw': '0.8397'}; Train Acc=0.403; Test Acc=0.3897; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2075071 2.1857524\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:36epoch=39; Loss Pred=2.1545; Val Loss=2.1858; Val Acc=0.4224; Loss Att={'forw': '0.8149'}; Train Acc=0.418; Test Acc=0.3942; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1857524 2.1764715\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:40epoch=40; Loss Pred=2.1506; Val Loss=2.1765; Val Acc=0.4351; Loss Att={'forw': '0.8290'}; Train Acc=0.421; Test Acc=0.4026; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1764715 2.4089713\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:45epoch=41; Loss Pred=2.3079; Val Loss=2.4090; Val Acc=0.3435; Loss Att={'forw': '1.0107'}; Train Acc=0.368; Test Acc=0.3458; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.1764715 2.4039907\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:49epoch=42; Loss Pred=2.3057; Val Loss=2.4040; Val Acc=0.3181; Loss Att={'forw': '0.8414'}; Train Acc=0.345; Test Acc=0.3386; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.1764715 2.2367668\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:55epoch=43; Loss Pred=2.2461; Val Loss=2.2368; Val Acc=0.3944; Loss Att={'forw': '0.8840'}; Train Acc=0.362; Test Acc=0.3645; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.1764715 2.2679753\n",
      "Logged Successfully: \n",
      "2018-07-16 13:55:59epoch=44; Loss Pred=2.2522; Val Loss=2.2680; Val Acc=0.3791; Loss Att={'forw': '0.8586'}; Train Acc=0.352; Test Acc=0.3630; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.1764715 2.2686567\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:04epoch=45; Loss Pred=2.2187; Val Loss=2.2687; Val Acc=0.4020; Loss Att={'forw': '0.8488'}; Train Acc=0.403; Test Acc=0.3856; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.1764715 2.29521\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:08epoch=46; Loss Pred=2.2164; Val Loss=2.2952; Val Acc=0.3944; Loss Att={'forw': '0.8417'}; Train Acc=0.409; Test Acc=0.3804; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.1764715 2.2494488\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:14epoch=47; Loss Pred=2.1835; Val Loss=2.2494; Val Acc=0.4275; Loss Att={'forw': '0.8504'}; Train Acc=0.412; Test Acc=0.3961; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.1764715 2.2481086\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:18epoch=48; Loss Pred=2.2072; Val Loss=2.2481; Val Acc=0.4071; Loss Att={'forw': '0.8342'}; Train Acc=0.400; Test Acc=0.3975; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.1764715 2.1896346\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:23epoch=49; Loss Pred=2.1500; Val Loss=2.1896; Val Acc=0.4097; Loss Att={'forw': '0.8682'}; Train Acc=0.408; Test Acc=0.3970; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.1764715 2.2038145\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:28epoch=50; Loss Pred=2.1521; Val Loss=2.2038; Val Acc=0.4198; Loss Att={'forw': '0.8435'}; Train Acc=0.419; Test Acc=0.3881; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.1764715 2.2059815\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:33epoch=51; Loss Pred=2.1484; Val Loss=2.2060; Val Acc=0.4198; Loss Att={'forw': '0.8750'}; Train Acc=0.406; Test Acc=0.3968; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.1764715 2.225558\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:37epoch=52; Loss Pred=2.1567; Val Loss=2.2256; Val Acc=0.4224; Loss Att={'forw': '0.8851'}; Train Acc=0.398; Test Acc=0.3941; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.1764715 2.1925526\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:43epoch=53; Loss Pred=2.1435; Val Loss=2.1926; Val Acc=0.4198; Loss Att={'forw': '0.8570'}; Train Acc=0.400; Test Acc=0.3866; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.1764715 2.1872895\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:47epoch=54; Loss Pred=2.1506; Val Loss=2.1873; Val Acc=0.4249; Loss Att={'forw': '0.8649'}; Train Acc=0.392; Test Acc=0.3969; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.1764715 2.2746072\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:52epoch=55; Loss Pred=2.1913; Val Loss=2.2746; Val Acc=0.4020; Loss Att={'forw': '0.8790'}; Train Acc=0.392; Test Acc=0.3837; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.1764715 2.2313154\n",
      "Logged Successfully: \n",
      "2018-07-16 13:56:56epoch=56; Loss Pred=2.1627; Val Loss=2.2313; Val Acc=0.4020; Loss Att={'forw': '0.8649'}; Train Acc=0.395; Test Acc=0.3885; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.1764715 2.2074397\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:02epoch=57; Loss Pred=2.1130; Val Loss=2.2074; Val Acc=0.4046; Loss Att={'forw': '0.8777'}; Train Acc=0.408; Test Acc=0.3804; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.1764715 2.227626\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:06epoch=58; Loss Pred=2.1564; Val Loss=2.2276; Val Acc=0.3944; Loss Att={'forw': '0.8420'}; Train Acc=0.398; Test Acc=0.3854; Entropy=0; Entropy_Test=\n",
      "\n",
      "18 2.1764715 2.2890675\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:11epoch=59; Loss Pred=2.2192; Val Loss=2.2891; Val Acc=0.4249; Loss Att={'forw': '0.9128'}; Train Acc=0.398; Test Acc=0.3881; Entropy=0; Entropy_Test=\n",
      "\n",
      "19 2.1764715 2.2702475\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:16epoch=60; Loss Pred=2.2252; Val Loss=2.2702; Val Acc=0.4097; Loss Att={'forw': '0.8753'}; Train Acc=0.385; Test Acc=0.3843; Entropy=0; Entropy_Test=\n",
      "\n",
      "20 2.1764715 2.1772466\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:21epoch=61; Loss Pred=2.1219; Val Loss=2.1772; Val Acc=0.4224; Loss Att={'forw': '0.8592'}; Train Acc=0.423; Test Acc=0.4089; Entropy=0; Entropy_Test=\n",
      "\n",
      "21 2.1764715 2.1844091\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:25epoch=62; Loss Pred=2.1212; Val Loss=2.1844; Val Acc=0.4326; Loss Att={'forw': '0.8466'}; Train Acc=0.434; Test Acc=0.4025; Entropy=0; Entropy_Test=\n",
      "\n",
      "22 2.1764715 2.2496004\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:30epoch=63; Loss Pred=2.1603; Val Loss=2.2496; Val Acc=0.3562; Loss Att={'forw': '0.8326'}; Train Acc=0.402; Test Acc=0.3755; Entropy=0; Entropy_Test=\n",
      "\n",
      "23 2.1764715 2.257745\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:35epoch=64; Loss Pred=2.1721; Val Loss=2.2577; Val Acc=0.3461; Loss Att={'forw': '0.8379'}; Train Acc=0.405; Test Acc=0.3680; Entropy=0; Entropy_Test=\n",
      "\n",
      "24 2.1764715 2.1750503\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:40epoch=65; Loss Pred=2.0957; Val Loss=2.1751; Val Acc=0.4326; Loss Att={'forw': '0.8441'}; Train Acc=0.424; Test Acc=0.3955; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1750503 2.162773\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:44epoch=66; Loss Pred=2.0821; Val Loss=2.1628; Val Acc=0.4071; Loss Att={'forw': '0.8341'}; Train Acc=0.416; Test Acc=0.3944; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.162773 2.1957896\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:49epoch=67; Loss Pred=2.0818; Val Loss=2.1958; Val Acc=0.4122; Loss Att={'forw': '0.8547'}; Train Acc=0.421; Test Acc=0.4041; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.162773 2.188308\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:53epoch=68; Loss Pred=2.0824; Val Loss=2.1883; Val Acc=0.4427; Loss Att={'forw': '0.8439'}; Train Acc=0.437; Test Acc=0.4037; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.162773 2.1638565\n",
      "Logged Successfully: \n",
      "2018-07-16 13:57:59epoch=69; Loss Pred=2.0712; Val Loss=2.1639; Val Acc=0.4046; Loss Att={'forw': '0.8252'}; Train Acc=0.431; Test Acc=0.3952; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.162773 2.1604638\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:03epoch=70; Loss Pred=2.0908; Val Loss=2.1605; Val Acc=0.3944; Loss Att={'forw': '0.8282'}; Train Acc=0.429; Test Acc=0.3967; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1604638 2.155002\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:09epoch=71; Loss Pred=2.0529; Val Loss=2.1550; Val Acc=0.3893; Loss Att={'forw': '0.8269'}; Train Acc=0.429; Test Acc=0.3854; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.155002 2.1816375\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:13epoch=72; Loss Pred=2.0525; Val Loss=2.1816; Val Acc=0.3893; Loss Att={'forw': '0.8208'}; Train Acc=0.433; Test Acc=0.3821; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.155002 2.282413\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:18epoch=73; Loss Pred=2.1988; Val Loss=2.2824; Val Acc=0.3410; Loss Att={'forw': '0.8324'}; Train Acc=0.393; Test Acc=0.3637; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.155002 2.271946\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:22epoch=74; Loss Pred=2.1644; Val Loss=2.2719; Val Acc=0.3359; Loss Att={'forw': '0.8222'}; Train Acc=0.405; Test Acc=0.3816; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.155002 2.2852104\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:28epoch=75; Loss Pred=2.1698; Val Loss=2.2852; Val Acc=0.4097; Loss Att={'forw': '0.8803'}; Train Acc=0.398; Test Acc=0.3862; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.155002 2.3181372\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:32epoch=76; Loss Pred=2.1663; Val Loss=2.3181; Val Acc=0.4071; Loss Att={'forw': '0.8520'}; Train Acc=0.404; Test Acc=0.3835; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.155002 2.2109804\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:37epoch=77; Loss Pred=2.1023; Val Loss=2.2110; Val Acc=0.3664; Loss Att={'forw': '0.8438'}; Train Acc=0.399; Test Acc=0.3737; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.155002 2.2358713\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:42epoch=78; Loss Pred=2.1106; Val Loss=2.2359; Val Acc=0.3817; Loss Att={'forw': '0.8297'}; Train Acc=0.398; Test Acc=0.3702; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.155002 2.1872556\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:47epoch=79; Loss Pred=2.0311; Val Loss=2.1873; Val Acc=0.4071; Loss Att={'forw': '0.8305'}; Train Acc=0.438; Test Acc=0.3988; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.155002 2.1704254\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:51epoch=80; Loss Pred=2.0352; Val Loss=2.1704; Val Acc=0.4198; Loss Att={'forw': '0.8234'}; Train Acc=0.432; Test Acc=0.3869; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.155002 2.1843996\n",
      "Logged Successfully: \n",
      "2018-07-16 13:58:57epoch=81; Loss Pred=2.0427; Val Loss=2.1844; Val Acc=0.3944; Loss Att={'forw': '0.8247'}; Train Acc=0.444; Test Acc=0.3965; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.155002 2.1905732\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:01epoch=82; Loss Pred=2.0219; Val Loss=2.1906; Val Acc=0.4071; Loss Att={'forw': '0.8145'}; Train Acc=0.447; Test Acc=0.3936; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.155002 2.2716486\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:06epoch=83; Loss Pred=2.2221; Val Loss=2.2716; Val Acc=0.3868; Loss Att={'forw': '0.8388'}; Train Acc=0.419; Test Acc=0.3815; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.155002 2.3324683\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:10epoch=84; Loss Pred=2.2336; Val Loss=2.3325; Val Acc=0.3715; Loss Att={'forw': '0.8448'}; Train Acc=0.430; Test Acc=0.3737; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.155002 2.2157843\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:16epoch=85; Loss Pred=2.1392; Val Loss=2.2158; Val Acc=0.4173; Loss Att={'forw': '0.8547'}; Train Acc=0.430; Test Acc=0.3863; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.155002 2.2318692\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:20epoch=86; Loss Pred=2.1498; Val Loss=2.2319; Val Acc=0.4020; Loss Att={'forw': '0.8341'}; Train Acc=0.431; Test Acc=0.3911; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.155002 2.2541885\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:25epoch=87; Loss Pred=2.1287; Val Loss=2.2542; Val Acc=0.3919; Loss Att={'forw': '0.8637'}; Train Acc=0.420; Test Acc=0.3948; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.155002 2.2513506\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:30epoch=88; Loss Pred=2.1173; Val Loss=2.2514; Val Acc=0.3740; Loss Att={'forw': '0.8299'}; Train Acc=0.425; Test Acc=0.3904; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.155002 2.2291822\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:35epoch=89; Loss Pred=2.0762; Val Loss=2.2292; Val Acc=0.4249; Loss Att={'forw': '0.8326'}; Train Acc=0.440; Test Acc=0.3949; Entropy=0; Entropy_Test=\n",
      "\n",
      "18 2.155002 2.2088091\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:39epoch=90; Loss Pred=2.0764; Val Loss=2.2088; Val Acc=0.4173; Loss Att={'forw': '0.8193'}; Train Acc=0.438; Test Acc=0.3910; Entropy=0; Entropy_Test=\n",
      "\n",
      "19 2.155002 2.1955762\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:44epoch=91; Loss Pred=2.0679; Val Loss=2.1956; Val Acc=0.4249; Loss Att={'forw': '0.8400'}; Train Acc=0.438; Test Acc=0.3902; Entropy=0; Entropy_Test=\n",
      "\n",
      "20 2.155002 2.2037036\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:49epoch=92; Loss Pred=2.0626; Val Loss=2.2037; Val Acc=0.4071; Loss Att={'forw': '0.8247'}; Train Acc=0.440; Test Acc=0.3870; Entropy=0; Entropy_Test=\n",
      "\n",
      "21 2.155002 2.1674745\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:54epoch=93; Loss Pred=2.0413; Val Loss=2.1675; Val Acc=0.4198; Loss Att={'forw': '0.8491'}; Train Acc=0.428; Test Acc=0.3946; Entropy=0; Entropy_Test=\n",
      "\n",
      "22 2.155002 2.1592574\n",
      "Logged Successfully: \n",
      "2018-07-16 13:59:58epoch=94; Loss Pred=2.0310; Val Loss=2.1593; Val Acc=0.4249; Loss Att={'forw': '0.8368'}; Train Acc=0.433; Test Acc=0.3966; Entropy=0; Entropy_Test=\n",
      "\n",
      "23 2.155002 2.1393838\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:04epoch=95; Loss Pred=2.0000; Val Loss=2.1394; Val Acc=0.4097; Loss Att={'forw': '0.8371'}; Train Acc=0.448; Test Acc=0.3936; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1393838 2.1311548\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:08epoch=96; Loss Pred=1.9943; Val Loss=2.1312; Val Acc=0.4377; Loss Att={'forw': '0.8254'}; Train Acc=0.441; Test Acc=0.3951; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1311548 2.1667047\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:13epoch=97; Loss Pred=2.0327; Val Loss=2.1667; Val Acc=0.3893; Loss Att={'forw': '0.8475'}; Train Acc=0.440; Test Acc=0.3864; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.1311548 2.1652818\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:17epoch=98; Loss Pred=1.9975; Val Loss=2.1653; Val Acc=0.3919; Loss Att={'forw': '0.8257'}; Train Acc=0.440; Test Acc=0.3938; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.1311548 2.2664683\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:23epoch=99; Loss Pred=2.1307; Val Loss=2.2665; Val Acc=0.4427; Loss Att={'forw': '0.8407'}; Train Acc=0.434; Test Acc=0.4012; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.1311548 2.2404249\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:27epoch=100; Loss Pred=2.0876; Val Loss=2.2404; Val Acc=0.4377; Loss Att={'forw': '0.8206'}; Train Acc=0.438; Test Acc=0.4046; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.1311548 2.176161\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:32epoch=101; Loss Pred=1.9623; Val Loss=2.1762; Val Acc=0.4198; Loss Att={'forw': '0.8165'}; Train Acc=0.471; Test Acc=0.4097; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.1311548 2.1785257\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:36epoch=102; Loss Pred=1.9681; Val Loss=2.1785; Val Acc=0.4249; Loss Att={'forw': '0.8189'}; Train Acc=0.462; Test Acc=0.4126; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.1311548 2.129975\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:42epoch=103; Loss Pred=1.9710; Val Loss=2.1300; Val Acc=0.4122; Loss Att={'forw': '0.8290'}; Train Acc=0.463; Test Acc=0.4065; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.129975 2.1628115\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:46epoch=104; Loss Pred=1.9533; Val Loss=2.1628; Val Acc=0.4148; Loss Att={'forw': '0.8229'}; Train Acc=0.479; Test Acc=0.4062; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.129975 2.1857893\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:51epoch=105; Loss Pred=2.0179; Val Loss=2.1858; Val Acc=0.3893; Loss Att={'forw': '0.8179'}; Train Acc=0.440; Test Acc=0.3909; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.129975 2.194543\n",
      "Logged Successfully: \n",
      "2018-07-16 14:00:56epoch=106; Loss Pred=2.0049; Val Loss=2.1945; Val Acc=0.3995; Loss Att={'forw': '0.8115'}; Train Acc=0.443; Test Acc=0.3941; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.129975 2.0777743\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:01epoch=107; Loss Pred=1.8952; Val Loss=2.0778; Val Acc=0.4122; Loss Att={'forw': '0.8185'}; Train Acc=0.471; Test Acc=0.4157; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.0777743 2.0815156\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:05epoch=108; Loss Pred=1.9075; Val Loss=2.0815; Val Acc=0.4326; Loss Att={'forw': '0.8116'}; Train Acc=0.472; Test Acc=0.4078; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.0777743 2.1296957\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:10epoch=109; Loss Pred=1.8942; Val Loss=2.1297; Val Acc=0.4198; Loss Att={'forw': '0.8190'}; Train Acc=0.483; Test Acc=0.4033; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.0777743 2.1223605\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:15epoch=110; Loss Pred=1.9024; Val Loss=2.1224; Val Acc=0.4249; Loss Att={'forw': '0.8116'}; Train Acc=0.484; Test Acc=0.4054; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.0777743 2.0633092\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:20epoch=111; Loss Pred=1.8509; Val Loss=2.0633; Val Acc=0.4275; Loss Att={'forw': '0.8080'}; Train Acc=0.497; Test Acc=0.4239; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.0633092 2.0929573\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:24epoch=112; Loss Pred=1.8540; Val Loss=2.0930; Val Acc=0.4326; Loss Att={'forw': '0.8038'}; Train Acc=0.503; Test Acc=0.4179; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.0633092 2.3811915\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:30epoch=113; Loss Pred=2.2490; Val Loss=2.3812; Val Acc=0.3613; Loss Att={'forw': '0.8468'}; Train Acc=0.371; Test Acc=0.3287; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.0633092 2.3680885\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:34epoch=114; Loss Pred=2.2623; Val Loss=2.3681; Val Acc=0.3537; Loss Att={'forw': '0.8427'}; Train Acc=0.370; Test Acc=0.3263; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.0633092 2.2592223\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:39epoch=115; Loss Pred=2.1351; Val Loss=2.2592; Val Acc=0.4071; Loss Att={'forw': '0.8443'}; Train Acc=0.463; Test Acc=0.3918; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.0633092 2.2638187\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:43epoch=116; Loss Pred=2.1409; Val Loss=2.2638; Val Acc=0.4148; Loss Att={'forw': '0.8458'}; Train Acc=0.454; Test Acc=0.3901; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.0633092 2.2498994\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:48epoch=117; Loss Pred=2.0992; Val Loss=2.2499; Val Acc=0.4148; Loss Att={'forw': '0.8413'}; Train Acc=0.437; Test Acc=0.3929; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.0633092 2.2651918\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:53epoch=118; Loss Pred=2.1052; Val Loss=2.2652; Val Acc=0.4148; Loss Att={'forw': '0.8312'}; Train Acc=0.445; Test Acc=0.3890; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.0633092 2.2318385\n",
      "Logged Successfully: \n",
      "2018-07-16 14:01:58epoch=119; Loss Pred=2.0453; Val Loss=2.2318; Val Acc=0.4046; Loss Att={'forw': '0.8370'}; Train Acc=0.467; Test Acc=0.3816; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.0633092 2.2566464\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:03epoch=120; Loss Pred=2.0504; Val Loss=2.2566; Val Acc=0.4071; Loss Att={'forw': '0.8354'}; Train Acc=0.456; Test Acc=0.3853; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.0633092 2.2010598\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:08epoch=121; Loss Pred=2.0297; Val Loss=2.2011; Val Acc=0.3995; Loss Att={'forw': '0.8386'}; Train Acc=0.436; Test Acc=0.3683; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.0633092 2.2306323\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:12epoch=122; Loss Pred=2.0301; Val Loss=2.2306; Val Acc=0.4173; Loss Att={'forw': '0.8326'}; Train Acc=0.426; Test Acc=0.3719; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.0633092 2.2118852\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:17epoch=123; Loss Pred=1.9378; Val Loss=2.2119; Val Acc=0.4148; Loss Att={'forw': '0.8480'}; Train Acc=0.482; Test Acc=0.3944; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.0633092 2.209374\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:22epoch=124; Loss Pred=1.9349; Val Loss=2.2094; Val Acc=0.4122; Loss Att={'forw': '0.8312'}; Train Acc=0.487; Test Acc=0.3942; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.0633092 2.2676783\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:27epoch=125; Loss Pred=2.0344; Val Loss=2.2677; Val Acc=0.4377; Loss Att={'forw': '0.8580'}; Train Acc=0.481; Test Acc=0.4201; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.0633092 2.2410717\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:31epoch=126; Loss Pred=1.9982; Val Loss=2.2411; Val Acc=0.4656; Loss Att={'forw': '0.8562'}; Train Acc=0.481; Test Acc=0.4171; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.0633092 2.2696788\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:37epoch=127; Loss Pred=1.9990; Val Loss=2.2697; Val Acc=0.4173; Loss Att={'forw': '0.8520'}; Train Acc=0.474; Test Acc=0.4027; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.0633092 2.276011\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:41epoch=128; Loss Pred=1.9987; Val Loss=2.2760; Val Acc=0.4377; Loss Att={'forw': '0.8323'}; Train Acc=0.463; Test Acc=0.4062; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.0633092 2.173818\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:46epoch=129; Loss Pred=1.8310; Val Loss=2.1738; Val Acc=0.4402; Loss Att={'forw': '0.8479'}; Train Acc=0.490; Test Acc=0.4221; Entropy=0; Entropy_Test=\n",
      "\n",
      "18 2.0633092 2.110922\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:50epoch=130; Loss Pred=1.8411; Val Loss=2.1109; Val Acc=0.4478; Loss Att={'forw': '0.8401'}; Train Acc=0.500; Test Acc=0.4169; Entropy=0; Entropy_Test=\n",
      "\n",
      "19 2.0633092 2.3740525\n",
      "Logged Successfully: \n",
      "2018-07-16 14:02:56epoch=131; Loss Pred=2.3027; Val Loss=2.3741; Val Acc=0.3690; Loss Att={'forw': '0.8849'}; Train Acc=0.384; Test Acc=0.3453; Entropy=0; Entropy_Test=\n",
      "\n",
      "20 2.0633092 2.3414032\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:00epoch=132; Loss Pred=2.2521; Val Loss=2.3414; Val Acc=0.3588; Loss Att={'forw': '0.8626'}; Train Acc=0.393; Test Acc=0.3524; Entropy=0; Entropy_Test=\n",
      "\n",
      "21 2.0633092 2.415036\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:05epoch=133; Loss Pred=2.1934; Val Loss=2.4150; Val Acc=0.3995; Loss Att={'forw': '0.8841'}; Train Acc=0.433; Test Acc=0.3856; Entropy=0; Entropy_Test=\n",
      "\n",
      "22 2.0633092 2.3505564\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:10epoch=134; Loss Pred=2.1887; Val Loss=2.3506; Val Acc=0.4198; Loss Att={'forw': '0.8648'}; Train Acc=0.438; Test Acc=0.3822; Entropy=0; Entropy_Test=\n",
      "\n",
      "23 2.0633092 2.374872\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:15epoch=135; Loss Pred=2.1556; Val Loss=2.3749; Val Acc=0.3766; Loss Att={'forw': '0.9132'}; Train Acc=0.429; Test Acc=0.3818; Entropy=0; Entropy_Test=\n",
      "\n",
      "24 2.0633092 2.3676376\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:19epoch=136; Loss Pred=2.1899; Val Loss=2.3676; Val Acc=0.3868; Loss Att={'forw': '0.8775'}; Train Acc=0.411; Test Acc=0.3806; Entropy=0; Entropy_Test=\n",
      "\n",
      "25 2.0633092 2.3229458\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:24epoch=137; Loss Pred=2.1312; Val Loss=2.3229; Val Acc=0.3740; Loss Att={'forw': '0.8443'}; Train Acc=0.427; Test Acc=0.3676; Entropy=0; Entropy_Test=\n",
      "\n",
      "26 2.0633092 2.3375638\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:28epoch=138; Loss Pred=2.1622; Val Loss=2.3376; Val Acc=0.3562; Loss Att={'forw': '0.8326'}; Train Acc=0.416; Test Acc=0.3690; Entropy=0; Entropy_Test=\n",
      "\n",
      "27 2.0633092 2.2737787\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:34epoch=139; Loss Pred=2.0948; Val Loss=2.2738; Val Acc=0.4097; Loss Att={'forw': '0.8555'}; Train Acc=0.445; Test Acc=0.3831; Entropy=0; Entropy_Test=\n",
      "\n",
      "28 2.0633092 2.2909317\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:38epoch=140; Loss Pred=2.1007; Val Loss=2.2909; Val Acc=0.4198; Loss Att={'forw': '0.8421'}; Train Acc=0.443; Test Acc=0.3769; Entropy=0; Entropy_Test=\n",
      "\n",
      "29 2.0633092 2.2638373\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:43epoch=141; Loss Pred=2.0844; Val Loss=2.2638; Val Acc=0.3893; Loss Att={'forw': '0.8487'}; Train Acc=0.446; Test Acc=0.3869; Entropy=0; Entropy_Test=\n",
      "\n",
      "30 2.0633092 2.292281\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:48epoch=142; Loss Pred=2.0869; Val Loss=2.2923; Val Acc=0.3944; Loss Att={'forw': '0.8321'}; Train Acc=0.451; Test Acc=0.3808; Entropy=0; Entropy_Test=\n",
      "\n",
      "31 2.0633092 2.2870328\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:53epoch=143; Loss Pred=2.0103; Val Loss=2.2870; Val Acc=0.4148; Loss Att={'forw': '0.8387'}; Train Acc=0.462; Test Acc=0.3792; Entropy=0; Entropy_Test=\n",
      "\n",
      "32 2.0633092 2.2741706\n",
      "Logged Successfully: \n",
      "2018-07-16 14:03:57epoch=144; Loss Pred=2.0287; Val Loss=2.2742; Val Acc=0.3969; Loss Att={'forw': '0.8415'}; Train Acc=0.452; Test Acc=0.3752; Entropy=0; Entropy_Test=\n",
      "\n",
      "33 2.0633092 2.2657177\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:03epoch=145; Loss Pred=1.9804; Val Loss=2.2657; Val Acc=0.4046; Loss Att={'forw': '0.8377'}; Train Acc=0.476; Test Acc=0.3924; Entropy=0; Entropy_Test=\n",
      "\n",
      "34 2.0633092 2.2795596\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:07epoch=146; Loss Pred=1.9893; Val Loss=2.2796; Val Acc=0.3969; Loss Att={'forw': '0.8359'}; Train Acc=0.474; Test Acc=0.3893; Entropy=0; Entropy_Test=\n",
      "\n",
      "35 2.0633092 2.2968562\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:12epoch=147; Loss Pred=1.9558; Val Loss=2.2969; Val Acc=0.4122; Loss Att={'forw': '0.8361'}; Train Acc=0.477; Test Acc=0.3851; Entropy=0; Entropy_Test=\n",
      "\n",
      "36 2.0633092 2.2728539\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:16epoch=148; Loss Pred=1.9463; Val Loss=2.2729; Val Acc=0.4249; Loss Att={'forw': '0.8289'}; Train Acc=0.478; Test Acc=0.3898; Entropy=0; Entropy_Test=\n",
      "\n",
      "37 2.0633092 2.2524452\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:22epoch=149; Loss Pred=1.9249; Val Loss=2.2524; Val Acc=0.4173; Loss Att={'forw': '0.8316'}; Train Acc=0.490; Test Acc=0.3831; Entropy=0; Entropy_Test=\n",
      "\n",
      "38 2.0633092 2.2677426\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:26epoch=150; Loss Pred=1.9146; Val Loss=2.2677; Val Acc=0.4020; Loss Att={'forw': '0.8309'}; Train Acc=0.494; Test Acc=0.3903; Entropy=0; Entropy_Test=\n",
      "\n",
      "39 2.0633092 2.2342496\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:31epoch=151; Loss Pred=1.8923; Val Loss=2.2342; Val Acc=0.4224; Loss Att={'forw': '0.8252'}; Train Acc=0.493; Test Acc=0.3972; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 2.0633092 2.2579348\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:35epoch=152; Loss Pred=1.9076; Val Loss=2.2579; Val Acc=0.4198; Loss Att={'forw': '0.8261'}; Train Acc=0.493; Test Acc=0.3983; Entropy=0; Entropy_Test=\n",
      "\n",
      "41 2.0633092 2.298102\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:41epoch=153; Loss Pred=1.8916; Val Loss=2.2981; Val Acc=0.4148; Loss Att={'forw': '0.8300'}; Train Acc=0.498; Test Acc=0.4000; Entropy=0; Entropy_Test=\n",
      "\n",
      "42 2.0633092 2.3076806\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:45epoch=154; Loss Pred=1.9091; Val Loss=2.3077; Val Acc=0.4122; Loss Att={'forw': '0.8290'}; Train Acc=0.496; Test Acc=0.4010; Entropy=0; Entropy_Test=\n",
      "\n",
      "43 2.0633092 2.32975\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:50epoch=155; Loss Pred=1.9093; Val Loss=2.3298; Val Acc=0.4275; Loss Att={'forw': '0.8552'}; Train Acc=0.510; Test Acc=0.3975; Entropy=0; Entropy_Test=\n",
      "\n",
      "44 2.0633092 2.3491457\n",
      "Logged Successfully: \n",
      "2018-07-16 14:04:54epoch=156; Loss Pred=1.9589; Val Loss=2.3491; Val Acc=0.4198; Loss Att={'forw': '0.8331'}; Train Acc=0.498; Test Acc=0.3927; Entropy=0; Entropy_Test=\n",
      "\n",
      "45 2.0633092 2.249987\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:00epoch=157; Loss Pred=1.8478; Val Loss=2.2500; Val Acc=0.4249; Loss Att={'forw': '0.8348'}; Train Acc=0.509; Test Acc=0.3959; Entropy=0; Entropy_Test=\n",
      "\n",
      "46 2.0633092 2.2405827\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:04epoch=158; Loss Pred=1.8698; Val Loss=2.2406; Val Acc=0.4148; Loss Att={'forw': '0.8266'}; Train Acc=0.492; Test Acc=0.3899; Entropy=0; Entropy_Test=\n",
      "\n",
      "47 2.0633092 2.2571852\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:09epoch=159; Loss Pred=1.8294; Val Loss=2.2572; Val Acc=0.4046; Loss Att={'forw': '0.8494'}; Train Acc=0.497; Test Acc=0.3979; Entropy=0; Entropy_Test=\n",
      "\n",
      "48 2.0633092 2.234077\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:14epoch=160; Loss Pred=1.8547; Val Loss=2.2341; Val Acc=0.4097; Loss Att={'forw': '0.8314'}; Train Acc=0.496; Test Acc=0.4006; Entropy=0; Entropy_Test=\n",
      "\n",
      "49 2.0633092 2.1884012\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:19epoch=161; Loss Pred=1.8043; Val Loss=2.1884; Val Acc=0.4224; Loss Att={'forw': '0.8256'}; Train Acc=0.503; Test Acc=0.3986; Entropy=0; Entropy_Test=\n",
      "\n",
      "50 2.0633092 2.196497\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 163\n",
      "Optimization Finished!\n",
      "********** replication  1  **********\n",
      "topic_classification\n",
      "((11228, 300), (11228, 1))\n",
      "0.0\n",
      "(1965, 300) (1965, 1) (393, 300) (393, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: topic_classification\n",
      "    hid: \t\t\t150,\n",
      "    h_hid: \t\t\t300\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t1965/2245, SEQ_LEN: 300\n",
      "Logged Successfully: \n",
      "0 10000000000.0 3.9243321\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:25epoch=0; Loss Pred=3.9208; Val Loss=3.9243; Val Acc=0.0153; Loss Att={'forw': '1.1109'}; Train Acc=0.009; Test Acc=0.0080; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 3.9243321 2.5445802\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:31epoch=1; Loss Pred=2.5297; Val Loss=2.5446; Val Acc=0.3562; Loss Att={'forw': '1.1287'}; Train Acc=0.340; Test Acc=0.3584; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.5445802 2.5317862\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:36epoch=2; Loss Pred=2.5222; Val Loss=2.5318; Val Acc=0.3562; Loss Att={'forw': '0.8574'}; Train Acc=0.341; Test Acc=0.3580; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.5317862 2.4515066\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:41epoch=3; Loss Pred=2.4688; Val Loss=2.4515; Val Acc=0.3562; Loss Att={'forw': '1.0670'}; Train Acc=0.340; Test Acc=0.3581; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4515066 2.4651473\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:45epoch=4; Loss Pred=2.4709; Val Loss=2.4651; Val Acc=0.3562; Loss Att={'forw': '0.7822'}; Train Acc=0.341; Test Acc=0.3552; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4515066 2.4572287\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:50epoch=5; Loss Pred=2.4281; Val Loss=2.4572; Val Acc=0.3537; Loss Att={'forw': '0.8666'}; Train Acc=0.341; Test Acc=0.3557; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.4515066 2.436909\n",
      "Logged Successfully: \n",
      "2018-07-16 14:05:55epoch=6; Loss Pred=2.4293; Val Loss=2.4369; Val Acc=0.3537; Loss Att={'forw': '0.8589'}; Train Acc=0.341; Test Acc=0.3569; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.436909 2.434006\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:00epoch=7; Loss Pred=2.4170; Val Loss=2.4340; Val Acc=0.3537; Loss Att={'forw': '0.9139'}; Train Acc=0.342; Test Acc=0.3559; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.434006 2.4148173\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:04epoch=8; Loss Pred=2.4159; Val Loss=2.4148; Val Acc=0.3562; Loss Att={'forw': '0.8723'}; Train Acc=0.341; Test Acc=0.3563; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4148173 2.4508176\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:09epoch=9; Loss Pred=2.4099; Val Loss=2.4508; Val Acc=0.3537; Loss Att={'forw': '0.8499'}; Train Acc=0.340; Test Acc=0.3553; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4148173 2.4450393\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:14epoch=10; Loss Pred=2.4090; Val Loss=2.4450; Val Acc=0.3562; Loss Att={'forw': '0.8419'}; Train Acc=0.342; Test Acc=0.3560; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.4148173 2.427943\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:19epoch=11; Loss Pred=2.4234; Val Loss=2.4279; Val Acc=0.3410; Loss Att={'forw': '0.8527'}; Train Acc=0.341; Test Acc=0.3549; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.4148173 2.432034\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:23epoch=12; Loss Pred=2.4154; Val Loss=2.4320; Val Acc=0.3537; Loss Att={'forw': '0.8532'}; Train Acc=0.339; Test Acc=0.3595; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.4148173 2.4344084\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:29epoch=13; Loss Pred=2.3980; Val Loss=2.4344; Val Acc=0.3664; Loss Att={'forw': '0.8131'}; Train Acc=0.345; Test Acc=0.3607; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.4148173 2.4265194\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:33epoch=14; Loss Pred=2.4054; Val Loss=2.4265; Val Acc=0.3639; Loss Att={'forw': '0.8142'}; Train Acc=0.341; Test Acc=0.3667; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.4148173 2.3911896\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:39epoch=15; Loss Pred=2.3761; Val Loss=2.3912; Val Acc=0.3690; Loss Att={'forw': '0.8149'}; Train Acc=0.344; Test Acc=0.3674; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3911896 2.3937771\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:43epoch=16; Loss Pred=2.3744; Val Loss=2.3938; Val Acc=0.3639; Loss Att={'forw': '0.8188'}; Train Acc=0.345; Test Acc=0.3649; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3911896 2.3651266\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:48epoch=17; Loss Pred=2.3570; Val Loss=2.3651; Val Acc=0.3562; Loss Att={'forw': '0.8104'}; Train Acc=0.344; Test Acc=0.3613; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3651266 2.3614264\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:52epoch=18; Loss Pred=2.3576; Val Loss=2.3614; Val Acc=0.3486; Loss Att={'forw': '0.8200'}; Train Acc=0.340; Test Acc=0.3575; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3614264 2.3653612\n",
      "Logged Successfully: \n",
      "2018-07-16 14:06:58epoch=19; Loss Pred=2.3288; Val Loss=2.3654; Val Acc=0.3791; Loss Att={'forw': '0.8484'}; Train Acc=0.354; Test Acc=0.3649; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3614264 2.35678\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:02epoch=20; Loss Pred=2.3299; Val Loss=2.3568; Val Acc=0.3893; Loss Att={'forw': '0.8451'}; Train Acc=0.351; Test Acc=0.3587; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.35678 2.317085\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:08epoch=21; Loss Pred=2.3157; Val Loss=2.3171; Val Acc=0.3893; Loss Att={'forw': '0.8715'}; Train Acc=0.362; Test Acc=0.3752; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.317085 2.3238246\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:12epoch=22; Loss Pred=2.3181; Val Loss=2.3238; Val Acc=0.3842; Loss Att={'forw': '0.8810'}; Train Acc=0.373; Test Acc=0.3604; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.317085 2.3474684\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:17epoch=23; Loss Pred=2.3163; Val Loss=2.3475; Val Acc=0.3664; Loss Att={'forw': '0.8514'}; Train Acc=0.353; Test Acc=0.3571; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.317085 2.3516755\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:21epoch=24; Loss Pred=2.3316; Val Loss=2.3517; Val Acc=0.3511; Loss Att={'forw': '0.8357'}; Train Acc=0.347; Test Acc=0.3511; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.317085 2.3667707\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:27epoch=25; Loss Pred=2.2924; Val Loss=2.3668; Val Acc=0.3740; Loss Att={'forw': '0.8806'}; Train Acc=0.385; Test Acc=0.3861; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.317085 2.3473012\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:31epoch=26; Loss Pred=2.3003; Val Loss=2.3473; Val Acc=0.3664; Loss Att={'forw': '0.8507'}; Train Acc=0.369; Test Acc=0.3861; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.317085 2.3645434\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:36epoch=27; Loss Pred=2.3178; Val Loss=2.3645; Val Acc=0.3817; Loss Att={'forw': '0.8670'}; Train Acc=0.353; Test Acc=0.3696; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.317085 2.3607314\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:40epoch=28; Loss Pred=2.3226; Val Loss=2.3607; Val Acc=0.3842; Loss Att={'forw': '0.8513'}; Train Acc=0.348; Test Acc=0.3702; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.317085 2.3816824\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:46epoch=29; Loss Pred=2.3159; Val Loss=2.3817; Val Acc=0.3639; Loss Att={'forw': '0.8137'}; Train Acc=0.374; Test Acc=0.3789; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.317085 2.3863828\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:50epoch=30; Loss Pred=2.3128; Val Loss=2.3864; Val Acc=0.3715; Loss Att={'forw': '0.8192'}; Train Acc=0.374; Test Acc=0.3715; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.317085 2.3003206\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:55epoch=31; Loss Pred=2.2572; Val Loss=2.3003; Val Acc=0.3664; Loss Att={'forw': '0.8544'}; Train Acc=0.394; Test Acc=0.3765; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3003206 2.3011293\n",
      "Logged Successfully: \n",
      "2018-07-16 14:07:59epoch=32; Loss Pred=2.2589; Val Loss=2.3011; Val Acc=0.3664; Loss Att={'forw': '0.8368'}; Train Acc=0.390; Test Acc=0.3881; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3003206 2.3127756\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:05epoch=33; Loss Pred=2.2557; Val Loss=2.3128; Val Acc=0.3791; Loss Att={'forw': '0.8562'}; Train Acc=0.374; Test Acc=0.3822; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3003206 2.330294\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:09epoch=34; Loss Pred=2.2531; Val Loss=2.3303; Val Acc=0.3817; Loss Att={'forw': '0.8453'}; Train Acc=0.378; Test Acc=0.3803; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.3003206 2.2982838\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:14epoch=35; Loss Pred=2.2233; Val Loss=2.2983; Val Acc=0.3791; Loss Att={'forw': '0.8401'}; Train Acc=0.403; Test Acc=0.3867; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2982838 2.3175344\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:19epoch=36; Loss Pred=2.2304; Val Loss=2.3175; Val Acc=0.3715; Loss Att={'forw': '0.8233'}; Train Acc=0.403; Test Acc=0.3922; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2982838 2.2714047\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:24epoch=37; Loss Pred=2.2168; Val Loss=2.2714; Val Acc=0.3664; Loss Att={'forw': '0.8198'}; Train Acc=0.393; Test Acc=0.3889; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2714047 2.2788668\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:28epoch=38; Loss Pred=2.2302; Val Loss=2.2789; Val Acc=0.3639; Loss Att={'forw': '0.8096'}; Train Acc=0.390; Test Acc=0.3866; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2714047 2.3115883\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:34epoch=39; Loss Pred=2.2155; Val Loss=2.3116; Val Acc=0.3842; Loss Att={'forw': '0.8305'}; Train Acc=0.394; Test Acc=0.3885; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2714047 2.3093462\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:38epoch=40; Loss Pred=2.2153; Val Loss=2.3093; Val Acc=0.3868; Loss Att={'forw': '0.8182'}; Train Acc=0.398; Test Acc=0.3884; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2714047 2.3095126\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:43epoch=41; Loss Pred=2.2228; Val Loss=2.3095; Val Acc=0.3639; Loss Att={'forw': '0.8128'}; Train Acc=0.390; Test Acc=0.3812; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2714047 2.2939188\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:47epoch=42; Loss Pred=2.2275; Val Loss=2.2939; Val Acc=0.3842; Loss Att={'forw': '0.8037'}; Train Acc=0.385; Test Acc=0.3840; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2714047 2.2375546\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:53epoch=43; Loss Pred=2.1811; Val Loss=2.2376; Val Acc=0.3944; Loss Att={'forw': '0.8255'}; Train Acc=0.390; Test Acc=0.3933; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2375546 2.2526612\n",
      "Logged Successfully: \n",
      "2018-07-16 14:08:57epoch=44; Loss Pred=2.1913; Val Loss=2.2527; Val Acc=0.4071; Loss Att={'forw': '0.8092'}; Train Acc=0.382; Test Acc=0.3812; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2375546 2.2859674\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:03epoch=45; Loss Pred=2.1952; Val Loss=2.2860; Val Acc=0.3868; Loss Att={'forw': '0.8153'}; Train Acc=0.388; Test Acc=0.3903; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2375546 2.2644827\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:07epoch=46; Loss Pred=2.1939; Val Loss=2.2645; Val Acc=0.3893; Loss Att={'forw': '0.8118'}; Train Acc=0.391; Test Acc=0.3859; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2375546 2.291067\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:12epoch=47; Loss Pred=2.2583; Val Loss=2.2911; Val Acc=0.3919; Loss Att={'forw': '0.7892'}; Train Acc=0.388; Test Acc=0.3832; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2375546 2.2878664\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:16epoch=48; Loss Pred=2.2348; Val Loss=2.2879; Val Acc=0.4020; Loss Att={'forw': '0.7833'}; Train Acc=0.395; Test Acc=0.3920; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2375546 2.2334516\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:22epoch=49; Loss Pred=2.1778; Val Loss=2.2335; Val Acc=0.3893; Loss Att={'forw': '0.8074'}; Train Acc=0.400; Test Acc=0.3925; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2334516 2.2305355\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:26epoch=50; Loss Pred=2.1792; Val Loss=2.2305; Val Acc=0.3715; Loss Att={'forw': '0.7986'}; Train Acc=0.401; Test Acc=0.3928; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2305355 2.1999896\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:31epoch=51; Loss Pred=2.1400; Val Loss=2.2000; Val Acc=0.4148; Loss Att={'forw': '0.8064'}; Train Acc=0.433; Test Acc=0.4211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1999896 2.2122896\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:36epoch=52; Loss Pred=2.1337; Val Loss=2.2123; Val Acc=0.3995; Loss Att={'forw': '0.7973'}; Train Acc=0.435; Test Acc=0.4206; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.1999896 2.2206552\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:41epoch=53; Loss Pred=2.1203; Val Loss=2.2207; Val Acc=0.4224; Loss Att={'forw': '0.8090'}; Train Acc=0.442; Test Acc=0.4353; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.1999896 2.2078738\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:46epoch=54; Loss Pred=2.1232; Val Loss=2.2079; Val Acc=0.4020; Loss Att={'forw': '0.7976'}; Train Acc=0.441; Test Acc=0.4338; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.1999896 2.2098134\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:51epoch=55; Loss Pred=2.1613; Val Loss=2.2098; Val Acc=0.3919; Loss Att={'forw': '0.8179'}; Train Acc=0.415; Test Acc=0.4043; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.1999896 2.2355988\n",
      "Logged Successfully: \n",
      "2018-07-16 14:09:55epoch=56; Loss Pred=2.1416; Val Loss=2.2356; Val Acc=0.3868; Loss Att={'forw': '0.8080'}; Train Acc=0.413; Test Acc=0.4061; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.1999896 2.3356564\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:01epoch=57; Loss Pred=2.2388; Val Loss=2.3357; Val Acc=0.3817; Loss Att={'forw': '0.8532'}; Train Acc=0.377; Test Acc=0.3840; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.1999896 2.2983174\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:05epoch=58; Loss Pred=2.2272; Val Loss=2.2983; Val Acc=0.3817; Loss Att={'forw': '0.8464'}; Train Acc=0.381; Test Acc=0.3799; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.1999896 2.23931\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:10epoch=59; Loss Pred=2.1752; Val Loss=2.2393; Val Acc=0.3969; Loss Att={'forw': '0.8411'}; Train Acc=0.405; Test Acc=0.3771; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.1999896 2.2185063\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:15epoch=60; Loss Pred=2.1748; Val Loss=2.2185; Val Acc=0.4148; Loss Att={'forw': '0.8326'}; Train Acc=0.406; Test Acc=0.3834; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.1999896 2.259095\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:20epoch=61; Loss Pred=2.1453; Val Loss=2.2591; Val Acc=0.4122; Loss Att={'forw': '0.8451'}; Train Acc=0.420; Test Acc=0.4034; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.1999896 2.258533\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:24epoch=62; Loss Pred=2.1429; Val Loss=2.2585; Val Acc=0.4173; Loss Att={'forw': '0.8367'}; Train Acc=0.423; Test Acc=0.4026; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.1999896 2.231198\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:29epoch=63; Loss Pred=2.1308; Val Loss=2.2312; Val Acc=0.4020; Loss Att={'forw': '0.8506'}; Train Acc=0.425; Test Acc=0.4084; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.1999896 2.2116117\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:34epoch=64; Loss Pred=2.1181; Val Loss=2.2116; Val Acc=0.4097; Loss Att={'forw': '0.8397'}; Train Acc=0.430; Test Acc=0.4080; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 2.1999896 2.2562218\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:39epoch=65; Loss Pred=2.0902; Val Loss=2.2562; Val Acc=0.4020; Loss Att={'forw': '0.8356'}; Train Acc=0.443; Test Acc=0.4139; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.1999896 2.243194\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:44epoch=66; Loss Pred=2.0903; Val Loss=2.2432; Val Acc=0.4020; Loss Att={'forw': '0.8272'}; Train Acc=0.444; Test Acc=0.4189; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.1999896 2.2222507\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:49epoch=67; Loss Pred=2.0755; Val Loss=2.2223; Val Acc=0.4224; Loss Att={'forw': '0.8270'}; Train Acc=0.444; Test Acc=0.4201; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.1999896 2.2303064\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:53epoch=68; Loss Pred=2.0853; Val Loss=2.2303; Val Acc=0.4122; Loss Att={'forw': '0.8170'}; Train Acc=0.442; Test Acc=0.4225; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.1999896 2.1768663\n",
      "Logged Successfully: \n",
      "2018-07-16 14:10:58epoch=69; Loss Pred=2.0537; Val Loss=2.1769; Val Acc=0.4020; Loss Att={'forw': '0.8295'}; Train Acc=0.450; Test Acc=0.4275; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1768663 2.188112\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:03epoch=70; Loss Pred=2.0424; Val Loss=2.1881; Val Acc=0.4046; Loss Att={'forw': '0.8244'}; Train Acc=0.460; Test Acc=0.4291; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.1768663 2.1821399\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:08epoch=71; Loss Pred=2.0185; Val Loss=2.1821; Val Acc=0.4071; Loss Att={'forw': '0.8336'}; Train Acc=0.466; Test Acc=0.4320; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.1768663 2.1812766\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:12epoch=72; Loss Pred=2.0129; Val Loss=2.1813; Val Acc=0.4148; Loss Att={'forw': '0.8297'}; Train Acc=0.455; Test Acc=0.4298; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.1768663 2.1848125\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:17epoch=73; Loss Pred=2.0214; Val Loss=2.1848; Val Acc=0.4046; Loss Att={'forw': '0.8246'}; Train Acc=0.468; Test Acc=0.4252; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.1768663 2.199416\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:21epoch=74; Loss Pred=2.0023; Val Loss=2.1994; Val Acc=0.4122; Loss Att={'forw': '0.8160'}; Train Acc=0.461; Test Acc=0.4233; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.1768663 2.2086964\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:27epoch=75; Loss Pred=2.1127; Val Loss=2.2087; Val Acc=0.3944; Loss Att={'forw': '0.8225'}; Train Acc=0.411; Test Acc=0.3957; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.1768663 2.2034624\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:31epoch=76; Loss Pred=2.0920; Val Loss=2.2035; Val Acc=0.4020; Loss Att={'forw': '0.8138'}; Train Acc=0.423; Test Acc=0.3972; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.1768663 2.1750853\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:37epoch=77; Loss Pred=2.0294; Val Loss=2.1751; Val Acc=0.4071; Loss Att={'forw': '0.8562'}; Train Acc=0.458; Test Acc=0.4294; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1750853 2.186402\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:41epoch=78; Loss Pred=2.0191; Val Loss=2.1864; Val Acc=0.4097; Loss Att={'forw': '0.8522'}; Train Acc=0.462; Test Acc=0.4247; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.1750853 2.1819115\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:46epoch=79; Loss Pred=2.0457; Val Loss=2.1819; Val Acc=0.4097; Loss Att={'forw': '0.8416'}; Train Acc=0.454; Test Acc=0.4107; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.1750853 2.1946154\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:50epoch=80; Loss Pred=2.0683; Val Loss=2.1946; Val Acc=0.4071; Loss Att={'forw': '0.8277'}; Train Acc=0.429; Test Acc=0.4147; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.1750853 2.1500318\n",
      "Logged Successfully: \n",
      "2018-07-16 14:11:56epoch=81; Loss Pred=2.0010; Val Loss=2.1500; Val Acc=0.4097; Loss Att={'forw': '0.8165'}; Train Acc=0.460; Test Acc=0.4219; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1500318 2.1487436\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:00epoch=82; Loss Pred=2.0013; Val Loss=2.1487; Val Acc=0.4173; Loss Att={'forw': '0.8188'}; Train Acc=0.459; Test Acc=0.4204; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1487436 2.3063998\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:05epoch=83; Loss Pred=2.1976; Val Loss=2.3064; Val Acc=0.3817; Loss Att={'forw': '0.8212'}; Train Acc=0.381; Test Acc=0.3712; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.1487436 2.3188453\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:09epoch=84; Loss Pred=2.2131; Val Loss=2.3188; Val Acc=0.3639; Loss Att={'forw': '0.8133'}; Train Acc=0.369; Test Acc=0.3653; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.1487436 2.2418847\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:15epoch=85; Loss Pred=2.0877; Val Loss=2.2419; Val Acc=0.4020; Loss Att={'forw': '0.8733'}; Train Acc=0.436; Test Acc=0.4076; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.1487436 2.2745497\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:19epoch=86; Loss Pred=2.0850; Val Loss=2.2745; Val Acc=0.4122; Loss Att={'forw': '0.8556'}; Train Acc=0.435; Test Acc=0.4061; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.1487436 2.229677\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:25epoch=87; Loss Pred=2.0789; Val Loss=2.2297; Val Acc=0.4020; Loss Att={'forw': '0.8451'}; Train Acc=0.451; Test Acc=0.4181; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.1487436 2.213513\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:29epoch=88; Loss Pred=2.0836; Val Loss=2.2135; Val Acc=0.4224; Loss Att={'forw': '0.8407'}; Train Acc=0.445; Test Acc=0.4183; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.1487436 2.1464128\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:34epoch=89; Loss Pred=2.0038; Val Loss=2.1464; Val Acc=0.4249; Loss Att={'forw': '0.8373'}; Train Acc=0.448; Test Acc=0.4010; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1464128 2.1384\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:39epoch=90; Loss Pred=2.0046; Val Loss=2.1384; Val Acc=0.4148; Loss Att={'forw': '0.8287'}; Train Acc=0.436; Test Acc=0.3955; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1384 2.0997186\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:44epoch=91; Loss Pred=1.9385; Val Loss=2.0997; Val Acc=0.4275; Loss Att={'forw': '0.8300'}; Train Acc=0.472; Test Acc=0.4355; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.0997186 2.108471\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:48epoch=92; Loss Pred=1.9463; Val Loss=2.1085; Val Acc=0.4275; Loss Att={'forw': '0.8160'}; Train Acc=0.468; Test Acc=0.4429; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.0997186 2.0660183\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:54epoch=93; Loss Pred=1.9051; Val Loss=2.0660; Val Acc=0.4351; Loss Att={'forw': '0.8184'}; Train Acc=0.479; Test Acc=0.4373; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.0660183 2.057245\n",
      "Logged Successfully: \n",
      "2018-07-16 14:12:58epoch=94; Loss Pred=1.8970; Val Loss=2.0572; Val Acc=0.4402; Loss Att={'forw': '0.8113'}; Train Acc=0.487; Test Acc=0.4374; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.057245 2.533289\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:04epoch=95; Loss Pred=2.4585; Val Loss=2.5333; Val Acc=0.4097; Loss Att={'forw': '0.8290'}; Train Acc=0.421; Test Acc=0.4003; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.057245 2.513589\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:08epoch=96; Loss Pred=2.4532; Val Loss=2.5136; Val Acc=0.4148; Loss Att={'forw': '0.8135'}; Train Acc=0.427; Test Acc=0.4067; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.057245 2.3034482\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:13epoch=97; Loss Pred=2.1892; Val Loss=2.3034; Val Acc=0.3461; Loss Att={'forw': '0.8572'}; Train Acc=0.378; Test Acc=0.3668; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.057245 2.245281\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:18epoch=98; Loss Pred=2.1648; Val Loss=2.2453; Val Acc=0.3715; Loss Att={'forw': '0.8342'}; Train Acc=0.386; Test Acc=0.3681; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.057245 2.3871992\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:23epoch=99; Loss Pred=2.2634; Val Loss=2.3872; Val Acc=0.3944; Loss Att={'forw': '0.8656'}; Train Acc=0.426; Test Acc=0.3902; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.057245 2.3655732\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:27epoch=100; Loss Pred=2.2624; Val Loss=2.3656; Val Acc=0.3995; Loss Att={'forw': '0.8677'}; Train Acc=0.434; Test Acc=0.4005; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.057245 2.3230932\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:33epoch=101; Loss Pred=2.1994; Val Loss=2.3231; Val Acc=0.3868; Loss Att={'forw': '0.8669'}; Train Acc=0.427; Test Acc=0.3906; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.057245 2.3362513\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:37epoch=102; Loss Pred=2.2122; Val Loss=2.3363; Val Acc=0.3791; Loss Att={'forw': '0.8357'}; Train Acc=0.427; Test Acc=0.3925; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.057245 2.2779624\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:43epoch=103; Loss Pred=2.1486; Val Loss=2.2780; Val Acc=0.3995; Loss Att={'forw': '0.8448'}; Train Acc=0.429; Test Acc=0.3939; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.057245 2.2884688\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:47epoch=104; Loss Pred=2.1604; Val Loss=2.2885; Val Acc=0.3995; Loss Att={'forw': '0.8314'}; Train Acc=0.427; Test Acc=0.4013; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.057245 2.2659914\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:52epoch=105; Loss Pred=2.1200; Val Loss=2.2660; Val Acc=0.4046; Loss Att={'forw': '0.8367'}; Train Acc=0.437; Test Acc=0.4102; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.057245 2.2853131\n",
      "Logged Successfully: \n",
      "2018-07-16 14:13:57epoch=106; Loss Pred=2.1246; Val Loss=2.2853; Val Acc=0.4097; Loss Att={'forw': '0.8266'}; Train Acc=0.443; Test Acc=0.4071; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.057245 2.2747056\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:02epoch=107; Loss Pred=2.0801; Val Loss=2.2747; Val Acc=0.4097; Loss Att={'forw': '0.8336'}; Train Acc=0.444; Test Acc=0.3967; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.057245 2.2761168\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:06epoch=108; Loss Pred=2.0974; Val Loss=2.2761; Val Acc=0.3995; Loss Att={'forw': '0.8245'}; Train Acc=0.442; Test Acc=0.3949; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.057245 2.2522125\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:12epoch=109; Loss Pred=2.0347; Val Loss=2.2522; Val Acc=0.4046; Loss Att={'forw': '0.8319'}; Train Acc=0.459; Test Acc=0.4137; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.057245 2.2479649\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:16epoch=110; Loss Pred=2.0276; Val Loss=2.2480; Val Acc=0.4020; Loss Att={'forw': '0.8238'}; Train Acc=0.463; Test Acc=0.4105; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.057245 2.251813\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:21epoch=111; Loss Pred=2.0013; Val Loss=2.2518; Val Acc=0.4148; Loss Att={'forw': '0.8296'}; Train Acc=0.468; Test Acc=0.4127; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.057245 2.2178543\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:25epoch=112; Loss Pred=2.0066; Val Loss=2.2179; Val Acc=0.4173; Loss Att={'forw': '0.8215'}; Train Acc=0.465; Test Acc=0.4147; Entropy=0; Entropy_Test=\n",
      "\n",
      "18 2.057245 2.176841\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:31epoch=113; Loss Pred=1.9769; Val Loss=2.1768; Val Acc=0.4071; Loss Att={'forw': '0.8203'}; Train Acc=0.466; Test Acc=0.4236; Entropy=0; Entropy_Test=\n",
      "\n",
      "19 2.057245 2.2012148\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:35epoch=114; Loss Pred=1.9898; Val Loss=2.2012; Val Acc=0.4198; Loss Att={'forw': '0.8177'}; Train Acc=0.468; Test Acc=0.4219; Entropy=0; Entropy_Test=\n",
      "\n",
      "20 2.057245 2.1758676\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:40epoch=115; Loss Pred=1.9838; Val Loss=2.1759; Val Acc=0.4148; Loss Att={'forw': '0.8146'}; Train Acc=0.467; Test Acc=0.4153; Entropy=0; Entropy_Test=\n",
      "\n",
      "21 2.057245 2.1784906\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:45epoch=116; Loss Pred=1.9709; Val Loss=2.1785; Val Acc=0.4122; Loss Att={'forw': '0.8053'}; Train Acc=0.475; Test Acc=0.4155; Entropy=0; Entropy_Test=\n",
      "\n",
      "22 2.057245 2.1501236\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:50epoch=117; Loss Pred=1.9440; Val Loss=2.1501; Val Acc=0.4249; Loss Att={'forw': '0.8176'}; Train Acc=0.490; Test Acc=0.4178; Entropy=0; Entropy_Test=\n",
      "\n",
      "23 2.057245 2.1304836\n",
      "Logged Successfully: \n",
      "2018-07-16 14:14:54epoch=118; Loss Pred=1.9577; Val Loss=2.1305; Val Acc=0.4453; Loss Att={'forw': '0.8104'}; Train Acc=0.480; Test Acc=0.4165; Entropy=0; Entropy_Test=\n",
      "\n",
      "24 2.057245 2.1659696\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:00epoch=119; Loss Pred=1.9161; Val Loss=2.1660; Val Acc=0.4326; Loss Att={'forw': '0.8123'}; Train Acc=0.499; Test Acc=0.4195; Entropy=0; Entropy_Test=\n",
      "\n",
      "25 2.057245 2.1686869\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:04epoch=120; Loss Pred=1.9096; Val Loss=2.1687; Val Acc=0.4377; Loss Att={'forw': '0.8027'}; Train Acc=0.493; Test Acc=0.4192; Entropy=0; Entropy_Test=\n",
      "\n",
      "26 2.057245 2.1528\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:09epoch=121; Loss Pred=1.9107; Val Loss=2.1528; Val Acc=0.4275; Loss Att={'forw': '0.8008'}; Train Acc=0.499; Test Acc=0.4169; Entropy=0; Entropy_Test=\n",
      "\n",
      "27 2.057245 2.1809487\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:13epoch=122; Loss Pred=1.8960; Val Loss=2.1809; Val Acc=0.4198; Loss Att={'forw': '0.7944'}; Train Acc=0.502; Test Acc=0.4220; Entropy=0; Entropy_Test=\n",
      "\n",
      "28 2.057245 2.1320245\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:19epoch=123; Loss Pred=1.8524; Val Loss=2.1320; Val Acc=0.4402; Loss Att={'forw': '0.7991'}; Train Acc=0.510; Test Acc=0.4304; Entropy=0; Entropy_Test=\n",
      "\n",
      "29 2.057245 2.1557057\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:23epoch=124; Loss Pred=1.8561; Val Loss=2.1557; Val Acc=0.4275; Loss Att={'forw': '0.7935'}; Train Acc=0.513; Test Acc=0.4295; Entropy=0; Entropy_Test=\n",
      "\n",
      "30 2.057245 2.2020106\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:28epoch=125; Loss Pred=1.9136; Val Loss=2.2020; Val Acc=0.4046; Loss Att={'forw': '0.8005'}; Train Acc=0.488; Test Acc=0.4101; Entropy=0; Entropy_Test=\n",
      "\n",
      "31 2.057245 2.2400854\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:33epoch=126; Loss Pred=1.8978; Val Loss=2.2401; Val Acc=0.4148; Loss Att={'forw': '0.7962'}; Train Acc=0.486; Test Acc=0.4080; Entropy=0; Entropy_Test=\n",
      "\n",
      "32 2.057245 2.1598375\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:38epoch=127; Loss Pred=1.8294; Val Loss=2.1598; Val Acc=0.4173; Loss Att={'forw': '0.8007'}; Train Acc=0.517; Test Acc=0.4320; Entropy=0; Entropy_Test=\n",
      "\n",
      "33 2.057245 2.1729698\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:42epoch=128; Loss Pred=1.8333; Val Loss=2.1730; Val Acc=0.4198; Loss Att={'forw': '0.7944'}; Train Acc=0.515; Test Acc=0.4225; Entropy=0; Entropy_Test=\n",
      "\n",
      "34 2.057245 2.134009\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:47epoch=129; Loss Pred=1.8299; Val Loss=2.1340; Val Acc=0.4377; Loss Att={'forw': '0.7983'}; Train Acc=0.505; Test Acc=0.4216; Entropy=0; Entropy_Test=\n",
      "\n",
      "35 2.057245 2.1530237\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:52epoch=130; Loss Pred=1.8456; Val Loss=2.1530; Val Acc=0.4351; Loss Att={'forw': '0.7932'}; Train Acc=0.508; Test Acc=0.4197; Entropy=0; Entropy_Test=\n",
      "\n",
      "36 2.057245 2.153579\n",
      "Logged Successfully: \n",
      "2018-07-16 14:15:57epoch=131; Loss Pred=1.7855; Val Loss=2.1536; Val Acc=0.4224; Loss Att={'forw': '0.8011'}; Train Acc=0.516; Test Acc=0.4293; Entropy=0; Entropy_Test=\n",
      "\n",
      "37 2.057245 2.1566262\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:01epoch=132; Loss Pred=1.7836; Val Loss=2.1566; Val Acc=0.4351; Loss Att={'forw': '0.8010'}; Train Acc=0.525; Test Acc=0.4315; Entropy=0; Entropy_Test=\n",
      "\n",
      "38 2.057245 2.1774907\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:07epoch=133; Loss Pred=1.7574; Val Loss=2.1775; Val Acc=0.4453; Loss Att={'forw': '0.7991'}; Train Acc=0.534; Test Acc=0.4361; Entropy=0; Entropy_Test=\n",
      "\n",
      "39 2.057245 2.174867\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:11epoch=134; Loss Pred=1.7528; Val Loss=2.1749; Val Acc=0.4555; Loss Att={'forw': '0.7956'}; Train Acc=0.530; Test Acc=0.4317; Entropy=0; Entropy_Test=\n",
      "\n",
      "40 2.057245 2.2208793\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:17epoch=135; Loss Pred=1.8997; Val Loss=2.2209; Val Acc=0.4020; Loss Att={'forw': '0.7921'}; Train Acc=0.495; Test Acc=0.3948; Entropy=0; Entropy_Test=\n",
      "\n",
      "41 2.057245 2.222052\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:21epoch=136; Loss Pred=1.8654; Val Loss=2.2221; Val Acc=0.4173; Loss Att={'forw': '0.7919'}; Train Acc=0.506; Test Acc=0.3953; Entropy=0; Entropy_Test=\n",
      "\n",
      "42 2.057245 2.211409\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:26epoch=137; Loss Pred=1.7494; Val Loss=2.2114; Val Acc=0.4122; Loss Att={'forw': '0.8002'}; Train Acc=0.534; Test Acc=0.4187; Entropy=0; Entropy_Test=\n",
      "\n",
      "43 2.057245 2.2021985\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:31epoch=138; Loss Pred=1.7498; Val Loss=2.2022; Val Acc=0.4351; Loss Att={'forw': '0.7972'}; Train Acc=0.535; Test Acc=0.4140; Entropy=0; Entropy_Test=\n",
      "\n",
      "44 2.057245 2.231952\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:36epoch=139; Loss Pred=1.8084; Val Loss=2.2320; Val Acc=0.4020; Loss Att={'forw': '0.8126'}; Train Acc=0.516; Test Acc=0.4029; Entropy=0; Entropy_Test=\n",
      "\n",
      "45 2.057245 2.2332227\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:40epoch=140; Loss Pred=1.7904; Val Loss=2.2332; Val Acc=0.4402; Loss Att={'forw': '0.8011'}; Train Acc=0.521; Test Acc=0.4095; Entropy=0; Entropy_Test=\n",
      "\n",
      "46 2.057245 2.223024\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:46epoch=141; Loss Pred=1.7942; Val Loss=2.2230; Val Acc=0.4097; Loss Att={'forw': '0.8045'}; Train Acc=0.522; Test Acc=0.4100; Entropy=0; Entropy_Test=\n",
      "\n",
      "47 2.057245 2.2143214\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:50epoch=142; Loss Pred=1.7965; Val Loss=2.2143; Val Acc=0.4122; Loss Att={'forw': '0.7979'}; Train Acc=0.516; Test Acc=0.4102; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 2.057245 2.1930761\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:55epoch=143; Loss Pred=1.7236; Val Loss=2.1931; Val Acc=0.4198; Loss Att={'forw': '0.8005'}; Train Acc=0.530; Test Acc=0.4078; Entropy=0; Entropy_Test=\n",
      "\n",
      "49 2.057245 2.178505\n",
      "Logged Successfully: \n",
      "2018-07-16 14:16:59epoch=144; Loss Pred=1.7337; Val Loss=2.1785; Val Acc=0.4224; Loss Att={'forw': '0.8076'}; Train Acc=0.539; Test Acc=0.4091; Entropy=0; Entropy_Test=\n",
      "\n",
      "50 2.057245 2.200171\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 146\n",
      "Optimization Finished!\n",
      "********** replication  2  **********\n",
      "topic_classification\n",
      "((11228, 300), (11228, 1))\n",
      "0.0\n",
      "(1965, 300) (1965, 1) (393, 300) (393, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: topic_classification\n",
      "    hid: \t\t\t150,\n",
      "    h_hid: \t\t\t300\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t1965/2245, SEQ_LEN: 300\n",
      "Logged Successfully: \n",
      "0 10000000000.0 3.8157003\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:07epoch=0; Loss Pred=3.8184; Val Loss=3.8157; Val Acc=0.0280; Loss Att={'forw': '1.1199'}; Train Acc=0.027; Test Acc=0.0283; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 3.8157003 2.7386954\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:13epoch=1; Loss Pred=2.5466; Val Loss=2.7387; Val Acc=0.3791; Loss Att={'forw': '1.0333'}; Train Acc=0.360; Test Acc=0.3559; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.7386954 2.7268505\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:17epoch=2; Loss Pred=2.5305; Val Loss=2.7269; Val Acc=0.3791; Loss Att={'forw': '0.7897'}; Train Acc=0.360; Test Acc=0.3558; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.7268505 2.663352\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:22epoch=3; Loss Pred=2.4717; Val Loss=2.6634; Val Acc=0.3740; Loss Att={'forw': '0.9705'}; Train Acc=0.361; Test Acc=0.3544; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.663352 2.6680548\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:27epoch=4; Loss Pred=2.4718; Val Loss=2.6681; Val Acc=0.3690; Loss Att={'forw': '0.8026'}; Train Acc=0.361; Test Acc=0.3574; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.663352 2.582214\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:32epoch=5; Loss Pred=2.4187; Val Loss=2.5822; Val Acc=0.3715; Loss Att={'forw': '0.9305'}; Train Acc=0.360; Test Acc=0.3584; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.582214 2.5816813\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:36epoch=6; Loss Pred=2.4126; Val Loss=2.5817; Val Acc=0.3740; Loss Att={'forw': '0.7703'}; Train Acc=0.360; Test Acc=0.3552; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.5816813 2.531227\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:42epoch=7; Loss Pred=2.4139; Val Loss=2.5312; Val Acc=0.3791; Loss Att={'forw': '0.9598'}; Train Acc=0.359; Test Acc=0.3584; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.531227 2.5684714\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:46epoch=8; Loss Pred=2.4101; Val Loss=2.5685; Val Acc=0.3791; Loss Att={'forw': '0.8967'}; Train Acc=0.360; Test Acc=0.3562; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.531227 2.5695183\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:51epoch=9; Loss Pred=2.3965; Val Loss=2.5695; Val Acc=0.3715; Loss Att={'forw': '0.9879'}; Train Acc=0.364; Test Acc=0.3629; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.531227 2.5793893\n",
      "Logged Successfully: \n",
      "2018-07-16 14:17:55epoch=10; Loss Pred=2.3986; Val Loss=2.5794; Val Acc=0.3740; Loss Att={'forw': '0.9067'}; Train Acc=0.363; Test Acc=0.3640; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.531227 2.5184343\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:01epoch=11; Loss Pred=2.3823; Val Loss=2.5184; Val Acc=0.3715; Loss Att={'forw': '0.8978'}; Train Acc=0.363; Test Acc=0.3576; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.5184343 2.537764\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:05epoch=12; Loss Pred=2.3910; Val Loss=2.5378; Val Acc=0.3715; Loss Att={'forw': '0.9091'}; Train Acc=0.361; Test Acc=0.3592; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.5184343 2.5233855\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:10epoch=13; Loss Pred=2.3882; Val Loss=2.5234; Val Acc=0.3690; Loss Att={'forw': '0.9284'}; Train Acc=0.361; Test Acc=0.3661; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.5184343 2.5486207\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:14epoch=14; Loss Pred=2.3838; Val Loss=2.5486; Val Acc=0.3690; Loss Att={'forw': '0.8832'}; Train Acc=0.364; Test Acc=0.3632; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.5184343 2.531437\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:20epoch=15; Loss Pred=2.3640; Val Loss=2.5314; Val Acc=0.3715; Loss Att={'forw': '0.8327'}; Train Acc=0.364; Test Acc=0.3636; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.5184343 2.5186617\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:24epoch=16; Loss Pred=2.3629; Val Loss=2.5187; Val Acc=0.3791; Loss Att={'forw': '0.8240'}; Train Acc=0.364; Test Acc=0.3608; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.5184343 2.4718215\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:29epoch=17; Loss Pred=2.3160; Val Loss=2.4718; Val Acc=0.3613; Loss Att={'forw': '0.9065'}; Train Acc=0.365; Test Acc=0.3623; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4718215 2.4681568\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:33epoch=18; Loss Pred=2.3244; Val Loss=2.4682; Val Acc=0.3791; Loss Att={'forw': '0.8811'}; Train Acc=0.363; Test Acc=0.3727; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4681568 2.5163677\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:39epoch=19; Loss Pred=2.3130; Val Loss=2.5164; Val Acc=0.3282; Loss Att={'forw': '0.8260'}; Train Acc=0.383; Test Acc=0.3791; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4681568 2.5093212\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:43epoch=20; Loss Pred=2.3179; Val Loss=2.5093; Val Acc=0.3282; Loss Att={'forw': '0.8193'}; Train Acc=0.375; Test Acc=0.3705; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.4681568 2.4890575\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:49epoch=21; Loss Pred=2.3096; Val Loss=2.4891; Val Acc=0.3181; Loss Att={'forw': '0.8046'}; Train Acc=0.358; Test Acc=0.3652; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.4681568 2.4977696\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:53epoch=22; Loss Pred=2.3116; Val Loss=2.4978; Val Acc=0.3435; Loss Att={'forw': '0.7642'}; Train Acc=0.370; Test Acc=0.3680; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.4681568 2.4282348\n",
      "Logged Successfully: \n",
      "2018-07-16 14:18:58epoch=23; Loss Pred=2.2745; Val Loss=2.4282; Val Acc=0.3740; Loss Att={'forw': '0.8138'}; Train Acc=0.382; Test Acc=0.3807; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4282348 2.4358032\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:02epoch=24; Loss Pred=2.2721; Val Loss=2.4358; Val Acc=0.3715; Loss Att={'forw': '0.8136'}; Train Acc=0.389; Test Acc=0.3751; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4282348 2.4295685\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:08epoch=25; Loss Pred=2.2512; Val Loss=2.4296; Val Acc=0.3868; Loss Att={'forw': '0.8112'}; Train Acc=0.390; Test Acc=0.3748; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.4282348 2.438349\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:12epoch=26; Loss Pred=2.2528; Val Loss=2.4383; Val Acc=0.3842; Loss Att={'forw': '0.7908'}; Train Acc=0.372; Test Acc=0.3791; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.4282348 2.4137807\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:17epoch=27; Loss Pred=2.2522; Val Loss=2.4138; Val Acc=0.3664; Loss Att={'forw': '0.8164'}; Train Acc=0.394; Test Acc=0.3802; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4137807 2.4168522\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:22epoch=28; Loss Pred=2.2514; Val Loss=2.4169; Val Acc=0.3664; Loss Att={'forw': '0.8236'}; Train Acc=0.399; Test Acc=0.3802; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4137807 2.3986452\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:27epoch=29; Loss Pred=2.2230; Val Loss=2.3986; Val Acc=0.3817; Loss Att={'forw': '0.8445'}; Train Acc=0.389; Test Acc=0.3807; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3986452 2.4017208\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:31epoch=30; Loss Pred=2.2302; Val Loss=2.4017; Val Acc=0.3919; Loss Att={'forw': '0.8059'}; Train Acc=0.386; Test Acc=0.3831; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3986452 2.405201\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:37epoch=31; Loss Pred=2.2130; Val Loss=2.4052; Val Acc=0.3817; Loss Att={'forw': '0.8404'}; Train Acc=0.393; Test Acc=0.3902; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3986452 2.4150531\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:41epoch=32; Loss Pred=2.2220; Val Loss=2.4151; Val Acc=0.3842; Loss Att={'forw': '0.7946'}; Train Acc=0.395; Test Acc=0.3859; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.3986452 2.4302802\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:47epoch=33; Loss Pred=2.2489; Val Loss=2.4303; Val Acc=0.3511; Loss Att={'forw': '0.8493'}; Train Acc=0.381; Test Acc=0.3806; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.3986452 2.4294178\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:51epoch=34; Loss Pred=2.2435; Val Loss=2.4294; Val Acc=0.3461; Loss Att={'forw': '0.8590'}; Train Acc=0.387; Test Acc=0.3745; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.3986452 2.4147847\n",
      "Logged Successfully: \n",
      "2018-07-16 14:19:56epoch=35; Loss Pred=2.2145; Val Loss=2.4148; Val Acc=0.3893; Loss Att={'forw': '0.8533'}; Train Acc=0.386; Test Acc=0.3683; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.3986452 2.429225\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:00epoch=36; Loss Pred=2.2049; Val Loss=2.4292; Val Acc=0.3740; Loss Att={'forw': '0.8524'}; Train Acc=0.385; Test Acc=0.3777; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.3986452 2.357992\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:06epoch=37; Loss Pred=2.1876; Val Loss=2.3580; Val Acc=0.3817; Loss Att={'forw': '0.8359'}; Train Acc=0.400; Test Acc=0.3894; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.357992 2.3954487\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:10epoch=38; Loss Pred=2.1913; Val Loss=2.3954; Val Acc=0.3791; Loss Att={'forw': '0.8485'}; Train Acc=0.395; Test Acc=0.3885; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.357992 2.35741\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:15epoch=39; Loss Pred=2.1890; Val Loss=2.3574; Val Acc=0.3969; Loss Att={'forw': '0.8273'}; Train Acc=0.387; Test Acc=0.3787; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.35741 2.3761396\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:20epoch=40; Loss Pred=2.1967; Val Loss=2.3761; Val Acc=0.3893; Loss Att={'forw': '0.8496'}; Train Acc=0.391; Test Acc=0.3742; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.35741 2.405035\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:25epoch=41; Loss Pred=2.2258; Val Loss=2.4050; Val Acc=0.3410; Loss Att={'forw': '0.8551'}; Train Acc=0.370; Test Acc=0.3687; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.35741 2.405147\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:29epoch=42; Loss Pred=2.2169; Val Loss=2.4051; Val Acc=0.3461; Loss Att={'forw': '0.8332'}; Train Acc=0.380; Test Acc=0.3658; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.35741 2.3767345\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:35epoch=43; Loss Pred=2.1567; Val Loss=2.3767; Val Acc=0.3868; Loss Att={'forw': '0.8446'}; Train Acc=0.413; Test Acc=0.3962; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.35741 2.3801339\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:39epoch=44; Loss Pred=2.1673; Val Loss=2.3801; Val Acc=0.3919; Loss Att={'forw': '0.8357'}; Train Acc=0.405; Test Acc=0.3903; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.35741 2.3602993\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:44epoch=45; Loss Pred=2.1544; Val Loss=2.3603; Val Acc=0.4020; Loss Att={'forw': '0.8480'}; Train Acc=0.407; Test Acc=0.3924; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.35741 2.3795037\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:49epoch=46; Loss Pred=2.1622; Val Loss=2.3795; Val Acc=0.3995; Loss Att={'forw': '0.8448'}; Train Acc=0.416; Test Acc=0.3943; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.35741 2.3379707\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:54epoch=47; Loss Pred=2.1682; Val Loss=2.3380; Val Acc=0.3893; Loss Att={'forw': '0.8237'}; Train Acc=0.410; Test Acc=0.3820; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3379707 2.3314955\n",
      "Logged Successfully: \n",
      "2018-07-16 14:20:58epoch=48; Loss Pred=2.1633; Val Loss=2.3315; Val Acc=0.3740; Loss Att={'forw': '0.8195'}; Train Acc=0.408; Test Acc=0.3850; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3314955 2.3295078\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:04epoch=49; Loss Pred=2.1515; Val Loss=2.3295; Val Acc=0.3613; Loss Att={'forw': '0.8242'}; Train Acc=0.403; Test Acc=0.3887; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3295078 2.3555994\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:08epoch=50; Loss Pred=2.1454; Val Loss=2.3556; Val Acc=0.3715; Loss Att={'forw': '0.8199'}; Train Acc=0.405; Test Acc=0.3832; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3295078 2.349691\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:14epoch=51; Loss Pred=2.1453; Val Loss=2.3497; Val Acc=0.3868; Loss Att={'forw': '0.8128'}; Train Acc=0.400; Test Acc=0.3811; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3295078 2.3276744\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:18epoch=52; Loss Pred=2.1452; Val Loss=2.3277; Val Acc=0.3893; Loss Att={'forw': '0.8193'}; Train Acc=0.407; Test Acc=0.3815; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3276744 2.363998\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:23epoch=53; Loss Pred=2.1678; Val Loss=2.3640; Val Acc=0.3791; Loss Att={'forw': '0.8435'}; Train Acc=0.399; Test Acc=0.3794; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3276744 2.3617454\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:27epoch=54; Loss Pred=2.1548; Val Loss=2.3617; Val Acc=0.3919; Loss Att={'forw': '0.8309'}; Train Acc=0.413; Test Acc=0.3809; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3276744 2.2962568\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:32epoch=55; Loss Pred=2.1040; Val Loss=2.2963; Val Acc=0.3842; Loss Att={'forw': '0.8396'}; Train Acc=0.403; Test Acc=0.3848; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2962568 2.2874224\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:37epoch=56; Loss Pred=2.1089; Val Loss=2.2874; Val Acc=0.4020; Loss Att={'forw': '0.8281'}; Train Acc=0.404; Test Acc=0.3777; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2874224 2.3790195\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:42epoch=57; Loss Pred=2.1590; Val Loss=2.3790; Val Acc=0.3740; Loss Att={'forw': '0.8274'}; Train Acc=0.401; Test Acc=0.3826; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2874224 2.3643432\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:46epoch=58; Loss Pred=2.1479; Val Loss=2.3643; Val Acc=0.3842; Loss Att={'forw': '0.8171'}; Train Acc=0.405; Test Acc=0.3835; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2874224 2.3120334\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:51epoch=59; Loss Pred=2.0927; Val Loss=2.3120; Val Acc=0.3995; Loss Att={'forw': '0.8262'}; Train Acc=0.423; Test Acc=0.3902; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2874224 2.3182826\n",
      "Logged Successfully: \n",
      "2018-07-16 14:21:55epoch=60; Loss Pred=2.0889; Val Loss=2.3183; Val Acc=0.4020; Loss Att={'forw': '0.8198'}; Train Acc=0.421; Test Acc=0.3905; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2874224 2.284444\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:01epoch=61; Loss Pred=2.0734; Val Loss=2.2844; Val Acc=0.3868; Loss Att={'forw': '0.8308'}; Train Acc=0.427; Test Acc=0.3909; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.284444 2.286531\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:05epoch=62; Loss Pred=2.0698; Val Loss=2.2865; Val Acc=0.3969; Loss Att={'forw': '0.8163'}; Train Acc=0.417; Test Acc=0.3938; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.284444 2.279893\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:11epoch=63; Loss Pred=2.0612; Val Loss=2.2799; Val Acc=0.3817; Loss Att={'forw': '0.8333'}; Train Acc=0.429; Test Acc=0.3978; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.279893 2.2802863\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:15epoch=64; Loss Pred=2.0606; Val Loss=2.2803; Val Acc=0.3893; Loss Att={'forw': '0.8222'}; Train Acc=0.423; Test Acc=0.4004; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.279893 2.2744553\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:20epoch=65; Loss Pred=2.0509; Val Loss=2.2745; Val Acc=0.4198; Loss Att={'forw': '0.8255'}; Train Acc=0.427; Test Acc=0.3829; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2744553 2.2771885\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:25epoch=66; Loss Pred=2.0531; Val Loss=2.2772; Val Acc=0.3969; Loss Att={'forw': '0.8122'}; Train Acc=0.424; Test Acc=0.3796; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2744553 2.3071344\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:30epoch=67; Loss Pred=2.0788; Val Loss=2.3071; Val Acc=0.3639; Loss Att={'forw': '0.8157'}; Train Acc=0.410; Test Acc=0.3731; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2744553 2.3199546\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:34epoch=68; Loss Pred=2.0867; Val Loss=2.3200; Val Acc=0.3588; Loss Att={'forw': '0.8086'}; Train Acc=0.411; Test Acc=0.3730; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2744553 2.3740325\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:40epoch=69; Loss Pred=2.1142; Val Loss=2.3740; Val Acc=0.3384; Loss Att={'forw': '0.8051'}; Train Acc=0.401; Test Acc=0.3616; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2744553 2.3175857\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:44epoch=70; Loss Pred=2.0816; Val Loss=2.3176; Val Acc=0.3639; Loss Att={'forw': '0.7993'}; Train Acc=0.422; Test Acc=0.3779; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2744553 2.3394234\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:49epoch=71; Loss Pred=2.0873; Val Loss=2.3394; Val Acc=0.3690; Loss Att={'forw': '0.8206'}; Train Acc=0.419; Test Acc=0.3694; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.2744553 2.3439202\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:53epoch=72; Loss Pred=2.0999; Val Loss=2.3439; Val Acc=0.3690; Loss Att={'forw': '0.8111'}; Train Acc=0.405; Test Acc=0.3626; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.2744553 2.3799462\n",
      "Logged Successfully: \n",
      "2018-07-16 14:22:59epoch=73; Loss Pred=2.1256; Val Loss=2.3799; Val Acc=0.3359; Loss Att={'forw': '0.8419'}; Train Acc=0.403; Test Acc=0.3657; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.2744553 2.356538\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:03epoch=74; Loss Pred=2.1222; Val Loss=2.3565; Val Acc=0.3461; Loss Att={'forw': '0.8441'}; Train Acc=0.400; Test Acc=0.3679; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.2744553 2.3653414\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:08epoch=75; Loss Pred=2.0990; Val Loss=2.3653; Val Acc=0.3664; Loss Att={'forw': '0.8348'}; Train Acc=0.403; Test Acc=0.3826; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.2744553 2.3756049\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:12epoch=76; Loss Pred=2.1232; Val Loss=2.3756; Val Acc=0.3791; Loss Att={'forw': '0.8393'}; Train Acc=0.398; Test Acc=0.3787; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.2744553 2.2810445\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:18epoch=77; Loss Pred=2.0409; Val Loss=2.2810; Val Acc=0.4224; Loss Att={'forw': '0.8436'}; Train Acc=0.430; Test Acc=0.4002; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.2744553 2.2851448\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:22epoch=78; Loss Pred=2.0308; Val Loss=2.2851; Val Acc=0.4020; Loss Att={'forw': '0.8328'}; Train Acc=0.433; Test Acc=0.3967; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.2744553 2.3650336\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:28epoch=79; Loss Pred=2.1018; Val Loss=2.3650; Val Acc=0.3664; Loss Att={'forw': '0.8361'}; Train Acc=0.410; Test Acc=0.3821; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.2744553 2.335797\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:32epoch=80; Loss Pred=2.0841; Val Loss=2.3358; Val Acc=0.3715; Loss Att={'forw': '0.8203'}; Train Acc=0.414; Test Acc=0.3840; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.2744553 2.3860505\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:37epoch=81; Loss Pred=2.0514; Val Loss=2.3861; Val Acc=0.4097; Loss Att={'forw': '0.8294'}; Train Acc=0.443; Test Acc=0.3990; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.2744553 2.3780727\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:41epoch=82; Loss Pred=2.0303; Val Loss=2.3781; Val Acc=0.4097; Loss Att={'forw': '0.8217'}; Train Acc=0.444; Test Acc=0.4000; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.2744553 2.2896729\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:46epoch=83; Loss Pred=1.9823; Val Loss=2.2897; Val Acc=0.4249; Loss Att={'forw': '0.8200'}; Train Acc=0.457; Test Acc=0.3967; Entropy=0; Entropy_Test=\n",
      "\n",
      "18 2.2744553 2.295409\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:51epoch=84; Loss Pred=1.9792; Val Loss=2.2954; Val Acc=0.3969; Loss Att={'forw': '0.8109'}; Train Acc=0.470; Test Acc=0.4063; Entropy=0; Entropy_Test=\n",
      "\n",
      "19 2.2744553 2.2850144\n",
      "Logged Successfully: \n",
      "2018-07-16 14:23:56epoch=85; Loss Pred=1.9660; Val Loss=2.2850; Val Acc=0.4275; Loss Att={'forw': '0.8141'}; Train Acc=0.464; Test Acc=0.3954; Entropy=0; Entropy_Test=\n",
      "\n",
      "20 2.2744553 2.2941911\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:01epoch=86; Loss Pred=1.9698; Val Loss=2.2942; Val Acc=0.4046; Loss Att={'forw': '0.8161'}; Train Acc=0.462; Test Acc=0.3895; Entropy=0; Entropy_Test=\n",
      "\n",
      "21 2.2744553 2.3552008\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:06epoch=87; Loss Pred=2.0342; Val Loss=2.3552; Val Acc=0.4046; Loss Att={'forw': '0.8119'}; Train Acc=0.458; Test Acc=0.4077; Entropy=0; Entropy_Test=\n",
      "\n",
      "22 2.2744553 2.3518996\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:11epoch=88; Loss Pred=2.0127; Val Loss=2.3519; Val Acc=0.4122; Loss Att={'forw': '0.8037'}; Train Acc=0.457; Test Acc=0.4043; Entropy=0; Entropy_Test=\n",
      "\n",
      "23 2.2744553 2.412562\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:16epoch=89; Loss Pred=2.0227; Val Loss=2.4126; Val Acc=0.3868; Loss Att={'forw': '0.8132'}; Train Acc=0.429; Test Acc=0.3926; Entropy=0; Entropy_Test=\n",
      "\n",
      "24 2.2744553 2.3933702\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:20epoch=90; Loss Pred=2.0255; Val Loss=2.3934; Val Acc=0.3868; Loss Att={'forw': '0.8198'}; Train Acc=0.423; Test Acc=0.3837; Entropy=0; Entropy_Test=\n",
      "\n",
      "25 2.2744553 2.4180243\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:25epoch=91; Loss Pred=2.1049; Val Loss=2.4180; Val Acc=0.4249; Loss Att={'forw': '0.8159'}; Train Acc=0.445; Test Acc=0.3987; Entropy=0; Entropy_Test=\n",
      "\n",
      "26 2.2744553 2.4142048\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:29epoch=92; Loss Pred=2.0754; Val Loss=2.4142; Val Acc=0.4020; Loss Att={'forw': '0.8077'}; Train Acc=0.450; Test Acc=0.3940; Entropy=0; Entropy_Test=\n",
      "\n",
      "27 2.2744553 2.3218532\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:35epoch=93; Loss Pred=1.9839; Val Loss=2.3219; Val Acc=0.4071; Loss Att={'forw': '0.8185'}; Train Acc=0.460; Test Acc=0.4066; Entropy=0; Entropy_Test=\n",
      "\n",
      "28 2.2744553 2.3418956\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:39epoch=94; Loss Pred=1.9974; Val Loss=2.3419; Val Acc=0.4377; Loss Att={'forw': '0.8117'}; Train Acc=0.457; Test Acc=0.4067; Entropy=0; Entropy_Test=\n",
      "\n",
      "29 2.2744553 2.2687669\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:44epoch=95; Loss Pred=1.9439; Val Loss=2.2688; Val Acc=0.4122; Loss Att={'forw': '0.8241'}; Train Acc=0.483; Test Acc=0.4129; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2687669 2.260896\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:49epoch=96; Loss Pred=1.9292; Val Loss=2.2609; Val Acc=0.4148; Loss Att={'forw': '0.8203'}; Train Acc=0.489; Test Acc=0.4167; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.260896 2.294334\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:54epoch=97; Loss Pred=1.9086; Val Loss=2.2943; Val Acc=0.4249; Loss Att={'forw': '0.8091'}; Train Acc=0.493; Test Acc=0.4164; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.260896 2.2846851\n",
      "Logged Successfully: \n",
      "2018-07-16 14:24:59epoch=98; Loss Pred=1.9205; Val Loss=2.2847; Val Acc=0.4249; Loss Att={'forw': '0.8072'}; Train Acc=0.491; Test Acc=0.4139; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.260896 2.2922726\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:04epoch=99; Loss Pred=1.8878; Val Loss=2.2923; Val Acc=0.4071; Loss Att={'forw': '0.8054'}; Train Acc=0.497; Test Acc=0.4068; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.260896 2.2808075\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:08epoch=100; Loss Pred=1.8993; Val Loss=2.2808; Val Acc=0.4300; Loss Att={'forw': '0.7961'}; Train Acc=0.491; Test Acc=0.4020; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.260896 2.2597952\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:13epoch=101; Loss Pred=1.8718; Val Loss=2.2598; Val Acc=0.4377; Loss Att={'forw': '0.7980'}; Train Acc=0.503; Test Acc=0.4143; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2597952 2.2942061\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:18epoch=102; Loss Pred=1.8721; Val Loss=2.2942; Val Acc=0.4300; Loss Att={'forw': '0.7913'}; Train Acc=0.498; Test Acc=0.4100; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2597952 2.2948432\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:23epoch=103; Loss Pred=1.8855; Val Loss=2.2948; Val Acc=0.4046; Loss Att={'forw': '0.8005'}; Train Acc=0.490; Test Acc=0.4062; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2597952 2.2590036\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:27epoch=104; Loss Pred=1.8868; Val Loss=2.2590; Val Acc=0.4198; Loss Att={'forw': '0.7946'}; Train Acc=0.494; Test Acc=0.4036; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2590036 2.277837\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:32epoch=105; Loss Pred=1.8307; Val Loss=2.2778; Val Acc=0.4249; Loss Att={'forw': '0.8066'}; Train Acc=0.516; Test Acc=0.4102; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2590036 2.2656713\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:37epoch=106; Loss Pred=1.8347; Val Loss=2.2657; Val Acc=0.4351; Loss Att={'forw': '0.8034'}; Train Acc=0.502; Test Acc=0.4114; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2590036 2.2967398\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:42epoch=107; Loss Pred=1.8200; Val Loss=2.2967; Val Acc=0.4326; Loss Att={'forw': '0.7938'}; Train Acc=0.510; Test Acc=0.4189; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2590036 2.2796142\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:46epoch=108; Loss Pred=1.8554; Val Loss=2.2796; Val Acc=0.4300; Loss Att={'forw': '0.7948'}; Train Acc=0.509; Test Acc=0.4168; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2590036 2.41216\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:52epoch=109; Loss Pred=2.0719; Val Loss=2.4122; Val Acc=0.3944; Loss Att={'forw': '0.8083'}; Train Acc=0.417; Test Acc=0.3639; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2590036 2.3959517\n",
      "Logged Successfully: \n",
      "2018-07-16 14:25:56epoch=110; Loss Pred=2.0769; Val Loss=2.3960; Val Acc=0.3893; Loss Att={'forw': '0.8074'}; Train Acc=0.423; Test Acc=0.3634; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.2590036 2.5242794\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:01epoch=111; Loss Pred=2.0333; Val Loss=2.5243; Val Acc=0.3791; Loss Att={'forw': '0.8003'}; Train Acc=0.452; Test Acc=0.3995; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.2590036 2.4685118\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:05epoch=112; Loss Pred=2.0057; Val Loss=2.4685; Val Acc=0.3944; Loss Att={'forw': '0.8014'}; Train Acc=0.445; Test Acc=0.3936; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.2590036 2.4265175\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:10epoch=113; Loss Pred=1.9626; Val Loss=2.4265; Val Acc=0.4046; Loss Att={'forw': '0.8233'}; Train Acc=0.492; Test Acc=0.4061; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.2590036 2.4707708\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:15epoch=114; Loss Pred=1.9889; Val Loss=2.4708; Val Acc=0.4097; Loss Att={'forw': '0.8130'}; Train Acc=0.488; Test Acc=0.4081; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.2590036 2.2611954\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:20epoch=115; Loss Pred=1.8565; Val Loss=2.2612; Val Acc=0.4300; Loss Att={'forw': '0.8248'}; Train Acc=0.506; Test Acc=0.4117; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.2590036 2.3027043\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:24epoch=116; Loss Pred=1.8641; Val Loss=2.3027; Val Acc=0.4122; Loss Att={'forw': '0.8136'}; Train Acc=0.502; Test Acc=0.4053; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.2590036 2.417344\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:30epoch=117; Loss Pred=1.8474; Val Loss=2.4173; Val Acc=0.3944; Loss Att={'forw': '0.8074'}; Train Acc=0.511; Test Acc=0.4210; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.2590036 2.3971875\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:34epoch=118; Loss Pred=1.8625; Val Loss=2.3972; Val Acc=0.3893; Loss Att={'forw': '0.8084'}; Train Acc=0.506; Test Acc=0.4193; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.2590036 2.4921334\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:40epoch=119; Loss Pred=2.1600; Val Loss=2.4921; Val Acc=0.3613; Loss Att={'forw': '0.8294'}; Train Acc=0.392; Test Acc=0.3393; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.2590036 2.4513154\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:44epoch=120; Loss Pred=2.1413; Val Loss=2.4513; Val Acc=0.3562; Loss Att={'forw': '0.8185'}; Train Acc=0.392; Test Acc=0.3580; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.2590036 2.4255044\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:49epoch=121; Loss Pred=2.0045; Val Loss=2.4255; Val Acc=0.3944; Loss Att={'forw': '0.8492'}; Train Acc=0.461; Test Acc=0.3972; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.2590036 2.450446\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:53epoch=122; Loss Pred=2.0307; Val Loss=2.4504; Val Acc=0.3842; Loss Att={'forw': '0.8184'}; Train Acc=0.457; Test Acc=0.3875; Entropy=0; Entropy_Test=\n",
      "\n",
      "18 2.2590036 2.3041854\n",
      "Logged Successfully: \n",
      "2018-07-16 14:26:59epoch=123; Loss Pred=1.9153; Val Loss=2.3042; Val Acc=0.4300; Loss Att={'forw': '0.8403'}; Train Acc=0.494; Test Acc=0.4168; Entropy=0; Entropy_Test=\n",
      "\n",
      "19 2.2590036 2.3359556\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:03epoch=124; Loss Pred=1.9528; Val Loss=2.3360; Val Acc=0.4122; Loss Att={'forw': '0.8354'}; Train Acc=0.464; Test Acc=0.3949; Entropy=0; Entropy_Test=\n",
      "\n",
      "20 2.2590036 2.3309298\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:08epoch=125; Loss Pred=1.8593; Val Loss=2.3309; Val Acc=0.3919; Loss Att={'forw': '0.8259'}; Train Acc=0.491; Test Acc=0.4151; Entropy=0; Entropy_Test=\n",
      "\n",
      "21 2.2590036 2.3044817\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:12epoch=126; Loss Pred=1.8724; Val Loss=2.3045; Val Acc=0.3995; Loss Att={'forw': '0.8189'}; Train Acc=0.484; Test Acc=0.4002; Entropy=0; Entropy_Test=\n",
      "\n",
      "22 2.2590036 2.322336\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:18epoch=127; Loss Pred=1.8586; Val Loss=2.3223; Val Acc=0.4148; Loss Att={'forw': '0.8215'}; Train Acc=0.502; Test Acc=0.4124; Entropy=0; Entropy_Test=\n",
      "\n",
      "23 2.2590036 2.3243375\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:22epoch=128; Loss Pred=1.8641; Val Loss=2.3243; Val Acc=0.4097; Loss Att={'forw': '0.8157'}; Train Acc=0.502; Test Acc=0.4188; Entropy=0; Entropy_Test=\n",
      "\n",
      "24 2.2590036 2.400567\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:27epoch=129; Loss Pred=1.8599; Val Loss=2.4006; Val Acc=0.4046; Loss Att={'forw': '0.8155'}; Train Acc=0.509; Test Acc=0.4195; Entropy=0; Entropy_Test=\n",
      "\n",
      "25 2.2590036 2.3976033\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:31epoch=130; Loss Pred=1.8459; Val Loss=2.3976; Val Acc=0.4020; Loss Att={'forw': '0.8096'}; Train Acc=0.518; Test Acc=0.4220; Entropy=0; Entropy_Test=\n",
      "\n",
      "26 2.2590036 2.3342528\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:37epoch=131; Loss Pred=1.8892; Val Loss=2.3343; Val Acc=0.3944; Loss Att={'forw': '0.8086'}; Train Acc=0.481; Test Acc=0.3894; Entropy=0; Entropy_Test=\n",
      "\n",
      "27 2.2590036 2.3278913\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:41epoch=132; Loss Pred=1.8810; Val Loss=2.3279; Val Acc=0.4046; Loss Att={'forw': '0.8017'}; Train Acc=0.491; Test Acc=0.3916; Entropy=0; Entropy_Test=\n",
      "\n",
      "28 2.2590036 2.3561528\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:46epoch=133; Loss Pred=1.7924; Val Loss=2.3562; Val Acc=0.4122; Loss Att={'forw': '0.8158'}; Train Acc=0.511; Test Acc=0.4033; Entropy=0; Entropy_Test=\n",
      "\n",
      "29 2.2590036 2.3559856\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:51epoch=134; Loss Pred=1.8206; Val Loss=2.3560; Val Acc=0.4071; Loss Att={'forw': '0.8013'}; Train Acc=0.511; Test Acc=0.4034; Entropy=0; Entropy_Test=\n",
      "\n",
      "30 2.2590036 2.5114245\n",
      "Logged Successfully: \n",
      "2018-07-16 14:27:56epoch=135; Loss Pred=1.8918; Val Loss=2.5114; Val Acc=0.4071; Loss Att={'forw': '0.8221'}; Train Acc=0.512; Test Acc=0.4180; Entropy=0; Entropy_Test=\n",
      "\n",
      "31 2.2590036 2.4327657\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:00epoch=136; Loss Pred=1.8545; Val Loss=2.4328; Val Acc=0.4122; Loss Att={'forw': '0.8067'}; Train Acc=0.521; Test Acc=0.4141; Entropy=0; Entropy_Test=\n",
      "\n",
      "32 2.2590036 2.2997484\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:06epoch=137; Loss Pred=1.8003; Val Loss=2.2997; Val Acc=0.4275; Loss Att={'forw': '0.8195'}; Train Acc=0.501; Test Acc=0.4082; Entropy=0; Entropy_Test=\n",
      "\n",
      "33 2.2590036 2.32034\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:09epoch=138; Loss Pred=1.7850; Val Loss=2.3203; Val Acc=0.4071; Loss Att={'forw': '0.8096'}; Train Acc=0.510; Test Acc=0.4125; Entropy=0; Entropy_Test=\n",
      "\n",
      "34 2.2590036 2.3564134\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:15epoch=139; Loss Pred=1.8215; Val Loss=2.3564; Val Acc=0.4071; Loss Att={'forw': '0.8074'}; Train Acc=0.505; Test Acc=0.4161; Entropy=0; Entropy_Test=\n",
      "\n",
      "35 2.2590036 2.3664422\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:19epoch=140; Loss Pred=1.8355; Val Loss=2.3664; Val Acc=0.3969; Loss Att={'forw': '0.8009'}; Train Acc=0.499; Test Acc=0.4242; Entropy=0; Entropy_Test=\n",
      "\n",
      "36 2.2590036 2.3983448\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:25epoch=141; Loss Pred=1.8477; Val Loss=2.3983; Val Acc=0.4020; Loss Att={'forw': '0.8184'}; Train Acc=0.477; Test Acc=0.4021; Entropy=0; Entropy_Test=\n",
      "\n",
      "37 2.2590036 2.4090197\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:29epoch=142; Loss Pred=1.8589; Val Loss=2.4090; Val Acc=0.4173; Loss Att={'forw': '0.8145'}; Train Acc=0.476; Test Acc=0.4003; Entropy=0; Entropy_Test=\n",
      "\n",
      "38 2.2590036 2.3367958\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:34epoch=143; Loss Pred=1.7823; Val Loss=2.3368; Val Acc=0.4097; Loss Att={'forw': '0.8152'}; Train Acc=0.524; Test Acc=0.4200; Entropy=0; Entropy_Test=\n",
      "\n",
      "39 2.2590036 2.3489976\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:38epoch=144; Loss Pred=1.7791; Val Loss=2.3490; Val Acc=0.4122; Loss Att={'forw': '0.8075'}; Train Acc=0.526; Test Acc=0.4212; Entropy=0; Entropy_Test=\n",
      "\n",
      "40 2.2590036 2.485758\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:43epoch=145; Loss Pred=1.9297; Val Loss=2.4858; Val Acc=0.4020; Loss Att={'forw': '0.8220'}; Train Acc=0.501; Test Acc=0.4205; Entropy=0; Entropy_Test=\n",
      "\n",
      "41 2.2590036 2.518298\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:48epoch=146; Loss Pred=1.9485; Val Loss=2.5183; Val Acc=0.4071; Loss Att={'forw': '0.8089'}; Train Acc=0.496; Test Acc=0.4196; Entropy=0; Entropy_Test=\n",
      "\n",
      "42 2.2590036 2.3378606\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:53epoch=147; Loss Pred=1.7961; Val Loss=2.3379; Val Acc=0.3842; Loss Att={'forw': '0.8059'}; Train Acc=0.513; Test Acc=0.4100; Entropy=0; Entropy_Test=\n",
      "\n",
      "43 2.2590036 2.3455982\n",
      "Logged Successfully: \n",
      "2018-07-16 14:28:57epoch=148; Loss Pred=1.7933; Val Loss=2.3456; Val Acc=0.3817; Loss Att={'forw': '0.8073'}; Train Acc=0.516; Test Acc=0.4122; Entropy=0; Entropy_Test=\n",
      "\n",
      "44 2.2590036 2.4108746\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:03epoch=149; Loss Pred=1.7577; Val Loss=2.4109; Val Acc=0.4173; Loss Att={'forw': '0.8112'}; Train Acc=0.516; Test Acc=0.4225; Entropy=0; Entropy_Test=\n",
      "\n",
      "45 2.2590036 2.4144013\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:07epoch=150; Loss Pred=1.7637; Val Loss=2.4144; Val Acc=0.4148; Loss Att={'forw': '0.8061'}; Train Acc=0.519; Test Acc=0.4226; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 2.2590036 2.3669178\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:12epoch=151; Loss Pred=1.7097; Val Loss=2.3669; Val Acc=0.4020; Loss Att={'forw': '0.7985'}; Train Acc=0.540; Test Acc=0.4206; Entropy=0; Entropy_Test=\n",
      "\n",
      "47 2.2590036 2.356343\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:16epoch=152; Loss Pred=1.7221; Val Loss=2.3563; Val Acc=0.4097; Loss Att={'forw': '0.7962'}; Train Acc=0.539; Test Acc=0.4217; Entropy=0; Entropy_Test=\n",
      "\n",
      "48 2.2590036 2.3797321\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:22epoch=153; Loss Pred=1.7040; Val Loss=2.3797; Val Acc=0.4097; Loss Att={'forw': '0.7981'}; Train Acc=0.537; Test Acc=0.4206; Entropy=0; Entropy_Test=\n",
      "\n",
      "49 2.2590036 2.3664753\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:26epoch=154; Loss Pred=1.7142; Val Loss=2.3665; Val Acc=0.4173; Loss Att={'forw': '0.7926'}; Train Acc=0.536; Test Acc=0.4230; Entropy=0; Entropy_Test=\n",
      "\n",
      "50 2.2590036 2.4450214\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 156\n",
      "Optimization Finished!\n",
      "********** replication  3  **********\n",
      "topic_classification\n",
      "((11228, 300), (11228, 1))\n",
      "0.0\n",
      "(1965, 300) (1965, 1) (393, 300) (393, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: topic_classification\n",
      "    hid: \t\t\t150,\n",
      "    h_hid: \t\t\t300\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t1965/2245, SEQ_LEN: 300\n",
      "Logged Successfully: \n",
      "0 10000000000.0 3.7739518\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:34epoch=0; Loss Pred=3.7763; Val Loss=3.7740; Val Acc=0.0076; Loss Att={'forw': '1.1067'}; Train Acc=0.004; Test Acc=0.0031; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 3.7739518 2.4058018\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:39epoch=1; Loss Pred=2.5222; Val Loss=2.4058; Val Acc=0.3435; Loss Att={'forw': '1.2183'}; Train Acc=0.349; Test Acc=0.3556; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4058018 2.3994842\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:44epoch=2; Loss Pred=2.5249; Val Loss=2.3995; Val Acc=0.3435; Loss Att={'forw': '0.9348'}; Train Acc=0.350; Test Acc=0.3565; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3994842 2.4030316\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:49epoch=3; Loss Pred=2.4492; Val Loss=2.4030; Val Acc=0.3435; Loss Att={'forw': '0.9686'}; Train Acc=0.350; Test Acc=0.3583; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3994842 2.3984804\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:53epoch=4; Loss Pred=2.4476; Val Loss=2.3985; Val Acc=0.3435; Loss Att={'forw': '0.8002'}; Train Acc=0.350; Test Acc=0.3552; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3984804 2.3723161\n",
      "Logged Successfully: \n",
      "2018-07-16 14:29:59epoch=5; Loss Pred=2.4345; Val Loss=2.3723; Val Acc=0.3282; Loss Att={'forw': '0.9561'}; Train Acc=0.333; Test Acc=0.3402; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3723161 2.381583\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:03epoch=6; Loss Pred=2.4260; Val Loss=2.3816; Val Acc=0.3282; Loss Att={'forw': '0.8683'}; Train Acc=0.331; Test Acc=0.3358; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3723161 2.3597891\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:08epoch=7; Loss Pred=2.4163; Val Loss=2.3598; Val Acc=0.3435; Loss Att={'forw': '0.9169'}; Train Acc=0.350; Test Acc=0.3552; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3597891 2.3742995\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:12epoch=8; Loss Pred=2.4188; Val Loss=2.3743; Val Acc=0.3435; Loss Att={'forw': '0.8786'}; Train Acc=0.350; Test Acc=0.3573; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3597891 2.3129315\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:18epoch=9; Loss Pred=2.3895; Val Loss=2.3129; Val Acc=0.3511; Loss Att={'forw': '0.9092'}; Train Acc=0.349; Test Acc=0.3585; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3129315 2.3227382\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:22epoch=10; Loss Pred=2.4059; Val Loss=2.3227; Val Acc=0.3486; Loss Att={'forw': '0.8893'}; Train Acc=0.350; Test Acc=0.3559; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3129315 2.3298075\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:27epoch=11; Loss Pred=2.3917; Val Loss=2.3298; Val Acc=0.3461; Loss Att={'forw': '0.8942'}; Train Acc=0.353; Test Acc=0.3611; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3129315 2.3396864\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:31epoch=12; Loss Pred=2.3916; Val Loss=2.3397; Val Acc=0.3486; Loss Att={'forw': '0.9321'}; Train Acc=0.351; Test Acc=0.3579; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.3129315 2.362922\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:37epoch=13; Loss Pred=2.3884; Val Loss=2.3629; Val Acc=0.3537; Loss Att={'forw': '0.8520'}; Train Acc=0.352; Test Acc=0.3609; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.3129315 2.3702996\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:41epoch=14; Loss Pred=2.3896; Val Loss=2.3703; Val Acc=0.3511; Loss Att={'forw': '0.8522'}; Train Acc=0.352; Test Acc=0.3624; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.3129315 2.3571446\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:47epoch=15; Loss Pred=2.3962; Val Loss=2.3571; Val Acc=0.3588; Loss Att={'forw': '0.8554'}; Train Acc=0.351; Test Acc=0.3550; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.3129315 2.3571014\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:51epoch=16; Loss Pred=2.3963; Val Loss=2.3571; Val Acc=0.3461; Loss Att={'forw': '0.8333'}; Train Acc=0.353; Test Acc=0.3593; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.3129315 2.3608694\n",
      "Logged Successfully: \n",
      "2018-07-16 14:30:56epoch=17; Loss Pred=2.3851; Val Loss=2.3609; Val Acc=0.3537; Loss Att={'forw': '0.8599'}; Train Acc=0.350; Test Acc=0.3688; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.3129315 2.3641832\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:00epoch=18; Loss Pred=2.3827; Val Loss=2.3642; Val Acc=0.3588; Loss Att={'forw': '0.8567'}; Train Acc=0.351; Test Acc=0.3664; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.3129315 2.362501\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:06epoch=19; Loss Pred=2.3603; Val Loss=2.3625; Val Acc=0.3562; Loss Att={'forw': '0.8530'}; Train Acc=0.350; Test Acc=0.3641; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.3129315 2.3565698\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:10epoch=20; Loss Pred=2.3683; Val Loss=2.3566; Val Acc=0.3537; Loss Att={'forw': '0.8462'}; Train Acc=0.352; Test Acc=0.3664; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.3129315 2.298431\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:15epoch=21; Loss Pred=2.3390; Val Loss=2.2984; Val Acc=0.3588; Loss Att={'forw': '0.8865'}; Train Acc=0.356; Test Acc=0.3694; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.298431 2.2749238\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:20epoch=22; Loss Pred=2.3233; Val Loss=2.2749; Val Acc=0.3639; Loss Att={'forw': '0.8796'}; Train Acc=0.364; Test Acc=0.3698; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2749238 2.2979605\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:25epoch=23; Loss Pred=2.3214; Val Loss=2.2980; Val Acc=0.3639; Loss Att={'forw': '0.8690'}; Train Acc=0.363; Test Acc=0.3671; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2749238 2.3043244\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:29epoch=24; Loss Pred=2.3166; Val Loss=2.3043; Val Acc=0.3562; Loss Att={'forw': '0.8714'}; Train Acc=0.354; Test Acc=0.3654; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2749238 2.2878764\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:34epoch=25; Loss Pred=2.3061; Val Loss=2.2879; Val Acc=0.3664; Loss Att={'forw': '0.8336'}; Train Acc=0.391; Test Acc=0.3866; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2749238 2.297341\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:39epoch=26; Loss Pred=2.2998; Val Loss=2.2973; Val Acc=0.3766; Loss Att={'forw': '0.8193'}; Train Acc=0.391; Test Acc=0.3884; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2749238 2.3109882\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:45epoch=27; Loss Pred=2.2908; Val Loss=2.3110; Val Acc=0.3410; Loss Att={'forw': '0.8151'}; Train Acc=0.398; Test Acc=0.3675; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2749238 2.301774\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:49epoch=28; Loss Pred=2.2964; Val Loss=2.3018; Val Acc=0.3435; Loss Att={'forw': '0.8062'}; Train Acc=0.389; Test Acc=0.3733; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.2749238 2.3127334\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:54epoch=29; Loss Pred=2.3126; Val Loss=2.3127; Val Acc=0.3461; Loss Att={'forw': '0.8404'}; Train Acc=0.383; Test Acc=0.3629; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.2749238 2.3223798\n",
      "Logged Successfully: \n",
      "2018-07-16 14:31:58epoch=30; Loss Pred=2.3005; Val Loss=2.3224; Val Acc=0.3410; Loss Att={'forw': '0.8258'}; Train Acc=0.383; Test Acc=0.3661; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.2749238 2.271766\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:03epoch=31; Loss Pred=2.2635; Val Loss=2.2718; Val Acc=0.3639; Loss Att={'forw': '0.8673'}; Train Acc=0.395; Test Acc=0.3896; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.271766 2.2813666\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:07epoch=32; Loss Pred=2.2669; Val Loss=2.2814; Val Acc=0.3639; Loss Att={'forw': '0.8575'}; Train Acc=0.384; Test Acc=0.3897; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.271766 2.253207\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:13epoch=33; Loss Pred=2.2407; Val Loss=2.2532; Val Acc=0.3817; Loss Att={'forw': '0.8987'}; Train Acc=0.400; Test Acc=0.3899; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.253207 2.2633793\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:17epoch=34; Loss Pred=2.2453; Val Loss=2.2634; Val Acc=0.3690; Loss Att={'forw': '0.8415'}; Train Acc=0.403; Test Acc=0.3927; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.253207 2.251007\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:23epoch=35; Loss Pred=2.2350; Val Loss=2.2510; Val Acc=0.3537; Loss Att={'forw': '0.8538'}; Train Acc=0.407; Test Acc=0.3901; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.251007 2.2266142\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:26epoch=36; Loss Pred=2.2336; Val Loss=2.2266; Val Acc=0.3562; Loss Att={'forw': '0.8823'}; Train Acc=0.409; Test Acc=0.3892; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2266142 2.3051353\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:32epoch=37; Loss Pred=2.2703; Val Loss=2.3051; Val Acc=0.3613; Loss Att={'forw': '0.8550'}; Train Acc=0.399; Test Acc=0.3884; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2266142 2.3194094\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:36epoch=38; Loss Pred=2.2907; Val Loss=2.3194; Val Acc=0.3740; Loss Att={'forw': '0.8668'}; Train Acc=0.390; Test Acc=0.3851; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2266142 2.2490385\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:41epoch=39; Loss Pred=2.2451; Val Loss=2.2490; Val Acc=0.3919; Loss Att={'forw': '0.9049'}; Train Acc=0.405; Test Acc=0.3781; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2266142 2.2378259\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:46epoch=40; Loss Pred=2.2365; Val Loss=2.2378; Val Acc=0.3715; Loss Att={'forw': '0.8521'}; Train Acc=0.402; Test Acc=0.3867; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2266142 2.2409916\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:51epoch=41; Loss Pred=2.2335; Val Loss=2.2410; Val Acc=0.3791; Loss Att={'forw': '0.8631'}; Train Acc=0.424; Test Acc=0.3947; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2266142 2.2432718\n",
      "Logged Successfully: \n",
      "2018-07-16 14:32:55epoch=42; Loss Pred=2.2425; Val Loss=2.2433; Val Acc=0.3817; Loss Att={'forw': '0.8453'}; Train Acc=0.423; Test Acc=0.3986; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.2266142 2.2387075\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:00epoch=43; Loss Pred=2.2179; Val Loss=2.2387; Val Acc=0.3740; Loss Att={'forw': '0.8596'}; Train Acc=0.408; Test Acc=0.4060; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.2266142 2.2152312\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:04epoch=44; Loss Pred=2.2232; Val Loss=2.2152; Val Acc=0.3690; Loss Att={'forw': '0.8443'}; Train Acc=0.406; Test Acc=0.4038; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2152312 2.2504706\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:10epoch=45; Loss Pred=2.2018; Val Loss=2.2505; Val Acc=0.3893; Loss Att={'forw': '0.8598'}; Train Acc=0.414; Test Acc=0.4099; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2152312 2.2535706\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:14epoch=46; Loss Pred=2.2065; Val Loss=2.2536; Val Acc=0.3995; Loss Att={'forw': '0.8492'}; Train Acc=0.415; Test Acc=0.4054; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2152312 2.2496238\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:19epoch=47; Loss Pred=2.2052; Val Loss=2.2496; Val Acc=0.3919; Loss Att={'forw': '0.8219'}; Train Acc=0.422; Test Acc=0.4068; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2152312 2.256138\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:23epoch=48; Loss Pred=2.1951; Val Loss=2.2561; Val Acc=0.3893; Loss Att={'forw': '0.8220'}; Train Acc=0.419; Test Acc=0.4042; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2152312 2.228114\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:29epoch=49; Loss Pred=2.1737; Val Loss=2.2281; Val Acc=0.3995; Loss Att={'forw': '0.8181'}; Train Acc=0.419; Test Acc=0.4059; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2152312 2.2167091\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:33epoch=50; Loss Pred=2.1644; Val Loss=2.2167; Val Acc=0.4122; Loss Att={'forw': '0.8087'}; Train Acc=0.413; Test Acc=0.4086; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.2152312 2.187236\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:39epoch=51; Loss Pred=2.1448; Val Loss=2.1872; Val Acc=0.3995; Loss Att={'forw': '0.8114'}; Train Acc=0.425; Test Acc=0.4101; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.187236 2.173732\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:43epoch=52; Loss Pred=2.1344; Val Loss=2.1737; Val Acc=0.4122; Loss Att={'forw': '0.8002'}; Train Acc=0.427; Test Acc=0.4106; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.173732 2.1740718\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:48epoch=53; Loss Pred=2.1221; Val Loss=2.1741; Val Acc=0.3893; Loss Att={'forw': '0.8002'}; Train Acc=0.429; Test Acc=0.4081; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.173732 2.1529522\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:52epoch=54; Loss Pred=2.1234; Val Loss=2.1530; Val Acc=0.4046; Loss Att={'forw': '0.7897'}; Train Acc=0.424; Test Acc=0.4112; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1529522 2.3177378\n",
      "Logged Successfully: \n",
      "2018-07-16 14:33:58epoch=55; Loss Pred=2.3512; Val Loss=2.3177; Val Acc=0.3384; Loss Att={'forw': '0.7873'}; Train Acc=0.342; Test Acc=0.3365; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.1529522 2.3669481\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:02epoch=56; Loss Pred=2.3993; Val Loss=2.3669; Val Acc=0.3257; Loss Att={'forw': '0.7884'}; Train Acc=0.324; Test Acc=0.3188; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.1529522 2.2793658\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:07epoch=57; Loss Pred=2.2497; Val Loss=2.2794; Val Acc=0.3588; Loss Att={'forw': '0.8118'}; Train Acc=0.385; Test Acc=0.3669; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.1529522 2.2961326\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:11epoch=58; Loss Pred=2.2610; Val Loss=2.2961; Val Acc=0.3562; Loss Att={'forw': '0.8226'}; Train Acc=0.393; Test Acc=0.3672; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.1529522 2.2663424\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:17epoch=59; Loss Pred=2.2068; Val Loss=2.2663; Val Acc=0.3868; Loss Att={'forw': '0.8454'}; Train Acc=0.386; Test Acc=0.3842; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.1529522 2.2813673\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:21epoch=60; Loss Pred=2.2212; Val Loss=2.2814; Val Acc=0.3511; Loss Att={'forw': '0.8207'}; Train Acc=0.380; Test Acc=0.3807; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.1529522 2.2406526\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:26epoch=61; Loss Pred=2.2128; Val Loss=2.2407; Val Acc=0.3740; Loss Att={'forw': '0.8314'}; Train Acc=0.405; Test Acc=0.3731; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.1529522 2.2542567\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:30epoch=62; Loss Pred=2.2117; Val Loss=2.2543; Val Acc=0.3613; Loss Att={'forw': '0.8341'}; Train Acc=0.402; Test Acc=0.3677; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.1529522 2.2340019\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:35epoch=63; Loss Pred=2.1735; Val Loss=2.2340; Val Acc=0.3817; Loss Att={'forw': '0.8207'}; Train Acc=0.407; Test Acc=0.3868; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.1529522 2.2240813\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:40epoch=64; Loss Pred=2.1617; Val Loss=2.2241; Val Acc=0.3740; Loss Att={'forw': '0.8119'}; Train Acc=0.413; Test Acc=0.3887; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.1529522 2.2131472\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:45epoch=65; Loss Pred=2.1582; Val Loss=2.2131; Val Acc=0.3919; Loss Att={'forw': '0.8087'}; Train Acc=0.422; Test Acc=0.3897; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.1529522 2.214107\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:49epoch=66; Loss Pred=2.1516; Val Loss=2.2141; Val Acc=0.3868; Loss Att={'forw': '0.8031'}; Train Acc=0.425; Test Acc=0.3918; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.1529522 2.2321537\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:54epoch=67; Loss Pred=2.1756; Val Loss=2.2322; Val Acc=0.3562; Loss Att={'forw': '0.8033'}; Train Acc=0.398; Test Acc=0.3734; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.1529522 2.2310247\n",
      "Logged Successfully: \n",
      "2018-07-16 14:34:58epoch=68; Loss Pred=2.1739; Val Loss=2.2310; Val Acc=0.3613; Loss Att={'forw': '0.7988'}; Train Acc=0.393; Test Acc=0.3758; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.1529522 2.1908903\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:04epoch=69; Loss Pred=2.1610; Val Loss=2.1909; Val Acc=0.3842; Loss Att={'forw': '0.8366'}; Train Acc=0.412; Test Acc=0.3926; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.1529522 2.199144\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:08epoch=70; Loss Pred=2.1550; Val Loss=2.1991; Val Acc=0.4097; Loss Att={'forw': '0.8154'}; Train Acc=0.411; Test Acc=0.3906; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2.1529522 2.1603975\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:13epoch=71; Loss Pred=2.1177; Val Loss=2.1604; Val Acc=0.3715; Loss Att={'forw': '0.8268'}; Train Acc=0.401; Test Acc=0.3917; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.1529522 2.1438644\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:18epoch=72; Loss Pred=2.1164; Val Loss=2.1439; Val Acc=0.4071; Loss Att={'forw': '0.8117'}; Train Acc=0.396; Test Acc=0.3854; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1438644 2.0634491\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:23epoch=73; Loss Pred=2.0631; Val Loss=2.0634; Val Acc=0.3969; Loss Att={'forw': '0.8053'}; Train Acc=0.402; Test Acc=0.3958; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.0634491 2.088372\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:27epoch=74; Loss Pred=2.0736; Val Loss=2.0884; Val Acc=0.3842; Loss Att={'forw': '0.7994'}; Train Acc=0.402; Test Acc=0.3959; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.0634491 2.0408716\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:32epoch=75; Loss Pred=2.0256; Val Loss=2.0409; Val Acc=0.3511; Loss Att={'forw': '0.8012'}; Train Acc=0.395; Test Acc=0.3712; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.0408716 2.0356119\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:37epoch=76; Loss Pred=2.0343; Val Loss=2.0356; Val Acc=0.3639; Loss Att={'forw': '0.7949'}; Train Acc=0.386; Test Acc=0.3660; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.0356119 2.4579358\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:42epoch=77; Loss Pred=2.3903; Val Loss=2.4579; Val Acc=0.2824; Loss Att={'forw': '0.8039'}; Train Acc=0.314; Test Acc=0.2980; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.0356119 2.449775\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:46epoch=78; Loss Pred=2.3776; Val Loss=2.4498; Val Acc=0.2646; Loss Att={'forw': '0.8028'}; Train Acc=0.310; Test Acc=0.3030; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.0356119 2.2620955\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:52epoch=79; Loss Pred=2.2593; Val Loss=2.2621; Val Acc=0.3588; Loss Att={'forw': '0.8084'}; Train Acc=0.391; Test Acc=0.3786; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.0356119 2.2599635\n",
      "Logged Successfully: \n",
      "2018-07-16 14:35:56epoch=80; Loss Pred=2.2626; Val Loss=2.2600; Val Acc=0.3766; Loss Att={'forw': '0.8132'}; Train Acc=0.405; Test Acc=0.3765; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.0356119 2.3296573\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:01epoch=81; Loss Pred=2.2592; Val Loss=2.3297; Val Acc=0.3537; Loss Att={'forw': '0.8463'}; Train Acc=0.386; Test Acc=0.3810; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.0356119 2.3192906\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:05epoch=82; Loss Pred=2.2577; Val Loss=2.3193; Val Acc=0.3384; Loss Att={'forw': '0.8259'}; Train Acc=0.387; Test Acc=0.3875; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.0356119 2.2715385\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:11epoch=83; Loss Pred=2.1894; Val Loss=2.2715; Val Acc=0.3817; Loss Att={'forw': '0.8170'}; Train Acc=0.426; Test Acc=0.3950; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.0356119 2.2689037\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:15epoch=84; Loss Pred=2.1920; Val Loss=2.2689; Val Acc=0.3919; Loss Att={'forw': '0.8104'}; Train Acc=0.420; Test Acc=0.3946; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.0356119 2.238125\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:20epoch=85; Loss Pred=2.1527; Val Loss=2.2381; Val Acc=0.3664; Loss Att={'forw': '0.8410'}; Train Acc=0.418; Test Acc=0.3877; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.0356119 2.2476168\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:24epoch=86; Loss Pred=2.1639; Val Loss=2.2476; Val Acc=0.3817; Loss Att={'forw': '0.8283'}; Train Acc=0.430; Test Acc=0.3818; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.0356119 2.2238019\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:29epoch=87; Loss Pred=2.1276; Val Loss=2.2238; Val Acc=0.3766; Loss Att={'forw': '0.8174'}; Train Acc=0.435; Test Acc=0.3953; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.0356119 2.2337444\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:33epoch=88; Loss Pred=2.1385; Val Loss=2.2337; Val Acc=0.3613; Loss Att={'forw': '0.8086'}; Train Acc=0.445; Test Acc=0.3883; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.0356119 2.1816266\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:39epoch=89; Loss Pred=2.0874; Val Loss=2.1816; Val Acc=0.3766; Loss Att={'forw': '0.8309'}; Train Acc=0.453; Test Acc=0.3945; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.0356119 2.171944\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:43epoch=90; Loss Pred=2.1005; Val Loss=2.1719; Val Acc=0.4020; Loss Att={'forw': '0.8222'}; Train Acc=0.444; Test Acc=0.4050; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.0356119 2.2134516\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:49epoch=91; Loss Pred=2.1219; Val Loss=2.2135; Val Acc=0.3791; Loss Att={'forw': '0.8251'}; Train Acc=0.444; Test Acc=0.3995; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.0356119 2.217706\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:53epoch=92; Loss Pred=2.1335; Val Loss=2.2177; Val Acc=0.3842; Loss Att={'forw': '0.8209'}; Train Acc=0.435; Test Acc=0.3995; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.0356119 2.2255802\n",
      "Logged Successfully: \n",
      "2018-07-16 14:36:58epoch=93; Loss Pred=2.1279; Val Loss=2.2256; Val Acc=0.3817; Loss Att={'forw': '0.8107'}; Train Acc=0.434; Test Acc=0.3814; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.0356119 2.2415648\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:02epoch=94; Loss Pred=2.1184; Val Loss=2.2416; Val Acc=0.3969; Loss Att={'forw': '0.8197'}; Train Acc=0.426; Test Acc=0.3743; Entropy=0; Entropy_Test=\n",
      "\n",
      "18 2.0356119 2.2184823\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:08epoch=95; Loss Pred=2.1197; Val Loss=2.2185; Val Acc=0.4071; Loss Att={'forw': '0.8158'}; Train Acc=0.443; Test Acc=0.3961; Entropy=0; Entropy_Test=\n",
      "\n",
      "19 2.0356119 2.2123406\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:12epoch=96; Loss Pred=2.1169; Val Loss=2.2123; Val Acc=0.3969; Loss Att={'forw': '0.8093'}; Train Acc=0.440; Test Acc=0.3937; Entropy=0; Entropy_Test=\n",
      "\n",
      "20 2.0356119 2.1311924\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:17epoch=97; Loss Pred=2.0541; Val Loss=2.1312; Val Acc=0.4148; Loss Att={'forw': '0.8288'}; Train Acc=0.443; Test Acc=0.4125; Entropy=0; Entropy_Test=\n",
      "\n",
      "21 2.0356119 2.1225963\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:21epoch=98; Loss Pred=2.0642; Val Loss=2.1226; Val Acc=0.4097; Loss Att={'forw': '0.8183'}; Train Acc=0.434; Test Acc=0.4115; Entropy=0; Entropy_Test=\n",
      "\n",
      "22 2.0356119 2.1977997\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:27epoch=99; Loss Pred=2.1425; Val Loss=2.1978; Val Acc=0.3919; Loss Att={'forw': '0.8335'}; Train Acc=0.409; Test Acc=0.4056; Entropy=0; Entropy_Test=\n",
      "\n",
      "23 2.0356119 2.223111\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:31epoch=100; Loss Pred=2.1443; Val Loss=2.2231; Val Acc=0.3969; Loss Att={'forw': '0.8227'}; Train Acc=0.416; Test Acc=0.4072; Entropy=0; Entropy_Test=\n",
      "\n",
      "24 2.0356119 2.1241877\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:36epoch=101; Loss Pred=2.0206; Val Loss=2.1242; Val Acc=0.4046; Loss Att={'forw': '0.8247'}; Train Acc=0.453; Test Acc=0.4016; Entropy=0; Entropy_Test=\n",
      "\n",
      "25 2.0356119 2.134691\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:40epoch=102; Loss Pred=2.0499; Val Loss=2.1347; Val Acc=0.4020; Loss Att={'forw': '0.8258'}; Train Acc=0.451; Test Acc=0.4036; Entropy=0; Entropy_Test=\n",
      "\n",
      "26 2.0356119 2.0620375\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:45epoch=103; Loss Pred=1.9549; Val Loss=2.0620; Val Acc=0.4097; Loss Att={'forw': '0.8196'}; Train Acc=0.477; Test Acc=0.4035; Entropy=0; Entropy_Test=\n",
      "\n",
      "27 2.0356119 2.0598934\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:49epoch=104; Loss Pred=1.9667; Val Loss=2.0599; Val Acc=0.3893; Loss Att={'forw': '0.8231'}; Train Acc=0.458; Test Acc=0.4107; Entropy=0; Entropy_Test=\n",
      "\n",
      "28 2.0356119 2.0147216\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:55epoch=105; Loss Pred=1.9052; Val Loss=2.0147; Val Acc=0.4198; Loss Att={'forw': '0.8152'}; Train Acc=0.483; Test Acc=0.4101; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.0147216 2.0147347\n",
      "Logged Successfully: \n",
      "2018-07-16 14:37:59epoch=106; Loss Pred=1.8938; Val Loss=2.0147; Val Acc=0.4224; Loss Att={'forw': '0.8119'}; Train Acc=0.486; Test Acc=0.4129; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.0147216 2.0789099\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:05epoch=107; Loss Pred=1.9286; Val Loss=2.0789; Val Acc=0.4427; Loss Att={'forw': '0.8275'}; Train Acc=0.481; Test Acc=0.3947; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.0147216 2.0695593\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:09epoch=108; Loss Pred=1.9308; Val Loss=2.0696; Val Acc=0.4275; Loss Att={'forw': '0.8166'}; Train Acc=0.475; Test Acc=0.4001; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.0147216 2.7772603\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:15epoch=109; Loss Pred=2.6467; Val Loss=2.7773; Val Acc=0.2392; Loss Att={'forw': '0.8181'}; Train Acc=0.291; Test Acc=0.2597; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.0147216 2.7804546\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:19epoch=110; Loss Pred=2.6170; Val Loss=2.7805; Val Acc=0.2417; Loss Att={'forw': '0.8146'}; Train Acc=0.303; Test Acc=0.2674; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.0147216 2.509386\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:24epoch=111; Loss Pred=2.4901; Val Loss=2.5094; Val Acc=0.3613; Loss Att={'forw': '0.8600'}; Train Acc=0.374; Test Acc=0.3462; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.0147216 2.5121007\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:28epoch=112; Loss Pred=2.4926; Val Loss=2.5121; Val Acc=0.3588; Loss Att={'forw': '0.8664'}; Train Acc=0.366; Test Acc=0.3514; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.0147216 2.3782008\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:34epoch=113; Loss Pred=2.3152; Val Loss=2.3782; Val Acc=0.3562; Loss Att={'forw': '0.8895'}; Train Acc=0.349; Test Acc=0.3577; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.0147216 2.4288065\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:38epoch=114; Loss Pred=2.3149; Val Loss=2.4288; Val Acc=0.3206; Loss Att={'forw': '0.8833'}; Train Acc=0.358; Test Acc=0.3552; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.0147216 2.3305118\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:43epoch=115; Loss Pred=2.1833; Val Loss=2.3305; Val Acc=0.3562; Loss Att={'forw': '0.8336'}; Train Acc=0.406; Test Acc=0.3698; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.0147216 2.3167272\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:47epoch=116; Loss Pred=2.1777; Val Loss=2.3167; Val Acc=0.3511; Loss Att={'forw': '0.8311'}; Train Acc=0.411; Test Acc=0.3693; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.0147216 2.2718549\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:52epoch=117; Loss Pred=2.1759; Val Loss=2.2719; Val Acc=0.3791; Loss Att={'forw': '0.8278'}; Train Acc=0.413; Test Acc=0.3952; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.0147216 2.2494555\n",
      "Logged Successfully: \n",
      "2018-07-16 14:38:56epoch=118; Loss Pred=2.1772; Val Loss=2.2495; Val Acc=0.3740; Loss Att={'forw': '0.8190'}; Train Acc=0.410; Test Acc=0.3949; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.0147216 2.2636492\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:02epoch=119; Loss Pred=2.1431; Val Loss=2.2636; Val Acc=0.3740; Loss Att={'forw': '0.8304'}; Train Acc=0.423; Test Acc=0.3793; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.0147216 2.266784\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:06epoch=120; Loss Pred=2.1510; Val Loss=2.2668; Val Acc=0.3944; Loss Att={'forw': '0.8181'}; Train Acc=0.415; Test Acc=0.3767; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.0147216 2.2451172\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:11epoch=121; Loss Pred=2.1041; Val Loss=2.2451; Val Acc=0.3791; Loss Att={'forw': '0.8442'}; Train Acc=0.436; Test Acc=0.3861; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.0147216 2.2376962\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:15epoch=122; Loss Pred=2.0888; Val Loss=2.2377; Val Acc=0.3868; Loss Att={'forw': '0.8282'}; Train Acc=0.437; Test Acc=0.3843; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.0147216 2.2135613\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:21epoch=123; Loss Pred=2.0628; Val Loss=2.2136; Val Acc=0.3868; Loss Att={'forw': '0.8262'}; Train Acc=0.442; Test Acc=0.3859; Entropy=0; Entropy_Test=\n",
      "\n",
      "18 2.0147216 2.208785\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:25epoch=124; Loss Pred=2.0638; Val Loss=2.2088; Val Acc=0.3715; Loss Att={'forw': '0.8150'}; Train Acc=0.444; Test Acc=0.3871; Entropy=0; Entropy_Test=\n",
      "\n",
      "19 2.0147216 2.1896138\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:30epoch=125; Loss Pred=2.0356; Val Loss=2.1896; Val Acc=0.3868; Loss Att={'forw': '0.8207'}; Train Acc=0.449; Test Acc=0.3963; Entropy=0; Entropy_Test=\n",
      "\n",
      "20 2.0147216 2.1807854\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:34epoch=126; Loss Pred=2.0370; Val Loss=2.1808; Val Acc=0.3817; Loss Att={'forw': '0.8116'}; Train Acc=0.445; Test Acc=0.3907; Entropy=0; Entropy_Test=\n",
      "\n",
      "21 2.0147216 2.1483095\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:40epoch=127; Loss Pred=2.0163; Val Loss=2.1483; Val Acc=0.3893; Loss Att={'forw': '0.8144'}; Train Acc=0.458; Test Acc=0.3917; Entropy=0; Entropy_Test=\n",
      "\n",
      "22 2.0147216 2.1870606\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:44epoch=128; Loss Pred=2.0161; Val Loss=2.1871; Val Acc=0.4020; Loss Att={'forw': '0.8089'}; Train Acc=0.453; Test Acc=0.3953; Entropy=0; Entropy_Test=\n",
      "\n",
      "23 2.0147216 2.1408617\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:49epoch=129; Loss Pred=2.0150; Val Loss=2.1409; Val Acc=0.3919; Loss Att={'forw': '0.8157'}; Train Acc=0.454; Test Acc=0.3929; Entropy=0; Entropy_Test=\n",
      "\n",
      "24 2.0147216 2.1536248\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:53epoch=130; Loss Pred=2.0008; Val Loss=2.1536; Val Acc=0.3868; Loss Att={'forw': '0.8114'}; Train Acc=0.449; Test Acc=0.3902; Entropy=0; Entropy_Test=\n",
      "\n",
      "25 2.0147216 2.1122978\n",
      "Logged Successfully: \n",
      "2018-07-16 14:39:59epoch=131; Loss Pred=1.9983; Val Loss=2.1123; Val Acc=0.4173; Loss Att={'forw': '0.8087'}; Train Acc=0.472; Test Acc=0.3952; Entropy=0; Entropy_Test=\n",
      "\n",
      "26 2.0147216 2.1610773\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:03epoch=132; Loss Pred=2.0053; Val Loss=2.1611; Val Acc=0.4148; Loss Att={'forw': '0.8046'}; Train Acc=0.473; Test Acc=0.3968; Entropy=0; Entropy_Test=\n",
      "\n",
      "27 2.0147216 2.1693857\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:08epoch=133; Loss Pred=1.9640; Val Loss=2.1694; Val Acc=0.4122; Loss Att={'forw': '0.8244'}; Train Acc=0.481; Test Acc=0.4016; Entropy=0; Entropy_Test=\n",
      "\n",
      "28 2.0147216 2.155818\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:12epoch=134; Loss Pred=1.9923; Val Loss=2.1558; Val Acc=0.4351; Loss Att={'forw': '0.8224'}; Train Acc=0.466; Test Acc=0.3982; Entropy=0; Entropy_Test=\n",
      "\n",
      "29 2.0147216 2.1335227\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:18epoch=135; Loss Pred=1.9476; Val Loss=2.1335; Val Acc=0.4249; Loss Att={'forw': '0.8200'}; Train Acc=0.485; Test Acc=0.3936; Entropy=0; Entropy_Test=\n",
      "\n",
      "30 2.0147216 2.134099\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:22epoch=136; Loss Pred=1.9321; Val Loss=2.1341; Val Acc=0.4275; Loss Att={'forw': '0.8239'}; Train Acc=0.486; Test Acc=0.3898; Entropy=0; Entropy_Test=\n",
      "\n",
      "31 2.0147216 2.111785\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:27epoch=137; Loss Pred=1.9111; Val Loss=2.1118; Val Acc=0.4173; Loss Att={'forw': '0.8283'}; Train Acc=0.495; Test Acc=0.4068; Entropy=0; Entropy_Test=\n",
      "\n",
      "32 2.0147216 2.117847\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:31epoch=138; Loss Pred=1.9141; Val Loss=2.1178; Val Acc=0.4402; Loss Att={'forw': '0.8223'}; Train Acc=0.493; Test Acc=0.4077; Entropy=0; Entropy_Test=\n",
      "\n",
      "33 2.0147216 2.0866416\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:36epoch=139; Loss Pred=1.8912; Val Loss=2.0866; Val Acc=0.4249; Loss Att={'forw': '0.8205'}; Train Acc=0.495; Test Acc=0.4076; Entropy=0; Entropy_Test=\n",
      "\n",
      "34 2.0147216 2.0802374\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:40epoch=140; Loss Pred=1.8835; Val Loss=2.0802; Val Acc=0.4351; Loss Att={'forw': '0.8271'}; Train Acc=0.505; Test Acc=0.4068; Entropy=0; Entropy_Test=\n",
      "\n",
      "35 2.0147216 2.0941627\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:46epoch=141; Loss Pred=1.8594; Val Loss=2.0942; Val Acc=0.4275; Loss Att={'forw': '0.8078'}; Train Acc=0.506; Test Acc=0.4083; Entropy=0; Entropy_Test=\n",
      "\n",
      "36 2.0147216 2.1064327\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:50epoch=142; Loss Pred=1.8685; Val Loss=2.1064; Val Acc=0.4224; Loss Att={'forw': '0.8104'}; Train Acc=0.511; Test Acc=0.4002; Entropy=0; Entropy_Test=\n",
      "\n",
      "37 2.0147216 2.1092532\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:55epoch=143; Loss Pred=1.8370; Val Loss=2.1093; Val Acc=0.4377; Loss Att={'forw': '0.8074'}; Train Acc=0.509; Test Acc=0.4134; Entropy=0; Entropy_Test=\n",
      "\n",
      "38 2.0147216 2.0796096\n",
      "Logged Successfully: \n",
      "2018-07-16 14:40:59epoch=144; Loss Pred=1.8497; Val Loss=2.0796; Val Acc=0.4733; Loss Att={'forw': '0.8109'}; Train Acc=0.501; Test Acc=0.4103; Entropy=0; Entropy_Test=\n",
      "\n",
      "39 2.0147216 2.12447\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:05epoch=145; Loss Pred=1.8397; Val Loss=2.1245; Val Acc=0.4097; Loss Att={'forw': '0.8103'}; Train Acc=0.509; Test Acc=0.4021; Entropy=0; Entropy_Test=\n",
      "\n",
      "40 2.0147216 2.1150992\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:09epoch=146; Loss Pred=1.8484; Val Loss=2.1151; Val Acc=0.4224; Loss Att={'forw': '0.8102'}; Train Acc=0.509; Test Acc=0.4015; Entropy=0; Entropy_Test=\n",
      "\n",
      "41 2.0147216 2.15479\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:15epoch=147; Loss Pred=1.8768; Val Loss=2.1548; Val Acc=0.4249; Loss Att={'forw': '0.8176'}; Train Acc=0.507; Test Acc=0.3973; Entropy=0; Entropy_Test=\n",
      "\n",
      "42 2.0147216 2.174664\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:19epoch=148; Loss Pred=1.9107; Val Loss=2.1747; Val Acc=0.3842; Loss Att={'forw': '0.8093'}; Train Acc=0.508; Test Acc=0.3865; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 2.0147216 2.2411437\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:24epoch=149; Loss Pred=1.9978; Val Loss=2.2411; Val Acc=0.3766; Loss Att={'forw': '0.8225'}; Train Acc=0.453; Test Acc=0.3613; Entropy=0; Entropy_Test=\n",
      "\n",
      "44 2.0147216 2.2268734\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:28epoch=150; Loss Pred=1.9858; Val Loss=2.2269; Val Acc=0.3715; Loss Att={'forw': '0.8308'}; Train Acc=0.466; Test Acc=0.3701; Entropy=0; Entropy_Test=\n",
      "\n",
      "45 2.0147216 2.1660917\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:33epoch=151; Loss Pred=1.8707; Val Loss=2.1661; Val Acc=0.3944; Loss Att={'forw': '0.8202'}; Train Acc=0.503; Test Acc=0.3908; Entropy=0; Entropy_Test=\n",
      "\n",
      "46 2.0147216 2.177804\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:37epoch=152; Loss Pred=1.8785; Val Loss=2.1778; Val Acc=0.4046; Loss Att={'forw': '0.8305'}; Train Acc=0.496; Test Acc=0.3950; Entropy=0; Entropy_Test=\n",
      "\n",
      "47 2.0147216 2.1382914\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:42epoch=153; Loss Pred=1.7995; Val Loss=2.1383; Val Acc=0.4326; Loss Att={'forw': '0.8294'}; Train Acc=0.520; Test Acc=0.4089; Entropy=0; Entropy_Test=\n",
      "\n",
      "48 2.0147216 2.1435373\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:47epoch=154; Loss Pred=1.7982; Val Loss=2.1435; Val Acc=0.4656; Loss Att={'forw': '0.8273'}; Train Acc=0.526; Test Acc=0.4078; Entropy=0; Entropy_Test=\n",
      "\n",
      "49 2.0147216 2.0971231\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:52epoch=155; Loss Pred=1.7835; Val Loss=2.0971; Val Acc=0.4580; Loss Att={'forw': '0.8393'}; Train Acc=0.524; Test Acc=0.4129; Entropy=0; Entropy_Test=\n",
      "\n",
      "50 2.0147216 2.0980747\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 157\n",
      "Optimization Finished!\n",
      "********** replication  4  **********\n",
      "topic_classification\n",
      "((11228, 300), (11228, 1))\n",
      "0.0\n",
      "(1965, 300) (1965, 1) (393, 300) (393, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: topic_classification\n",
      "    hid: \t\t\t150,\n",
      "    h_hid: \t\t\t300\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t1965/2245, SEQ_LEN: 300\n",
      "Logged Successfully: \n",
      "0 10000000000.0 3.8928487\n",
      "Logged Successfully: \n",
      "2018-07-16 14:41:59epoch=0; Loss Pred=3.8988; Val Loss=3.8928; Val Acc=0.0051; Loss Att={'forw': '1.1079'}; Train Acc=0.010; Test Acc=0.0065; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 3.8928487 2.6922617\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:04epoch=1; Loss Pred=2.6007; Val Loss=2.6923; Val Acc=0.3206; Loss Att={'forw': '1.0187'}; Train Acc=0.326; Test Acc=0.3571; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.6922617 2.7094522\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:09epoch=2; Loss Pred=2.6058; Val Loss=2.7095; Val Acc=0.3206; Loss Att={'forw': '0.8342'}; Train Acc=0.326; Test Acc=0.3555; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.6922617 2.656676\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:14epoch=3; Loss Pred=2.5305; Val Loss=2.6567; Val Acc=0.2392; Loss Att={'forw': '1.1972'}; Train Acc=0.232; Test Acc=0.2437; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.656676 2.6402879\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:18epoch=4; Loss Pred=2.5288; Val Loss=2.6403; Val Acc=0.2595; Loss Att={'forw': '0.8594'}; Train Acc=0.272; Test Acc=0.2765; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.6402879 2.6128118\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:23epoch=5; Loss Pred=2.4945; Val Loss=2.6128; Val Acc=0.3206; Loss Att={'forw': '0.8336'}; Train Acc=0.325; Test Acc=0.3555; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.6128118 2.6008053\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:28epoch=6; Loss Pred=2.4917; Val Loss=2.6008; Val Acc=0.3206; Loss Att={'forw': '0.8128'}; Train Acc=0.325; Test Acc=0.3574; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.6008053 2.5689342\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:33epoch=7; Loss Pred=2.4793; Val Loss=2.5689; Val Acc=0.3155; Loss Att={'forw': '0.9176'}; Train Acc=0.312; Test Acc=0.3492; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.5689342 2.5782359\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:38epoch=8; Loss Pred=2.4755; Val Loss=2.5782; Val Acc=0.3257; Loss Att={'forw': '0.8844'}; Train Acc=0.322; Test Acc=0.3595; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.5689342 2.575513\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:43epoch=9; Loss Pred=2.4599; Val Loss=2.5755; Val Acc=0.3079; Loss Att={'forw': '0.8907'}; Train Acc=0.327; Test Acc=0.3551; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.5689342 2.5794594\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:47epoch=10; Loss Pred=2.4645; Val Loss=2.5795; Val Acc=0.3181; Loss Att={'forw': '0.8633'}; Train Acc=0.328; Test Acc=0.3563; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.5689342 2.5451157\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:52epoch=11; Loss Pred=2.4311; Val Loss=2.5451; Val Acc=0.3359; Loss Att={'forw': '0.8664'}; Train Acc=0.340; Test Acc=0.3663; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.5451157 2.557852\n",
      "Logged Successfully: \n",
      "2018-07-16 14:42:57epoch=12; Loss Pred=2.4379; Val Loss=2.5579; Val Acc=0.3232; Loss Att={'forw': '0.8469'}; Train Acc=0.341; Test Acc=0.3601; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.5451157 2.5090652\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:02epoch=13; Loss Pred=2.3943; Val Loss=2.5091; Val Acc=0.3588; Loss Att={'forw': '0.8969'}; Train Acc=0.365; Test Acc=0.3749; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.5090652 2.5017622\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:07epoch=14; Loss Pred=2.4079; Val Loss=2.5018; Val Acc=0.3817; Loss Att={'forw': '0.8842'}; Train Acc=0.353; Test Acc=0.3795; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.5017622 2.499871\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:12epoch=15; Loss Pred=2.3884; Val Loss=2.4999; Val Acc=0.3588; Loss Att={'forw': '0.8689'}; Train Acc=0.363; Test Acc=0.3657; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.499871 2.5127902\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:16epoch=16; Loss Pred=2.3843; Val Loss=2.5128; Val Acc=0.3639; Loss Att={'forw': '0.8306'}; Train Acc=0.361; Test Acc=0.3677; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.499871 2.5258474\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:22epoch=17; Loss Pred=2.3982; Val Loss=2.5258; Val Acc=0.3537; Loss Att={'forw': '0.9838'}; Train Acc=0.349; Test Acc=0.3561; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.499871 2.5265312\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:26epoch=18; Loss Pred=2.3877; Val Loss=2.5265; Val Acc=0.3486; Loss Att={'forw': '0.8734'}; Train Acc=0.345; Test Acc=0.3556; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.499871 2.4918644\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:32epoch=19; Loss Pred=2.3395; Val Loss=2.4919; Val Acc=0.3511; Loss Att={'forw': '0.8880'}; Train Acc=0.346; Test Acc=0.3801; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4918644 2.5006943\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:36epoch=20; Loss Pred=2.3397; Val Loss=2.5007; Val Acc=0.3384; Loss Att={'forw': '0.8693'}; Train Acc=0.351; Test Acc=0.3779; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4918644 2.4679136\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:41epoch=21; Loss Pred=2.3152; Val Loss=2.4679; Val Acc=0.3639; Loss Att={'forw': '0.8775'}; Train Acc=0.367; Test Acc=0.3928; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4679136 2.4887922\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:45epoch=22; Loss Pred=2.3198; Val Loss=2.4888; Val Acc=0.3817; Loss Att={'forw': '0.8625'}; Train Acc=0.371; Test Acc=0.3942; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4679136 2.4718955\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:51epoch=23; Loss Pred=2.2814; Val Loss=2.4719; Val Acc=0.3791; Loss Att={'forw': '0.8431'}; Train Acc=0.385; Test Acc=0.4020; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.4679136 2.4722116\n",
      "Logged Successfully: \n",
      "2018-07-16 14:43:55epoch=24; Loss Pred=2.2797; Val Loss=2.4722; Val Acc=0.3817; Loss Att={'forw': '0.8355'}; Train Acc=0.385; Test Acc=0.4018; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.4679136 2.4620566\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:00epoch=25; Loss Pred=2.2795; Val Loss=2.4621; Val Acc=0.3842; Loss Att={'forw': '0.8178'}; Train Acc=0.387; Test Acc=0.3948; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4620566 2.4590015\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:05epoch=26; Loss Pred=2.2827; Val Loss=2.4590; Val Acc=0.3766; Loss Att={'forw': '0.8063'}; Train Acc=0.381; Test Acc=0.3883; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4590015 2.4264212\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:10epoch=27; Loss Pred=2.2551; Val Loss=2.4264; Val Acc=0.3919; Loss Att={'forw': '0.8088'}; Train Acc=0.386; Test Acc=0.3917; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4264212 2.4223216\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:15epoch=28; Loss Pred=2.2468; Val Loss=2.4223; Val Acc=0.4020; Loss Att={'forw': '0.7982'}; Train Acc=0.392; Test Acc=0.3920; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.4223216 2.4208555\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:20epoch=29; Loss Pred=2.2414; Val Loss=2.4209; Val Acc=0.3817; Loss Att={'forw': '0.7949'}; Train Acc=0.388; Test Acc=0.3895; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4208555 2.4183574\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:24epoch=30; Loss Pred=2.2438; Val Loss=2.4184; Val Acc=0.3766; Loss Att={'forw': '0.7881'}; Train Acc=0.393; Test Acc=0.3928; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4183574 2.4179933\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:30epoch=31; Loss Pred=2.2234; Val Loss=2.4180; Val Acc=0.3969; Loss Att={'forw': '0.8081'}; Train Acc=0.395; Test Acc=0.3920; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4179933 2.4119966\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:34epoch=32; Loss Pred=2.2148; Val Loss=2.4120; Val Acc=0.3817; Loss Att={'forw': '0.7998'}; Train Acc=0.399; Test Acc=0.3958; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4119966 2.3763633\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:40epoch=33; Loss Pred=2.1959; Val Loss=2.3764; Val Acc=0.3791; Loss Att={'forw': '0.8227'}; Train Acc=0.408; Test Acc=0.3943; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3763633 2.3778756\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:44epoch=34; Loss Pred=2.1951; Val Loss=2.3779; Val Acc=0.3919; Loss Att={'forw': '0.8019'}; Train Acc=0.394; Test Acc=0.3898; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3763633 2.4217165\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:49epoch=35; Loss Pred=2.1943; Val Loss=2.4217; Val Acc=0.3766; Loss Att={'forw': '0.8372'}; Train Acc=0.400; Test Acc=0.4044; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3763633 2.4135458\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:53epoch=36; Loss Pred=2.1922; Val Loss=2.4135; Val Acc=0.3639; Loss Att={'forw': '0.8414'}; Train Acc=0.404; Test Acc=0.4069; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.3763633 2.3553767\n",
      "Logged Successfully: \n",
      "2018-07-16 14:44:58epoch=37; Loss Pred=2.1555; Val Loss=2.3554; Val Acc=0.3969; Loss Att={'forw': '0.8295'}; Train Acc=0.415; Test Acc=0.4073; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3553767 2.3554251\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:03epoch=38; Loss Pred=2.1550; Val Loss=2.3554; Val Acc=0.3842; Loss Att={'forw': '0.8298'}; Train Acc=0.412; Test Acc=0.4057; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3553767 2.39353\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:08epoch=39; Loss Pred=2.1714; Val Loss=2.3935; Val Acc=0.3817; Loss Att={'forw': '0.8351'}; Train Acc=0.398; Test Acc=0.4034; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3553767 2.4123292\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:12epoch=40; Loss Pred=2.1914; Val Loss=2.4123; Val Acc=0.3740; Loss Att={'forw': '0.8202'}; Train Acc=0.399; Test Acc=0.4036; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.3553767 2.4506316\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:17epoch=41; Loss Pred=2.2063; Val Loss=2.4506; Val Acc=0.3435; Loss Att={'forw': '0.8405'}; Train Acc=0.367; Test Acc=0.3801; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.3553767 2.4597263\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:22epoch=42; Loss Pred=2.2265; Val Loss=2.4597; Val Acc=0.3562; Loss Att={'forw': '0.8075'}; Train Acc=0.360; Test Acc=0.3796; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.3553767 2.37072\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:27epoch=43; Loss Pred=2.1637; Val Loss=2.3707; Val Acc=0.3842; Loss Att={'forw': '0.8396'}; Train Acc=0.408; Test Acc=0.4015; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.3553767 2.3727217\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:32epoch=44; Loss Pred=2.1662; Val Loss=2.3727; Val Acc=0.3715; Loss Att={'forw': '0.8204'}; Train Acc=0.408; Test Acc=0.4088; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.3553767 2.373011\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:37epoch=45; Loss Pred=2.1798; Val Loss=2.3730; Val Acc=0.3715; Loss Att={'forw': '0.8314'}; Train Acc=0.394; Test Acc=0.3909; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.3553767 2.387672\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:41epoch=46; Loss Pred=2.1730; Val Loss=2.3877; Val Acc=0.3817; Loss Att={'forw': '0.8253'}; Train Acc=0.404; Test Acc=0.3969; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.3553767 2.3653274\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:46epoch=47; Loss Pred=2.1355; Val Loss=2.3653; Val Acc=0.3868; Loss Att={'forw': '0.8254'}; Train Acc=0.401; Test Acc=0.4045; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.3553767 2.3743434\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:51epoch=48; Loss Pred=2.1458; Val Loss=2.3743; Val Acc=0.3842; Loss Att={'forw': '0.8169'}; Train Acc=0.405; Test Acc=0.3972; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.3553767 2.3933315\n",
      "Logged Successfully: \n",
      "2018-07-16 14:45:56epoch=49; Loss Pred=2.1431; Val Loss=2.3933; Val Acc=0.3919; Loss Att={'forw': '0.8194'}; Train Acc=0.415; Test Acc=0.4052; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.3553767 2.4320202\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:00epoch=50; Loss Pred=2.1634; Val Loss=2.4320; Val Acc=0.3919; Loss Att={'forw': '0.8128'}; Train Acc=0.414; Test Acc=0.3986; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.3553767 2.3600762\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:05epoch=51; Loss Pred=2.0789; Val Loss=2.3601; Val Acc=0.3842; Loss Att={'forw': '0.8121'}; Train Acc=0.436; Test Acc=0.4073; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.3553767 2.3347704\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:09epoch=52; Loss Pred=2.0834; Val Loss=2.3348; Val Acc=0.3969; Loss Att={'forw': '0.8071'}; Train Acc=0.429; Test Acc=0.4092; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3347704 2.364217\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:15epoch=53; Loss Pred=2.1406; Val Loss=2.3642; Val Acc=0.3944; Loss Att={'forw': '0.8275'}; Train Acc=0.431; Test Acc=0.4078; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3347704 2.3500774\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:19epoch=54; Loss Pred=2.1356; Val Loss=2.3501; Val Acc=0.4071; Loss Att={'forw': '0.8088'}; Train Acc=0.434; Test Acc=0.4079; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3347704 2.3665128\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:25epoch=55; Loss Pred=2.0761; Val Loss=2.3665; Val Acc=0.3562; Loss Att={'forw': '0.8149'}; Train Acc=0.438; Test Acc=0.4033; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.3347704 2.362722\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:29epoch=56; Loss Pred=2.0743; Val Loss=2.3627; Val Acc=0.3868; Loss Att={'forw': '0.8116'}; Train Acc=0.429; Test Acc=0.4051; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.3347704 2.335433\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:35epoch=57; Loss Pred=2.0483; Val Loss=2.3354; Val Acc=0.4020; Loss Att={'forw': '0.8075'}; Train Acc=0.448; Test Acc=0.4144; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.3347704 2.335434\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:39epoch=58; Loss Pred=2.0417; Val Loss=2.3354; Val Acc=0.4071; Loss Att={'forw': '0.8026'}; Train Acc=0.444; Test Acc=0.4164; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.3347704 2.3091075\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:44epoch=59; Loss Pred=2.0354; Val Loss=2.3091; Val Acc=0.4046; Loss Att={'forw': '0.8124'}; Train Acc=0.451; Test Acc=0.4188; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3091075 2.3345602\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:48epoch=60; Loss Pred=2.0351; Val Loss=2.3346; Val Acc=0.3842; Loss Att={'forw': '0.8090'}; Train Acc=0.445; Test Acc=0.4032; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3091075 2.3484426\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:54epoch=61; Loss Pred=2.0368; Val Loss=2.3484; Val Acc=0.3740; Loss Att={'forw': '0.8218'}; Train Acc=0.444; Test Acc=0.3964; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3091075 2.3365307\n",
      "Logged Successfully: \n",
      "2018-07-16 14:46:58epoch=62; Loss Pred=2.0203; Val Loss=2.3365; Val Acc=0.3715; Loss Att={'forw': '0.8105'}; Train Acc=0.443; Test Acc=0.4001; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.3091075 2.3267283\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:03epoch=63; Loss Pred=2.0210; Val Loss=2.3267; Val Acc=0.3715; Loss Att={'forw': '0.8139'}; Train Acc=0.446; Test Acc=0.3955; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.3091075 2.338683\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:07epoch=64; Loss Pred=2.0357; Val Loss=2.3387; Val Acc=0.3664; Loss Att={'forw': '0.8117'}; Train Acc=0.446; Test Acc=0.3967; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.3091075 2.3142056\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:13epoch=65; Loss Pred=2.0230; Val Loss=2.3142; Val Acc=0.3817; Loss Att={'forw': '0.8087'}; Train Acc=0.431; Test Acc=0.3934; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.3091075 2.311382\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:17epoch=66; Loss Pred=2.0244; Val Loss=2.3114; Val Acc=0.3995; Loss Att={'forw': '0.8040'}; Train Acc=0.436; Test Acc=0.3975; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.3091075 2.338214\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:22epoch=67; Loss Pred=1.9967; Val Loss=2.3382; Val Acc=0.3868; Loss Att={'forw': '0.8249'}; Train Acc=0.456; Test Acc=0.4053; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.3091075 2.31367\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:27epoch=68; Loss Pred=1.9932; Val Loss=2.3137; Val Acc=0.3791; Loss Att={'forw': '0.8130'}; Train Acc=0.454; Test Acc=0.4036; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.3091075 2.3708847\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:32epoch=69; Loss Pred=2.0642; Val Loss=2.3709; Val Acc=0.3537; Loss Att={'forw': '0.8240'}; Train Acc=0.416; Test Acc=0.3775; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.3091075 2.3767502\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:36epoch=70; Loss Pred=2.0423; Val Loss=2.3768; Val Acc=0.3511; Loss Att={'forw': '0.8186'}; Train Acc=0.428; Test Acc=0.3809; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.3091075 2.4011106\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:42epoch=71; Loss Pred=2.0584; Val Loss=2.4011; Val Acc=0.3766; Loss Att={'forw': '0.8574'}; Train Acc=0.429; Test Acc=0.3769; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.3091075 2.3987913\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:46epoch=72; Loss Pred=2.0804; Val Loss=2.3988; Val Acc=0.3486; Loss Att={'forw': '0.8433'}; Train Acc=0.431; Test Acc=0.3652; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.3091075 2.3880136\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:51epoch=73; Loss Pred=2.0956; Val Loss=2.3880; Val Acc=0.3639; Loss Att={'forw': '0.8361'}; Train Acc=0.415; Test Acc=0.3750; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.3091075 2.3779976\n",
      "Logged Successfully: \n",
      "2018-07-16 14:47:56epoch=74; Loss Pred=2.0888; Val Loss=2.3780; Val Acc=0.3868; Loss Att={'forw': '0.8273'}; Train Acc=0.413; Test Acc=0.3710; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.3091075 2.4025233\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:01epoch=75; Loss Pred=1.9900; Val Loss=2.4025; Val Acc=0.3511; Loss Att={'forw': '0.8545'}; Train Acc=0.429; Test Acc=0.3925; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.3091075 2.4274836\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:05epoch=76; Loss Pred=1.9963; Val Loss=2.4275; Val Acc=0.3282; Loss Att={'forw': '0.8440'}; Train Acc=0.423; Test Acc=0.3863; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.3091075 2.2874386\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:11epoch=77; Loss Pred=1.9473; Val Loss=2.2874; Val Acc=0.3893; Loss Att={'forw': '0.8438'}; Train Acc=0.466; Test Acc=0.4036; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2874386 2.3079493\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:15epoch=78; Loss Pred=1.9487; Val Loss=2.3079; Val Acc=0.3842; Loss Att={'forw': '0.8283'}; Train Acc=0.468; Test Acc=0.4018; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2874386 2.2625663\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:20epoch=79; Loss Pred=1.9234; Val Loss=2.2626; Val Acc=0.4122; Loss Att={'forw': '0.8408'}; Train Acc=0.480; Test Acc=0.4267; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2625663 2.252908\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:25epoch=80; Loss Pred=1.9313; Val Loss=2.2529; Val Acc=0.3995; Loss Att={'forw': '0.8383'}; Train Acc=0.476; Test Acc=0.4222; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.252908 2.309779\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:30epoch=81; Loss Pred=1.9485; Val Loss=2.3098; Val Acc=0.3995; Loss Att={'forw': '0.8355'}; Train Acc=0.463; Test Acc=0.4130; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.252908 2.3024437\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:34epoch=82; Loss Pred=1.9549; Val Loss=2.3024; Val Acc=0.3919; Loss Att={'forw': '0.8250'}; Train Acc=0.466; Test Acc=0.4142; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.252908 2.3085675\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:39epoch=83; Loss Pred=1.9077; Val Loss=2.3086; Val Acc=0.4020; Loss Att={'forw': '0.8494'}; Train Acc=0.483; Test Acc=0.4229; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.252908 2.333957\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:44epoch=84; Loss Pred=1.9085; Val Loss=2.3340; Val Acc=0.4020; Loss Att={'forw': '0.8391'}; Train Acc=0.482; Test Acc=0.4232; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.252908 2.2883077\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:49epoch=85; Loss Pred=1.8739; Val Loss=2.2883; Val Acc=0.4173; Loss Att={'forw': '0.8393'}; Train Acc=0.497; Test Acc=0.4407; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.252908 2.2749898\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:54epoch=86; Loss Pred=1.8754; Val Loss=2.2750; Val Acc=0.4198; Loss Att={'forw': '0.8335'}; Train Acc=0.499; Test Acc=0.4324; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.252908 2.3108218\n",
      "Logged Successfully: \n",
      "2018-07-16 14:48:59epoch=87; Loss Pred=1.8996; Val Loss=2.3108; Val Acc=0.4020; Loss Att={'forw': '0.8233'}; Train Acc=0.476; Test Acc=0.4076; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.252908 2.2558658\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:03epoch=88; Loss Pred=1.8819; Val Loss=2.2559; Val Acc=0.4097; Loss Att={'forw': '0.8233'}; Train Acc=0.499; Test Acc=0.4166; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.252908 2.3281376\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:09epoch=89; Loss Pred=1.8969; Val Loss=2.3281; Val Acc=0.3944; Loss Att={'forw': '0.8321'}; Train Acc=0.482; Test Acc=0.4135; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.252908 2.2983513\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:13epoch=90; Loss Pred=1.9085; Val Loss=2.2984; Val Acc=0.3842; Loss Att={'forw': '0.8279'}; Train Acc=0.484; Test Acc=0.4133; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.252908 2.3344073\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:18epoch=91; Loss Pred=1.8297; Val Loss=2.3344; Val Acc=0.3969; Loss Att={'forw': '0.8326'}; Train Acc=0.502; Test Acc=0.4209; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.252908 2.34967\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:23epoch=92; Loss Pred=1.8337; Val Loss=2.3497; Val Acc=0.3944; Loss Att={'forw': '0.8287'}; Train Acc=0.506; Test Acc=0.4214; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.252908 2.4105418\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:28epoch=93; Loss Pred=2.1264; Val Loss=2.4105; Val Acc=0.4173; Loss Att={'forw': '0.8364'}; Train Acc=0.459; Test Acc=0.4320; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.252908 2.4207237\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:32epoch=94; Loss Pred=2.1124; Val Loss=2.4207; Val Acc=0.4173; Loss Att={'forw': '0.8284'}; Train Acc=0.454; Test Acc=0.4300; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.252908 2.9312835\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:38epoch=95; Loss Pred=2.8335; Val Loss=2.9313; Val Acc=0.2188; Loss Att={'forw': '1.0817'}; Train Acc=0.208; Test Acc=0.2141; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.252908 2.9471126\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:42epoch=96; Loss Pred=2.8450; Val Loss=2.9471; Val Acc=0.2061; Loss Att={'forw': '0.9921'}; Train Acc=0.212; Test Acc=0.2172; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.252908 2.999057\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:47epoch=97; Loss Pred=2.8925; Val Loss=2.9991; Val Acc=0.2010; Loss Att={'forw': '1.0293'}; Train Acc=0.191; Test Acc=0.1831; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.252908 3.0408974\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:52epoch=98; Loss Pred=2.9389; Val Loss=3.0409; Val Acc=0.1858; Loss Att={'forw': '0.9638'}; Train Acc=0.203; Test Acc=0.1839; Entropy=0; Entropy_Test=\n",
      "\n",
      "18 2.252908 2.6612387\n",
      "Logged Successfully: \n",
      "2018-07-16 14:49:57epoch=99; Loss Pred=2.5494; Val Loss=2.6612; Val Acc=0.2799; Loss Att={'forw': '0.9769'}; Train Acc=0.267; Test Acc=0.2872; Entropy=0; Entropy_Test=\n",
      "\n",
      "19 2.252908 2.6385608\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:01epoch=100; Loss Pred=2.5353; Val Loss=2.6386; Val Acc=0.2646; Loss Att={'forw': '0.9106'}; Train Acc=0.272; Test Acc=0.2919; Entropy=0; Entropy_Test=\n",
      "\n",
      "20 2.252908 2.8469083\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:07epoch=101; Loss Pred=2.7178; Val Loss=2.8469; Val Acc=0.3028; Loss Att={'forw': '0.8903'}; Train Acc=0.305; Test Acc=0.3304; Entropy=0; Entropy_Test=\n",
      "\n",
      "21 2.252908 2.8391893\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:11epoch=102; Loss Pred=2.7400; Val Loss=2.8392; Val Acc=0.3155; Loss Att={'forw': '0.8603'}; Train Acc=0.318; Test Acc=0.3310; Entropy=0; Entropy_Test=\n",
      "\n",
      "22 2.252908 2.8272488\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:16epoch=103; Loss Pred=2.7029; Val Loss=2.8272; Val Acc=0.2036; Loss Att={'forw': '0.8323'}; Train Acc=0.213; Test Acc=0.2341; Entropy=0; Entropy_Test=\n",
      "\n",
      "23 2.252908 2.821801\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:20epoch=104; Loss Pred=2.7087; Val Loss=2.8218; Val Acc=0.1654; Loss Att={'forw': '0.7897'}; Train Acc=0.208; Test Acc=0.2218; Entropy=0; Entropy_Test=\n",
      "\n",
      "24 2.252908 2.6816666\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:26epoch=105; Loss Pred=2.6093; Val Loss=2.6817; Val Acc=0.3181; Loss Att={'forw': '0.8117'}; Train Acc=0.315; Test Acc=0.3520; Entropy=0; Entropy_Test=\n",
      "\n",
      "25 2.252908 2.720432\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:30epoch=106; Loss Pred=2.6087; Val Loss=2.7204; Val Acc=0.2977; Loss Att={'forw': '0.7940'}; Train Acc=0.321; Test Acc=0.3460; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 2.252908 2.6561353\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:35epoch=107; Loss Pred=2.5282; Val Loss=2.6561; Val Acc=0.3333; Loss Att={'forw': '0.8031'}; Train Acc=0.315; Test Acc=0.3450; Entropy=0; Entropy_Test=\n",
      "\n",
      "27 2.252908 2.618278\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:40epoch=108; Loss Pred=2.5216; Val Loss=2.6183; Val Acc=0.3104; Loss Att={'forw': '0.7879'}; Train Acc=0.328; Test Acc=0.3556; Entropy=0; Entropy_Test=\n",
      "\n",
      "28 2.252908 2.6244223\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:45epoch=109; Loss Pred=2.5349; Val Loss=2.6244; Val Acc=0.3104; Loss Att={'forw': '0.7881'}; Train Acc=0.315; Test Acc=0.3497; Entropy=0; Entropy_Test=\n",
      "\n",
      "29 2.252908 2.6135159\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:49epoch=110; Loss Pred=2.5240; Val Loss=2.6135; Val Acc=0.3232; Loss Att={'forw': '0.7793'}; Train Acc=0.319; Test Acc=0.3499; Entropy=0; Entropy_Test=\n",
      "\n",
      "30 2.252908 2.6501052\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:55epoch=111; Loss Pred=2.5383; Val Loss=2.6501; Val Acc=0.3079; Loss Att={'forw': '0.7781'}; Train Acc=0.314; Test Acc=0.3546; Entropy=0; Entropy_Test=\n",
      "\n",
      "31 2.252908 2.6243734\n",
      "Logged Successfully: \n",
      "2018-07-16 14:50:59epoch=112; Loss Pred=2.5367; Val Loss=2.6244; Val Acc=0.3104; Loss Att={'forw': '0.7732'}; Train Acc=0.314; Test Acc=0.3466; Entropy=0; Entropy_Test=\n",
      "\n",
      "32 2.252908 2.667383\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:04epoch=113; Loss Pred=2.5595; Val Loss=2.6674; Val Acc=0.3181; Loss Att={'forw': '0.7709'}; Train Acc=0.301; Test Acc=0.3334; Entropy=0; Entropy_Test=\n",
      "\n",
      "33 2.252908 2.7138097\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:09epoch=114; Loss Pred=2.5600; Val Loss=2.7138; Val Acc=0.2952; Loss Att={'forw': '0.7653'}; Train Acc=0.299; Test Acc=0.3288; Entropy=0; Entropy_Test=\n",
      "\n",
      "34 2.252908 2.718557\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:14epoch=115; Loss Pred=2.5581; Val Loss=2.7186; Val Acc=0.2875; Loss Att={'forw': '0.7633'}; Train Acc=0.305; Test Acc=0.3373; Entropy=0; Entropy_Test=\n",
      "\n",
      "35 2.252908 2.7258487\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:18epoch=116; Loss Pred=2.5703; Val Loss=2.7258; Val Acc=0.2875; Loss Att={'forw': '0.7575'}; Train Acc=0.303; Test Acc=0.3257; Entropy=0; Entropy_Test=\n",
      "\n",
      "36 2.252908 2.627819\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:23epoch=117; Loss Pred=2.5121; Val Loss=2.6278; Val Acc=0.3053; Loss Att={'forw': '0.7540'}; Train Acc=0.319; Test Acc=0.3437; Entropy=0; Entropy_Test=\n",
      "\n",
      "37 2.252908 2.636933\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:28epoch=118; Loss Pred=2.4926; Val Loss=2.6369; Val Acc=0.2977; Loss Att={'forw': '0.7490'}; Train Acc=0.321; Test Acc=0.3394; Entropy=0; Entropy_Test=\n",
      "\n",
      "38 2.252908 2.5931807\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:33epoch=119; Loss Pred=2.4829; Val Loss=2.5932; Val Acc=0.3130; Loss Att={'forw': '0.7451'}; Train Acc=0.326; Test Acc=0.3514; Entropy=0; Entropy_Test=\n",
      "\n",
      "39 2.252908 2.6154208\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:38epoch=120; Loss Pred=2.4959; Val Loss=2.6154; Val Acc=0.3130; Loss Att={'forw': '0.7421'}; Train Acc=0.328; Test Acc=0.3489; Entropy=0; Entropy_Test=\n",
      "\n",
      "40 2.252908 2.7028458\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:43epoch=121; Loss Pred=2.5900; Val Loss=2.7028; Val Acc=0.2621; Loss Att={'forw': '0.7552'}; Train Acc=0.284; Test Acc=0.2999; Entropy=0; Entropy_Test=\n",
      "\n",
      "41 2.252908 2.6979444\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:47epoch=122; Loss Pred=2.5743; Val Loss=2.6979; Val Acc=0.2646; Loss Att={'forw': '0.7525'}; Train Acc=0.288; Test Acc=0.2956; Entropy=0; Entropy_Test=\n",
      "\n",
      "42 2.252908 2.6121798\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:52epoch=123; Loss Pred=2.4656; Val Loss=2.6122; Val Acc=0.3282; Loss Att={'forw': '0.7370'}; Train Acc=0.326; Test Acc=0.3576; Entropy=0; Entropy_Test=\n",
      "\n",
      "43 2.252908 2.59073\n",
      "Logged Successfully: \n",
      "2018-07-16 14:51:56epoch=124; Loss Pred=2.4628; Val Loss=2.5907; Val Acc=0.3232; Loss Att={'forw': '0.7325'}; Train Acc=0.338; Test Acc=0.3628; Entropy=0; Entropy_Test=\n",
      "\n",
      "44 2.252908 2.573123\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:01epoch=125; Loss Pred=2.4490; Val Loss=2.5731; Val Acc=0.3410; Loss Att={'forw': '0.7411'}; Train Acc=0.332; Test Acc=0.3608; Entropy=0; Entropy_Test=\n",
      "\n",
      "45 2.252908 2.5790176\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:06epoch=126; Loss Pred=2.4605; Val Loss=2.5790; Val Acc=0.2977; Loss Att={'forw': '0.7376'}; Train Acc=0.334; Test Acc=0.3496; Entropy=0; Entropy_Test=\n",
      "\n",
      "46 2.252908 2.5939274\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:11epoch=127; Loss Pred=2.4469; Val Loss=2.5939; Val Acc=0.3308; Loss Att={'forw': '0.7427'}; Train Acc=0.337; Test Acc=0.3661; Entropy=0; Entropy_Test=\n",
      "\n",
      "47 2.252908 2.619855\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:15epoch=128; Loss Pred=2.4428; Val Loss=2.6199; Val Acc=0.3130; Loss Att={'forw': '0.7349'}; Train Acc=0.337; Test Acc=0.3585; Entropy=0; Entropy_Test=\n",
      "\n",
      "48 2.252908 2.5855145\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:21epoch=129; Loss Pred=2.4207; Val Loss=2.5855; Val Acc=0.3384; Loss Att={'forw': '0.7361'}; Train Acc=0.357; Test Acc=0.3731; Entropy=0; Entropy_Test=\n",
      "\n",
      "49 2.252908 2.5630095\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:25epoch=130; Loss Pred=2.4248; Val Loss=2.5630; Val Acc=0.3410; Loss Att={'forw': '0.7299'}; Train Acc=0.352; Test Acc=0.3676; Entropy=0; Entropy_Test=\n",
      "\n",
      "50 2.252908 2.5674896\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 132\n",
      "Optimization Finished!\n",
      "********** replication  5  **********\n",
      "topic_classification\n",
      "((11228, 300), (11228, 1))\n",
      "0.0\n",
      "(1965, 300) (1965, 1) (393, 300) (393, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: topic_classification\n",
      "    hid: \t\t\t150,\n",
      "    h_hid: \t\t\t300\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t1965/2245, SEQ_LEN: 300\n",
      "Logged Successfully: \n",
      "0 10000000000.0 3.8789248\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:33epoch=0; Loss Pred=3.8852; Val Loss=3.8789; Val Acc=0.0076; Loss Att={'forw': '1.1051'}; Train Acc=0.006; Test Acc=0.0089; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 3.8789248 2.452958\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:39epoch=1; Loss Pred=2.5478; Val Loss=2.4530; Val Acc=0.3461; Loss Att={'forw': '1.0731'}; Train Acc=0.357; Test Acc=0.3568; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.452958 2.4535167\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:43epoch=2; Loss Pred=2.5499; Val Loss=2.4535; Val Acc=0.3461; Loss Att={'forw': '0.8520'}; Train Acc=0.358; Test Acc=0.3564; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.452958 2.3835151\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:48epoch=3; Loss Pred=2.4652; Val Loss=2.3835; Val Acc=0.3511; Loss Att={'forw': '0.9192'}; Train Acc=0.331; Test Acc=0.3204; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3835151 2.3974469\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:53epoch=4; Loss Pred=2.4676; Val Loss=2.3974; Val Acc=0.3053; Loss Att={'forw': '0.7549'}; Train Acc=0.333; Test Acc=0.3144; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3835151 2.4068859\n",
      "Logged Successfully: \n",
      "2018-07-16 14:52:58epoch=5; Loss Pred=2.4537; Val Loss=2.4069; Val Acc=0.3461; Loss Att={'forw': '0.9135'}; Train Acc=0.358; Test Acc=0.3569; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3835151 2.4149325\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:02epoch=6; Loss Pred=2.4562; Val Loss=2.4149; Val Acc=0.3461; Loss Att={'forw': '0.8032'}; Train Acc=0.358; Test Acc=0.3572; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.3835151 2.3973382\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:07epoch=7; Loss Pred=2.4424; Val Loss=2.3973; Val Acc=0.3257; Loss Att={'forw': '0.9380'}; Train Acc=0.326; Test Acc=0.3242; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.3835151 2.3996081\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:11epoch=8; Loss Pred=2.4463; Val Loss=2.3996; Val Acc=0.3130; Loss Att={'forw': '0.8120'}; Train Acc=0.316; Test Acc=0.3148; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.3835151 2.3986995\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:17epoch=9; Loss Pred=2.4403; Val Loss=2.3987; Val Acc=0.3461; Loss Att={'forw': '0.9369'}; Train Acc=0.358; Test Acc=0.3572; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.3835151 2.401328\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:21epoch=10; Loss Pred=2.4445; Val Loss=2.4013; Val Acc=0.3461; Loss Att={'forw': '0.9195'}; Train Acc=0.358; Test Acc=0.3562; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.3835151 2.3630998\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:26epoch=11; Loss Pred=2.4148; Val Loss=2.3631; Val Acc=0.3333; Loss Att={'forw': '0.8914'}; Train Acc=0.353; Test Acc=0.3458; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.3630998 2.3556256\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:30epoch=12; Loss Pred=2.4136; Val Loss=2.3556; Val Acc=0.3359; Loss Att={'forw': '0.8589'}; Train Acc=0.348; Test Acc=0.3371; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3556256 2.3748574\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:35epoch=13; Loss Pred=2.4100; Val Loss=2.3749; Val Acc=0.3461; Loss Att={'forw': '0.8555'}; Train Acc=0.358; Test Acc=0.3571; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3556256 2.3817902\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:40epoch=14; Loss Pred=2.4016; Val Loss=2.3818; Val Acc=0.3461; Loss Att={'forw': '0.8706'}; Train Acc=0.358; Test Acc=0.3556; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3556256 2.3315544\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:45epoch=15; Loss Pred=2.4047; Val Loss=2.3316; Val Acc=0.3511; Loss Att={'forw': '0.8237'}; Train Acc=0.359; Test Acc=0.3580; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3315544 2.358902\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:49epoch=16; Loss Pred=2.4016; Val Loss=2.3589; Val Acc=0.3410; Loss Att={'forw': '0.8365'}; Train Acc=0.359; Test Acc=0.3536; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3315544 2.350319\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:55epoch=17; Loss Pred=2.3966; Val Loss=2.3503; Val Acc=0.3486; Loss Att={'forw': '0.8312'}; Train Acc=0.359; Test Acc=0.3583; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3315544 2.3557928\n",
      "Logged Successfully: \n",
      "2018-07-16 14:53:59epoch=18; Loss Pred=2.4002; Val Loss=2.3558; Val Acc=0.3435; Loss Att={'forw': '0.8502'}; Train Acc=0.359; Test Acc=0.3583; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.3315544 2.3570914\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:04epoch=19; Loss Pred=2.3898; Val Loss=2.3571; Val Acc=0.3511; Loss Att={'forw': '0.8597'}; Train Acc=0.360; Test Acc=0.3603; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.3315544 2.3434887\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:09epoch=20; Loss Pred=2.3941; Val Loss=2.3435; Val Acc=0.3537; Loss Att={'forw': '0.8676'}; Train Acc=0.358; Test Acc=0.3651; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.3315544 2.3258653\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:14epoch=21; Loss Pred=2.3655; Val Loss=2.3259; Val Acc=0.3461; Loss Att={'forw': '0.8244'}; Train Acc=0.360; Test Acc=0.3601; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3258653 2.3292832\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:18epoch=22; Loss Pred=2.3674; Val Loss=2.3293; Val Acc=0.3461; Loss Att={'forw': '0.8240'}; Train Acc=0.363; Test Acc=0.3641; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.3258653 2.332502\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:24epoch=23; Loss Pred=2.3433; Val Loss=2.3325; Val Acc=0.3690; Loss Att={'forw': '0.8570'}; Train Acc=0.363; Test Acc=0.3657; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.3258653 2.3186991\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:28epoch=24; Loss Pred=2.3373; Val Loss=2.3187; Val Acc=0.3562; Loss Att={'forw': '0.8155'}; Train Acc=0.358; Test Acc=0.3654; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3186991 2.2954075\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:33epoch=25; Loss Pred=2.3134; Val Loss=2.2954; Val Acc=0.3588; Loss Att={'forw': '0.8716'}; Train Acc=0.362; Test Acc=0.3600; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2954075 2.2892988\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:37epoch=26; Loss Pred=2.3161; Val Loss=2.2893; Val Acc=0.3511; Loss Att={'forw': '0.8452'}; Train Acc=0.369; Test Acc=0.3761; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2892988 2.2777574\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:43epoch=27; Loss Pred=2.2916; Val Loss=2.2778; Val Acc=0.3842; Loss Att={'forw': '0.9306'}; Train Acc=0.387; Test Acc=0.3735; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2777574 2.2898605\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:47epoch=28; Loss Pred=2.2888; Val Loss=2.2899; Val Acc=0.3766; Loss Att={'forw': '0.8852'}; Train Acc=0.381; Test Acc=0.3770; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2777574 2.300623\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:53epoch=29; Loss Pred=2.2935; Val Loss=2.3006; Val Acc=0.3715; Loss Att={'forw': '0.8477'}; Train Acc=0.394; Test Acc=0.3906; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2777574 2.3034444\n",
      "Logged Successfully: \n",
      "2018-07-16 14:54:57epoch=30; Loss Pred=2.2937; Val Loss=2.3034; Val Acc=0.3868; Loss Att={'forw': '0.8548'}; Train Acc=0.397; Test Acc=0.3857; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2777574 2.276511\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:02epoch=31; Loss Pred=2.2624; Val Loss=2.2765; Val Acc=0.3766; Loss Att={'forw': '0.8606'}; Train Acc=0.391; Test Acc=0.3908; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.276511 2.2842908\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:06epoch=32; Loss Pred=2.2548; Val Loss=2.2843; Val Acc=0.3766; Loss Att={'forw': '0.8310'}; Train Acc=0.395; Test Acc=0.3888; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.276511 2.2848036\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:12epoch=33; Loss Pred=2.2416; Val Loss=2.2848; Val Acc=0.3639; Loss Att={'forw': '0.8519'}; Train Acc=0.394; Test Acc=0.3830; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.276511 2.2739222\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:16epoch=34; Loss Pred=2.2381; Val Loss=2.2739; Val Acc=0.3791; Loss Att={'forw': '0.8761'}; Train Acc=0.394; Test Acc=0.3880; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2739222 2.2745569\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:22epoch=35; Loss Pred=2.2292; Val Loss=2.2746; Val Acc=0.3766; Loss Att={'forw': '0.8659'}; Train Acc=0.396; Test Acc=0.3959; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2739222 2.2607641\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:25epoch=36; Loss Pred=2.2254; Val Loss=2.2608; Val Acc=0.3664; Loss Att={'forw': '0.8569'}; Train Acc=0.401; Test Acc=0.3902; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2607641 2.2973342\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:31epoch=37; Loss Pred=2.2163; Val Loss=2.2973; Val Acc=0.3791; Loss Att={'forw': '0.8633'}; Train Acc=0.389; Test Acc=0.3874; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2607641 2.2790444\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:35epoch=38; Loss Pred=2.2141; Val Loss=2.2790; Val Acc=0.3715; Loss Att={'forw': '0.8495'}; Train Acc=0.394; Test Acc=0.3850; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2607641 2.3155975\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:41epoch=39; Loss Pred=2.2342; Val Loss=2.3156; Val Acc=0.3817; Loss Att={'forw': '0.8936'}; Train Acc=0.396; Test Acc=0.3856; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2607641 2.308314\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:45epoch=40; Loss Pred=2.2298; Val Loss=2.3083; Val Acc=0.3817; Loss Att={'forw': '0.8773'}; Train Acc=0.396; Test Acc=0.3787; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2607641 2.2719088\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:50epoch=41; Loss Pred=2.2293; Val Loss=2.2719; Val Acc=0.3868; Loss Att={'forw': '0.8564'}; Train Acc=0.398; Test Acc=0.3929; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2607641 2.2630904\n",
      "Logged Successfully: \n",
      "2018-07-16 14:55:54epoch=42; Loss Pred=2.2219; Val Loss=2.2631; Val Acc=0.3817; Loss Att={'forw': '0.8343'}; Train Acc=0.398; Test Acc=0.3912; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.2607641 2.2648075\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:00epoch=43; Loss Pred=2.2222; Val Loss=2.2648; Val Acc=0.3817; Loss Att={'forw': '0.9070'}; Train Acc=0.391; Test Acc=0.3794; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.2607641 2.2741106\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:04epoch=44; Loss Pred=2.2144; Val Loss=2.2741; Val Acc=0.3817; Loss Att={'forw': '0.8310'}; Train Acc=0.402; Test Acc=0.3830; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.2607641 2.2544718\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:09epoch=45; Loss Pred=2.1705; Val Loss=2.2545; Val Acc=0.4046; Loss Att={'forw': '0.8615'}; Train Acc=0.412; Test Acc=0.4075; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2544718 2.2565444\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:14epoch=46; Loss Pred=2.1722; Val Loss=2.2565; Val Acc=0.4097; Loss Att={'forw': '0.8868'}; Train Acc=0.418; Test Acc=0.4038; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2544718 2.2274225\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:19epoch=47; Loss Pred=2.1370; Val Loss=2.2274; Val Acc=0.4046; Loss Att={'forw': '0.9094'}; Train Acc=0.426; Test Acc=0.4058; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2274225 2.2266545\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:23epoch=48; Loss Pred=2.1466; Val Loss=2.2267; Val Acc=0.4046; Loss Att={'forw': '0.8627'}; Train Acc=0.424; Test Acc=0.4086; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2266545 2.2520928\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:29epoch=49; Loss Pred=2.1607; Val Loss=2.2521; Val Acc=0.3893; Loss Att={'forw': '0.8770'}; Train Acc=0.415; Test Acc=0.4011; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2266545 2.2485466\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:33epoch=50; Loss Pred=2.1635; Val Loss=2.2485; Val Acc=0.3995; Loss Att={'forw': '0.8555'}; Train Acc=0.411; Test Acc=0.4008; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.2266545 2.285494\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:38epoch=51; Loss Pred=2.1829; Val Loss=2.2855; Val Acc=0.3639; Loss Att={'forw': '0.8731'}; Train Acc=0.398; Test Acc=0.3738; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2266545 2.2835412\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:42epoch=52; Loss Pred=2.1903; Val Loss=2.2835; Val Acc=0.3690; Loss Att={'forw': '0.8530'}; Train Acc=0.385; Test Acc=0.3740; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2266545 2.273694\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:48epoch=53; Loss Pred=2.1649; Val Loss=2.2737; Val Acc=0.3613; Loss Att={'forw': '0.8698'}; Train Acc=0.395; Test Acc=0.3848; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2266545 2.2514653\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:52epoch=54; Loss Pred=2.1568; Val Loss=2.2515; Val Acc=0.3562; Loss Att={'forw': '0.8453'}; Train Acc=0.404; Test Acc=0.3870; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.2266545 2.256545\n",
      "Logged Successfully: \n",
      "2018-07-16 14:56:57epoch=55; Loss Pred=2.1794; Val Loss=2.2565; Val Acc=0.3791; Loss Att={'forw': '0.8207'}; Train Acc=0.398; Test Acc=0.3862; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.2266545 2.2513728\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:01epoch=56; Loss Pred=2.1603; Val Loss=2.2514; Val Acc=0.3740; Loss Att={'forw': '0.8105'}; Train Acc=0.405; Test Acc=0.3933; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.2266545 2.2203233\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:07epoch=57; Loss Pred=2.1210; Val Loss=2.2203; Val Acc=0.4020; Loss Att={'forw': '0.8402'}; Train Acc=0.421; Test Acc=0.3982; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2203233 2.2339244\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:11epoch=58; Loss Pred=2.1226; Val Loss=2.2339; Val Acc=0.3842; Loss Att={'forw': '0.8252'}; Train Acc=0.418; Test Acc=0.3956; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.2203233 2.252643\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:16epoch=59; Loss Pred=2.0960; Val Loss=2.2526; Val Acc=0.3639; Loss Att={'forw': '0.8207'}; Train Acc=0.405; Test Acc=0.3859; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.2203233 2.2796383\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:21epoch=60; Loss Pred=2.0961; Val Loss=2.2796; Val Acc=0.3639; Loss Att={'forw': '0.8075'}; Train Acc=0.410; Test Acc=0.3954; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.2203233 2.268366\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:26epoch=61; Loss Pred=2.0934; Val Loss=2.2684; Val Acc=0.4224; Loss Att={'forw': '0.8355'}; Train Acc=0.428; Test Acc=0.4011; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.2203233 2.2792838\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:30epoch=62; Loss Pred=2.0901; Val Loss=2.2793; Val Acc=0.4046; Loss Att={'forw': '0.8144'}; Train Acc=0.428; Test Acc=0.4030; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.2203233 2.2921658\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:36epoch=63; Loss Pred=2.0843; Val Loss=2.2922; Val Acc=0.3690; Loss Att={'forw': '0.8177'}; Train Acc=0.418; Test Acc=0.3998; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.2203233 2.282464\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:40epoch=64; Loss Pred=2.0832; Val Loss=2.2825; Val Acc=0.3740; Loss Att={'forw': '0.8060'}; Train Acc=0.411; Test Acc=0.4007; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.2203233 2.2278256\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:46epoch=65; Loss Pred=2.0395; Val Loss=2.2278; Val Acc=0.4198; Loss Att={'forw': '0.8207'}; Train Acc=0.443; Test Acc=0.4076; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.2203233 2.2285342\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:50epoch=66; Loss Pred=2.0473; Val Loss=2.2285; Val Acc=0.4097; Loss Att={'forw': '0.8118'}; Train Acc=0.447; Test Acc=0.4057; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.2203233 2.2285872\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:55epoch=67; Loss Pred=2.0556; Val Loss=2.2286; Val Acc=0.3842; Loss Att={'forw': '0.8030'}; Train Acc=0.419; Test Acc=0.3896; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.2203233 2.2536922\n",
      "Logged Successfully: \n",
      "2018-07-16 14:57:59epoch=68; Loss Pred=2.0537; Val Loss=2.2537; Val Acc=0.3537; Loss Att={'forw': '0.7957'}; Train Acc=0.420; Test Acc=0.3959; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.2203233 2.244866\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:05epoch=69; Loss Pred=2.0588; Val Loss=2.2449; Val Acc=0.3969; Loss Att={'forw': '0.8065'}; Train Acc=0.426; Test Acc=0.3880; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.2203233 2.2227216\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:09epoch=70; Loss Pred=2.0695; Val Loss=2.2227; Val Acc=0.3944; Loss Att={'forw': '0.7985'}; Train Acc=0.424; Test Acc=0.3843; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.2203233 2.261508\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:14epoch=71; Loss Pred=2.0367; Val Loss=2.2615; Val Acc=0.3893; Loss Att={'forw': '0.8282'}; Train Acc=0.438; Test Acc=0.3882; Entropy=0; Entropy_Test=\n",
      "\n",
      "14 2.2203233 2.2691972\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:18epoch=72; Loss Pred=2.0485; Val Loss=2.2692; Val Acc=0.4046; Loss Att={'forw': '0.8115'}; Train Acc=0.425; Test Acc=0.3977; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.2203233 2.23251\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:24epoch=73; Loss Pred=2.0490; Val Loss=2.2325; Val Acc=0.3919; Loss Att={'forw': '0.8338'}; Train Acc=0.428; Test Acc=0.3903; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.2203233 2.2177162\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:28epoch=74; Loss Pred=2.0516; Val Loss=2.2177; Val Acc=0.3715; Loss Att={'forw': '0.8063'}; Train Acc=0.426; Test Acc=0.3870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.2177162 2.1823514\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:33epoch=75; Loss Pred=1.9876; Val Loss=2.1824; Val Acc=0.4071; Loss Att={'forw': '0.8485'}; Train Acc=0.447; Test Acc=0.4094; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.1823514 2.2342215\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:38epoch=76; Loss Pred=1.9957; Val Loss=2.2342; Val Acc=0.3893; Loss Att={'forw': '0.8303'}; Train Acc=0.446; Test Acc=0.4080; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.1823514 2.3520293\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:43epoch=77; Loss Pred=2.0794; Val Loss=2.3520; Val Acc=0.3868; Loss Att={'forw': '0.8501'}; Train Acc=0.445; Test Acc=0.4111; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.1823514 2.345549\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:47epoch=78; Loss Pred=2.0897; Val Loss=2.3455; Val Acc=0.4020; Loss Att={'forw': '0.8366'}; Train Acc=0.433; Test Acc=0.4142; Entropy=0; Entropy_Test=\n",
      "\n",
      "3 2.1823514 2.2993991\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:53epoch=79; Loss Pred=2.0250; Val Loss=2.2994; Val Acc=0.3715; Loss Att={'forw': '0.8356'}; Train Acc=0.431; Test Acc=0.4100; Entropy=0; Entropy_Test=\n",
      "\n",
      "4 2.1823514 2.30395\n",
      "Logged Successfully: \n",
      "2018-07-16 14:58:57epoch=80; Loss Pred=2.0292; Val Loss=2.3040; Val Acc=0.3791; Loss Att={'forw': '0.8295'}; Train Acc=0.427; Test Acc=0.4041; Entropy=0; Entropy_Test=\n",
      "\n",
      "5 2.1823514 2.2905445\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:02epoch=81; Loss Pred=2.0130; Val Loss=2.2905; Val Acc=0.3944; Loss Att={'forw': '0.8309'}; Train Acc=0.453; Test Acc=0.3958; Entropy=0; Entropy_Test=\n",
      "\n",
      "6 2.1823514 2.290977\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:06epoch=82; Loss Pred=2.0011; Val Loss=2.2910; Val Acc=0.3944; Loss Att={'forw': '0.8144'}; Train Acc=0.447; Test Acc=0.4046; Entropy=0; Entropy_Test=\n",
      "\n",
      "7 2.1823514 2.3001347\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:12epoch=83; Loss Pred=1.9325; Val Loss=2.3001; Val Acc=0.3969; Loss Att={'forw': '0.8169'}; Train Acc=0.471; Test Acc=0.4117; Entropy=0; Entropy_Test=\n",
      "\n",
      "8 2.1823514 2.2732642\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:16epoch=84; Loss Pred=1.9306; Val Loss=2.2733; Val Acc=0.3944; Loss Att={'forw': '0.8110'}; Train Acc=0.474; Test Acc=0.4075; Entropy=0; Entropy_Test=\n",
      "\n",
      "9 2.1823514 2.259494\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:21epoch=85; Loss Pred=1.9165; Val Loss=2.2595; Val Acc=0.3919; Loss Att={'forw': '0.8248'}; Train Acc=0.472; Test Acc=0.3962; Entropy=0; Entropy_Test=\n",
      "\n",
      "10 2.1823514 2.281783\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:25epoch=86; Loss Pred=1.9218; Val Loss=2.2818; Val Acc=0.3893; Loss Att={'forw': '0.8184'}; Train Acc=0.470; Test Acc=0.3941; Entropy=0; Entropy_Test=\n",
      "\n",
      "11 2.1823514 2.2472694\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:31epoch=87; Loss Pred=1.8989; Val Loss=2.2473; Val Acc=0.3995; Loss Att={'forw': '0.8279'}; Train Acc=0.463; Test Acc=0.3981; Entropy=0; Entropy_Test=\n",
      "\n",
      "12 2.1823514 2.2386463\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:35epoch=88; Loss Pred=1.8954; Val Loss=2.2386; Val Acc=0.3944; Loss Att={'forw': '0.8153'}; Train Acc=0.471; Test Acc=0.3965; Entropy=0; Entropy_Test=\n",
      "\n",
      "13 2.1823514 2.240147\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:41epoch=89; Loss Pred=1.8905; Val Loss=2.2401; Val Acc=0.4173; Loss Att={'forw': '0.8448'}; Train Acc=0.477; Test Acc=0.4079; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 2.1823514 2.2351425\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:45epoch=90; Loss Pred=1.8961; Val Loss=2.2351; Val Acc=0.4249; Loss Att={'forw': '0.8310'}; Train Acc=0.474; Test Acc=0.4086; Entropy=0; Entropy_Test=\n",
      "\n",
      "15 2.1823514 2.2237444\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:50epoch=91; Loss Pred=1.9262; Val Loss=2.2237; Val Acc=0.3868; Loss Att={'forw': '0.8481'}; Train Acc=0.457; Test Acc=0.3722; Entropy=0; Entropy_Test=\n",
      "\n",
      "16 2.1823514 2.241009\n",
      "Logged Successfully: \n",
      "2018-07-16 14:59:55epoch=92; Loss Pred=1.9331; Val Loss=2.2410; Val Acc=0.3969; Loss Att={'forw': '0.8383'}; Train Acc=0.453; Test Acc=0.3832; Entropy=0; Entropy_Test=\n",
      "\n",
      "17 2.1823514 2.285837\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:00epoch=93; Loss Pred=1.9641; Val Loss=2.2858; Val Acc=0.3893; Loss Att={'forw': '0.8439'}; Train Acc=0.445; Test Acc=0.3672; Entropy=0; Entropy_Test=\n",
      "\n",
      "18 2.1823514 2.2718945\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:04epoch=94; Loss Pred=1.9582; Val Loss=2.2719; Val Acc=0.4046; Loss Att={'forw': '0.8312'}; Train Acc=0.449; Test Acc=0.3733; Entropy=0; Entropy_Test=\n",
      "\n",
      "19 2.1823514 2.299091\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:09epoch=95; Loss Pred=1.9001; Val Loss=2.2991; Val Acc=0.4071; Loss Att={'forw': '0.8654'}; Train Acc=0.476; Test Acc=0.3943; Entropy=0; Entropy_Test=\n",
      "\n",
      "20 2.1823514 2.3088648\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:14epoch=96; Loss Pred=1.9017; Val Loss=2.3089; Val Acc=0.3995; Loss Att={'forw': '0.8375'}; Train Acc=0.463; Test Acc=0.3892; Entropy=0; Entropy_Test=\n",
      "\n",
      "21 2.1823514 2.2514453\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:19epoch=97; Loss Pred=1.8614; Val Loss=2.2514; Val Acc=0.4173; Loss Att={'forw': '0.8460'}; Train Acc=0.481; Test Acc=0.4076; Entropy=0; Entropy_Test=\n",
      "\n",
      "22 2.1823514 2.2422717\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:23epoch=98; Loss Pred=1.8872; Val Loss=2.2423; Val Acc=0.4198; Loss Att={'forw': '0.8429'}; Train Acc=0.476; Test Acc=0.4051; Entropy=0; Entropy_Test=\n",
      "\n",
      "23 2.1823514 2.3486972\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:29epoch=99; Loss Pred=1.9321; Val Loss=2.3487; Val Acc=0.4122; Loss Att={'forw': '0.8459'}; Train Acc=0.445; Test Acc=0.3922; Entropy=0; Entropy_Test=\n",
      "\n",
      "24 2.1823514 2.3855636\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:33epoch=100; Loss Pred=1.9453; Val Loss=2.3856; Val Acc=0.3588; Loss Att={'forw': '0.8300'}; Train Acc=0.438; Test Acc=0.3951; Entropy=0; Entropy_Test=\n",
      "\n",
      "25 2.1823514 2.2556417\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:39epoch=101; Loss Pred=1.8822; Val Loss=2.2556; Val Acc=0.4224; Loss Att={'forw': '0.8534'}; Train Acc=0.496; Test Acc=0.4146; Entropy=0; Entropy_Test=\n",
      "\n",
      "26 2.1823514 2.2444768\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:43epoch=102; Loss Pred=1.8709; Val Loss=2.2445; Val Acc=0.4122; Loss Att={'forw': '0.8249'}; Train Acc=0.491; Test Acc=0.4068; Entropy=0; Entropy_Test=\n",
      "\n",
      "27 2.1823514 2.2746227\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:48epoch=103; Loss Pred=1.8367; Val Loss=2.2746; Val Acc=0.4224; Loss Att={'forw': '0.8590'}; Train Acc=0.497; Test Acc=0.4203; Entropy=0; Entropy_Test=\n",
      "\n",
      "28 2.1823514 2.2571275\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:52epoch=104; Loss Pred=1.8556; Val Loss=2.2571; Val Acc=0.4249; Loss Att={'forw': '0.8447'}; Train Acc=0.488; Test Acc=0.4188; Entropy=0; Entropy_Test=\n",
      "\n",
      "29 2.1823514 2.3530588\n",
      "Logged Successfully: \n",
      "2018-07-16 15:00:58epoch=105; Loss Pred=1.8954; Val Loss=2.3531; Val Acc=0.4173; Loss Att={'forw': '0.8473'}; Train Acc=0.492; Test Acc=0.4242; Entropy=0; Entropy_Test=\n",
      "\n",
      "30 2.1823514 2.386758\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:02epoch=106; Loss Pred=1.9119; Val Loss=2.3868; Val Acc=0.4173; Loss Att={'forw': '0.8374'}; Train Acc=0.499; Test Acc=0.4220; Entropy=0; Entropy_Test=\n",
      "\n",
      "31 2.1823514 2.5324814\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:07epoch=107; Loss Pred=2.1461; Val Loss=2.5325; Val Acc=0.4071; Loss Att={'forw': '0.8794'}; Train Acc=0.451; Test Acc=0.4163; Entropy=0; Entropy_Test=\n",
      "\n",
      "32 2.1823514 2.5133502\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:11epoch=108; Loss Pred=2.1183; Val Loss=2.5134; Val Acc=0.4402; Loss Att={'forw': '0.8499'}; Train Acc=0.451; Test Acc=0.4184; Entropy=0; Entropy_Test=\n",
      "\n",
      "33 2.1823514 2.408962\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:17epoch=109; Loss Pred=1.9497; Val Loss=2.4090; Val Acc=0.3588; Loss Att={'forw': '0.8502'}; Train Acc=0.435; Test Acc=0.3466; Entropy=0; Entropy_Test=\n",
      "\n",
      "34 2.1823514 2.4302275\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:21epoch=110; Loss Pred=1.9513; Val Loss=2.4302; Val Acc=0.3359; Loss Att={'forw': '0.8532'}; Train Acc=0.437; Test Acc=0.3467; Entropy=0; Entropy_Test=\n",
      "\n",
      "35 2.1823514 2.2592993\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:27epoch=111; Loss Pred=1.8779; Val Loss=2.2593; Val Acc=0.4326; Loss Att={'forw': '0.8551'}; Train Acc=0.466; Test Acc=0.4101; Entropy=0; Entropy_Test=\n",
      "\n",
      "36 2.1823514 2.2594893\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:31epoch=112; Loss Pred=1.8686; Val Loss=2.2595; Val Acc=0.4326; Loss Att={'forw': '0.8385'}; Train Acc=0.463; Test Acc=0.4138; Entropy=0; Entropy_Test=\n",
      "\n",
      "37 2.1823514 2.264404\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:36epoch=113; Loss Pred=1.7633; Val Loss=2.2644; Val Acc=0.3919; Loss Att={'forw': '0.8478'}; Train Acc=0.508; Test Acc=0.3968; Entropy=0; Entropy_Test=\n",
      "\n",
      "38 2.1823514 2.2914124\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:41epoch=114; Loss Pred=1.7786; Val Loss=2.2914; Val Acc=0.3893; Loss Att={'forw': '0.8391'}; Train Acc=0.514; Test Acc=0.3976; Entropy=0; Entropy_Test=\n",
      "\n",
      "39 2.1823514 2.3052325\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:46epoch=115; Loss Pred=1.7561; Val Loss=2.3052; Val Acc=0.4198; Loss Att={'forw': '0.8407'}; Train Acc=0.529; Test Acc=0.4090; Entropy=0; Entropy_Test=\n",
      "\n",
      "40 2.1823514 2.2714436\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:50epoch=116; Loss Pred=1.7566; Val Loss=2.2714; Val Acc=0.4326; Loss Att={'forw': '0.8284'}; Train Acc=0.527; Test Acc=0.4057; Entropy=0; Entropy_Test=\n",
      "\n",
      "41 2.1823514 2.2386067\n",
      "Logged Successfully: \n",
      "2018-07-16 15:01:55epoch=117; Loss Pred=1.7056; Val Loss=2.2386; Val Acc=0.4427; Loss Att={'forw': '0.8524'}; Train Acc=0.527; Test Acc=0.4071; Entropy=0; Entropy_Test=\n",
      "\n",
      "42 2.1823514 2.2530177\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:00epoch=118; Loss Pred=1.7047; Val Loss=2.2530; Val Acc=0.4071; Loss Att={'forw': '0.8330'}; Train Acc=0.532; Test Acc=0.4162; Entropy=0; Entropy_Test=\n",
      "\n",
      "43 2.1823514 2.3065307\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:05epoch=119; Loss Pred=1.7177; Val Loss=2.3065; Val Acc=0.4275; Loss Att={'forw': '0.8501'}; Train Acc=0.526; Test Acc=0.4047; Entropy=0; Entropy_Test=\n",
      "\n",
      "44 2.1823514 2.3229413\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:10epoch=120; Loss Pred=1.7188; Val Loss=2.3229; Val Acc=0.4198; Loss Att={'forw': '0.8384'}; Train Acc=0.529; Test Acc=0.4008; Entropy=0; Entropy_Test=\n",
      "\n",
      "45 2.1823514 2.3019254\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:15epoch=121; Loss Pred=1.7009; Val Loss=2.3019; Val Acc=0.4122; Loss Att={'forw': '0.8439'}; Train Acc=0.526; Test Acc=0.4085; Entropy=0; Entropy_Test=\n",
      "\n",
      "46 2.1823514 2.3021185\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:19epoch=122; Loss Pred=1.6951; Val Loss=2.3021; Val Acc=0.4300; Loss Att={'forw': '0.8344'}; Train Acc=0.527; Test Acc=0.4038; Entropy=0; Entropy_Test=\n",
      "\n",
      "47 2.1823514 2.3262727\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:24epoch=123; Loss Pred=1.6691; Val Loss=2.3263; Val Acc=0.4046; Loss Att={'forw': '0.8423'}; Train Acc=0.544; Test Acc=0.3923; Entropy=0; Entropy_Test=\n",
      "\n",
      "48 2.1823514 2.3112144\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:28epoch=124; Loss Pred=1.6667; Val Loss=2.3112; Val Acc=0.3995; Loss Att={'forw': '0.8287'}; Train Acc=0.535; Test Acc=0.3871; Entropy=0; Entropy_Test=\n",
      "\n",
      "49 2.1823514 2.340886\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:34epoch=125; Loss Pred=1.7408; Val Loss=2.3409; Val Acc=0.4020; Loss Att={'forw': '0.8610'}; Train Acc=0.526; Test Acc=0.3745; Entropy=0; Entropy_Test=\n",
      "\n",
      "50 2.1823514 2.3079004\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 127\n",
      "Optimization Finished!\n",
      "********** replication  6  **********\n",
      "topic_classification\n",
      "((11228, 300), (11228, 1))\n",
      "0.0\n",
      "(1965, 300) (1965, 1) (393, 300) (393, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: topic_classification\n",
      "    hid: \t\t\t150,\n",
      "    h_hid: \t\t\t300\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.2\n",
      "    TRAIN/TEST_SIZE: \t1965/2245, SEQ_LEN: 300\n",
      "Logged Successfully: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000000000.0 3.796451\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:41epoch=0; Loss Pred=3.7841; Val Loss=3.7965; Val Acc=0.0051; Loss Att={'forw': '1.1077'}; Train Acc=0.008; Test Acc=0.0081; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 3.796451 2.59911\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:46epoch=1; Loss Pred=2.5909; Val Loss=2.5991; Val Acc=0.3155; Loss Att={'forw': '1.0987'}; Train Acc=0.360; Test Acc=0.3566; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.59911 2.6157205\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:50epoch=2; Loss Pred=2.6085; Val Loss=2.6157; Val Acc=0.3155; Loss Att={'forw': '0.8493'}; Train Acc=0.359; Test Acc=0.3562; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.59911 2.5292048\n",
      "Logged Successfully: \n",
      "2018-07-16 15:02:56epoch=3; Loss Pred=2.4711; Val Loss=2.5292; Val Acc=0.3155; Loss Att={'forw': '1.0700'}; Train Acc=0.358; Test Acc=0.3577; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.5292048 2.5164459\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:00epoch=4; Loss Pred=2.4649; Val Loss=2.5164; Val Acc=0.3155; Loss Att={'forw': '0.8497'}; Train Acc=0.359; Test Acc=0.3573; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.5164459 2.4689395\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:05epoch=5; Loss Pred=2.4345; Val Loss=2.4689; Val Acc=0.3206; Loss Att={'forw': '0.9274'}; Train Acc=0.362; Test Acc=0.3596; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4689395 2.4593606\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:10epoch=6; Loss Pred=2.4349; Val Loss=2.4594; Val Acc=0.3181; Loss Att={'forw': '0.8470'}; Train Acc=0.359; Test Acc=0.3626; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4593606 2.4731767\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:15epoch=7; Loss Pred=2.4112; Val Loss=2.4732; Val Acc=0.3104; Loss Att={'forw': '0.8518'}; Train Acc=0.360; Test Acc=0.3615; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.4593606 2.4554482\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:20epoch=8; Loss Pred=2.4115; Val Loss=2.4554; Val Acc=0.3155; Loss Att={'forw': '0.8354'}; Train Acc=0.364; Test Acc=0.3614; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.4554482 2.446084\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:25epoch=9; Loss Pred=2.3863; Val Loss=2.4461; Val Acc=0.3130; Loss Att={'forw': '0.8807'}; Train Acc=0.364; Test Acc=0.3649; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.446084 2.44404\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:29epoch=10; Loss Pred=2.3851; Val Loss=2.4440; Val Acc=0.3104; Loss Att={'forw': '0.8391'}; Train Acc=0.365; Test Acc=0.3637; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.44404 2.4482505\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:35epoch=11; Loss Pred=2.3754; Val Loss=2.4483; Val Acc=0.3104; Loss Att={'forw': '0.9671'}; Train Acc=0.363; Test Acc=0.3635; Entropy=0; Entropy_Test=\n",
      "\n",
      "1 2.44404 2.4665406\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:39epoch=12; Loss Pred=2.3910; Val Loss=2.4665; Val Acc=0.3155; Loss Att={'forw': '0.9224'}; Train Acc=0.365; Test Acc=0.3638; Entropy=0; Entropy_Test=\n",
      "\n",
      "2 2.44404 2.3840652\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:44epoch=13; Loss Pred=2.3234; Val Loss=2.3841; Val Acc=0.3130; Loss Att={'forw': '0.8893'}; Train Acc=0.373; Test Acc=0.3735; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3840652 2.3734121\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:48epoch=14; Loss Pred=2.3249; Val Loss=2.3734; Val Acc=0.3282; Loss Att={'forw': '0.8780'}; Train Acc=0.370; Test Acc=0.3671; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3734121 2.3447316\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:54epoch=15; Loss Pred=2.3166; Val Loss=2.3447; Val Acc=0.3740; Loss Att={'forw': '0.8197'}; Train Acc=0.386; Test Acc=0.3732; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 2.3447316 2.3249054\n",
      "Logged Successfully: \n",
      "2018-07-16 15:03:58epoch=16; Loss Pred=2.3118; Val Loss=2.3249; Val Acc=0.3715; Loss Att={'forw': '0.8222'}; Train Acc=0.393; Test Acc=0.3871; Entropy=0; Entropy_Test=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "#!/usr/local/bin/python\n",
    "\n",
    "# This version of the code trains the attractor connections with a separate\n",
    "# objective function than the objective function used to train all other weights\n",
    "# in the network (on the prediction task).\n",
    "\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload\n",
    "\n",
    "from tensorflow_helpers import *\n",
    "from data_generator import generate_examples, pick_task\n",
    "\n",
    "from helper_functions import get_batches, load_pretrained_embeddings, \\\n",
    "    get_model_type_str, translate_ids_to_words, \\\n",
    "    save_results, print_into_log, print_some_translated_sentences, \\\n",
    "    get_training_progress_comment\n",
    "from graph_init import GRU_attractor, TANH_attractor\n",
    "\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, patience_max, disp_epoch, min_delta = 0.00):\n",
    "        self.best = 1e10\n",
    "        self.patience = 0  # our patience\n",
    "        self.patience_max = patience_max\n",
    "        self.display_epoch = disp_epoch\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def update(self, current):\n",
    "        if self.best > current:\n",
    "            self.best = current\n",
    "            self.patience = 0\n",
    "        elif abs(self.best - current) > self.min_delta:\n",
    "            self.patience += 1\n",
    "\n",
    "    def patience_ran_out(self):\n",
    "        if self.patience*self.display_epoch > self.patience_max:\n",
    "            return True\n",
    "        else:\n",
    "            False\n",
    "            \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "ops = {\n",
    "    'model_type': \"TANH\",  # OPTIONS: vanilla, LSTM_raw, LSTM_tensorflow, LSTM_attractor\n",
    "    'hid': 150,\n",
    "    'in': None,  # TBD\n",
    "    'out': 1,\n",
    "    #         'batch_size':n_examples, #since the sequences are 1-dimensional it's easier to just run them all at once\n",
    "    'n_attractor_iterations': 15,\n",
    "    'attractor_dynamics': \"projection2\",  # OPTIONS:  \"\" (for no attractor dynamics),\n",
    "    #           \"direct\" (simple attractor weights applied to hidden states directly, trained with noise addition)\n",
    "    #           \"projection\" (project the hidden state into a separate space via weights, do attraction, project back)\n",
    "    #           \"helper_hidden\" (hidden-hidden neurons) - IMPORTANT: don't forget to add h_hid number\n",
    "    'h_hid': 300,  # helper hidden for \"helper hidden\" \"attractory_dynamics\" mode\n",
    "    'attractor_noise_level': 0.5,\n",
    "    'attractor_noise_type': \"bernoilli\",  # OPTIONS: \"gaussian\", \"dropout\", \"random_drop\"\n",
    "\n",
    "    'train_attr_weights_on_pred': False,  \n",
    "\n",
    "    'attractor_regularization': \"l2_regularization\",  # OPTIONS: \"l2_regularization\", \"l2_norm\"\n",
    "    'attractor_regularization_lambda': 0.0,\n",
    "\n",
    "    'record_mutual_information': True,\n",
    "    'problem_type': \"topic_classification\",  # OPTIONS: parity, parity_length, majority, reber, kazakov, pos_brown, ner_german, sentiment_imdb, topic_classification\n",
    "    'masking': True,#\"seq\", \"final\"\n",
    "    'prediction_type': 'final_class', #'seq', 'final', 'final_class'\n",
    "    'seq_len': None,\n",
    "\n",
    "    'save_best_model': True,\n",
    "    'reshuffle_data_each_replication': False,  # relevant for POS datasets (since they are loaded from files)\n",
    "    'test_partition': 0.3,\n",
    "    'lrate': 0.0001,  # was 0.008\n",
    "\n",
    "    # NLP related (pos_brown task)\n",
    "    'bidirectional': False,\n",
    "    'embedding_size': 100,\n",
    "    'load_word_embeddings': True,\n",
    "    'train_word_embeddings': False,\n",
    "    'trainable_logic_symbols': 2, #make first *N* embeddings trainable(Pad, unknown, start symbols make it a separate matrix and trainable)\n",
    "    'input_type': \"embed\",  # embed&prior, embed, prior\n",
    "    'dropout': 0.2  # in range(0,1)\n",
    "}\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# SEQ_LEN = 12 # number of bits in input sequence\n",
    "N_HIDDEN = ops['hid']  # number of hidden units\n",
    "N_H_HIDDEN = ops['h_hid']\n",
    "TASK = ops['problem_type']\n",
    "ARCH = ops['model_type']  # hidden layer type: 'GRU' or 'tanh'\n",
    "NOISE_LEVEL = ops['attractor_noise_level']\n",
    "# noise in training attractor net\n",
    "# if >=0, Gaussian with std dev NOISE_LEVEL\n",
    "# if < 0, Bernoulli dropout proportion -NOISE_LEVEL\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "INPUT_NOISE_LEVEL = 0.1\n",
    "ATTRACTOR_TYPE = ops['attractor_dynamics']\n",
    "N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "# number of time steps in attractor dynamics\n",
    "# if = 0, then no attractor net\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# ATTR_WEIGHT_CONSTRAINTS = True\n",
    "# True: make attractor weights symmetric and have zero diag\n",
    "# False: unconstrained\n",
    "TRAIN_ATTR_WEIGHTS_ON_PREDICTION = False\n",
    "# True: train attractor weights on attractor net _and_ prediction\n",
    "REPORT_BEST_TRAIN_PERFORMANCE = True\n",
    "# True: save the train/test perf on the epoch for which train perf was best\n",
    "LOSS_SWITCH_FREQ = 1\n",
    "# how often (in epochs) to switch between attractor\n",
    "# and prediction loss\n",
    "\n",
    "ops, SEQ_LEN, N_INPUT, N_CLASSES, N_TRAIN, N_TEST = pick_task(ops['problem_type'],\n",
    "                                                              ops)  # task (parity, majority, reber, kazakov)\n",
    "\n",
    "# Training Parameters\n",
    "\n",
    "TRAINING_EPOCHS = 5000\n",
    "N_REPLICATIONS = 20\n",
    "BATCH_SIZE = 500\n",
    "DISPLAY_EPOCH = 1\n",
    "EARLY_STOPPING_THRESH = 0. # 1e-3 for POS, 0.03 for Sentiment\n",
    "EARLY_STOPPING_PATIENCE = 50  # in epochs\n",
    "EARLY_STOPPING_MINIMUM_EPOCH = 0\n",
    "\n",
    "# NOTEBOOK CODE\n",
    "\n",
    "######### MAIN CODE #############################################################\n",
    "#0.02, 0.05, 0.1, 0.2, 0.35, 0.5, \n",
    "for dataset_part in [0.25, 0.5, 0.75, 0.99]:\n",
    "    for attractor_steps in [15,0]:\n",
    "#     for att_reg in [0.0]:\n",
    "        # the tf seed needs to be within the context of the graph.\n",
    "        tf.reset_default_graph()\n",
    "        np.random.seed(13)\n",
    "        tf.set_random_seed(13)\n",
    "        ops['n_attractor_iterations'] = attractor_steps\n",
    "        N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "\n",
    "        #\n",
    "        # PLACEHOLDERS\n",
    "        #\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            # last dimension is left singular, tensorflow will expect it to be an id number, not 1-hot embed\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN], name='Y')\n",
    "        elif ops['problem_type'] == 'sentiment_imdb':\n",
    "             # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            Y = tf.placeholder(\"int64\", [None, N_CLASSES], name='Y')\n",
    "        elif ops['problem_type'] == 'topic_classification':\n",
    "             # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            Y = tf.placeholder(\"int64\", [None, 1], name='Y')\n",
    "        elif ops['problem_type'] == 'ner_german':\n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN])\n",
    "        else:  # single output \n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, N_CLASSES])\n",
    "        attractor_tgt_net = tf.placeholder(\"float\", [None, N_HIDDEN], name='attractor_tgt')\n",
    "\n",
    "        # Embedding matrix initialization\n",
    "        if 'pos' in ops['problem_type'] or 'sentiment' in ops['problem_type'] or ops['problem_type'] == \"topic_classification\":\n",
    "            [_, _, _, _, _, _, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                         INPUT_NOISE_LEVEL, TASK, ops)\n",
    "\n",
    "            if ops['load_word_embeddings']:\n",
    "                embeddings_loaded, _ = load_pretrained_embeddings('data/glove.6B.{}d.txt'.format(ops['embedding_size']),\n",
    "                                                               maps, ops)\n",
    "                if ops['trainable_logic_symbols'] > 0:\n",
    "                    with tf.variable_scope(\"TASK_WEIGHTS\"):\n",
    "                        symbols_embedding = tf.get_variable(\"symb_embedding\",\n",
    "                                                initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                                shape=[ops['trainable_logic_symbols'], ops['embedding_size']],\n",
    "                                                dtype=tf.float32,\n",
    "                                                trainable=True)\n",
    "                    \n",
    "                word_embedding = tf.get_variable(\"embedding\",\n",
    "                                            initializer=embeddings_loaded,\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=ops['train_word_embeddings'])\n",
    "                if ops['trainable_logic_symbols'] > 0:\n",
    "                    embedding = tf.concat([symbols_embedding, word_embedding], axis=0)\n",
    "                else:\n",
    "                    embedding = word_embedding\n",
    "            else:  # initialize randomly\n",
    "                embedding = tf.get_variable(\"embedding\",\n",
    "                                            initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                            shape=[ops['vocab_size'], ops['embedding_size']],\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=ops['train_word_embeddings'])\n",
    "            embed_lookup = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "            # load priors information\n",
    "            if ops['input_type'] == 'prior' or ops['input_type'] == 'embed&prior':\n",
    "                id2prior = maps['id2prior']\n",
    "                word2id = maps['word2id']\n",
    "                priors = np.zeros([len(id2prior), len(id2prior[0])]).astype(\"float32\")\n",
    "                for id, prior in id2prior.items():\n",
    "                    priors[id] = prior\n",
    "                priors_op = tf.get_variable(\"priors\",\n",
    "                                            initializer=priors,\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=False)\n",
    "                prior_lookup = tf.nn.embedding_lookup(priors_op, X)\n",
    "\n",
    "            if ops['input_type'] == 'embed':\n",
    "                embed = embed_lookup\n",
    "            elif ops['input_type'] == 'prior':\n",
    "                embed = prior_lookup\n",
    "            elif ops['input_type'] == 'embed&prior':\n",
    "                embed = tf.concat([embed_lookup, prior_lookup], axis=2)\n",
    "\n",
    "        # Graph + all the training variables\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            net_inputs = {'X': embed, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        elif ops['problem_type'] == 'sentiment_imdb' or ops['problem_type'] == 'topic_classification':\n",
    "            net_inputs = {'X': embed, 'mask': X, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        else:\n",
    "            net_inputs = {'X': X, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        if ops['model_type'] == \"TANH\":\n",
    "            cell = TANH_attractor\n",
    "        elif ops['model_type'] == \"GRU\":\n",
    "            cell = GRU_attractor\n",
    "        if ops['bidirectional']:\n",
    "            G_attractors = {'forw': [], 'back': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = cell(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            # Backward:\n",
    "            G_back = cell(ops, inputs=net_inputs, direction='backward', suffix=names[1])\n",
    "            attr_loss_op_back = G_back.attr_loss_op\n",
    "            attr_train_op_back = G_back.attr_train_op\n",
    "            h_clean_seq_flat_back = G_back.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_back = G_back.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['back'] = {'attr_loss_op': attr_loss_op_back, \"attr_train_op\": attr_train_op_back,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_back, 'h_net_seq_flat': h_net_seq_flat_back}\n",
    "            G_back_output = G_back.output\n",
    "\n",
    "            \n",
    "            \n",
    "            # Merge: [seq_len, batch_size, n_hid*2]\n",
    "            # Note that we reverse the backward cell's output to align with original direction\n",
    "            # note in \"final\" only prediction, one less dimension\n",
    "            if 'final' in ops['prediction_type']:\n",
    "                merge_index = 1\n",
    "            else:\n",
    "                merge_index = 2\n",
    "            output = tf.concat([G_forw_output, tf.reverse(G_back_output, axis=[0])], axis=merge_index)\n",
    "    \n",
    "            if ops['dropout'] > 0.0:\n",
    "                # note keep_prob = 1.0 - drop_probability (not sure why they implemented it this way)\n",
    "                # tensorflow implementation scales by 1/keep_prob automatically\n",
    "                output = tf.nn.dropout(output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output = output\n",
    "\n",
    "            input_size_final_projection = 2 * ops['hid']\n",
    "            Y_ =  project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "            \n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            \n",
    "            if ops['train_attr_weights_on_pred']:\n",
    "                prediction_parameters += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"ATTRACTOR_WEIGHTS\")\n",
    "            \n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "        else:\n",
    "            G_attractors = {'forw': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = cell(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            input_size_final_projection = ops['hid']\n",
    "            \n",
    "            if ops['dropout'] > 0.0:\n",
    "                output = tf.nn.dropout(G_forw_output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output = G_forw_output\n",
    "            \n",
    "            Y_ = project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "\n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            \n",
    "            if ops['train_attr_weights_on_pred']:\n",
    "                prediction_parameters += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"ATTRACTOR_WEIGHTS\")\n",
    "            \n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "\n",
    "            \n",
    "        mask_op = tf.cast(tf.sign(Y), dtype=tf.float32)\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "            # TODO: make a class for all \"best\" quantities (a lot of space)\n",
    "            saved_train_acc = []\n",
    "            saved_test_acc = []\n",
    "            saved_epoch = []\n",
    "            saved_att_loss = []\n",
    "            saved_entropy_final = []\n",
    "            saved_val_acc = []\n",
    "            saved_val_loss = []\n",
    "            saved_traini_loss = []\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # Start training\n",
    "            for replication in range(N_REPLICATIONS):\n",
    "                print(\"********** replication \", replication, \" **********\")\n",
    "                early_stopper = EarlyStopper(EARLY_STOPPING_PATIENCE, DISPLAY_EPOCH)\n",
    "                [X_full_train, Y_full_train, X_test, Y_test, X_val, Y_val, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                                                           INPUT_NOISE_LEVEL, TASK, ops)\n",
    "                # Take Only part of dataset:\n",
    "                all_ids = range(len(X_full_train))\n",
    "                np.random.shuffle(all_ids)\n",
    "                train_part = int(dataset_part * len(X_full_train))\n",
    "                ids_to_take = all_ids[0:train_part]\n",
    "                ids_for_val = all_ids[train_part:int(train_part + 0.2*train_part)]\n",
    "                if len(ids_to_take) > X_full_train.shape[0]:\n",
    "                    ids_to_take = range(X_full_train.shape[0])\n",
    "                X_train = X_full_train[ids_to_take, :]\n",
    "                Y_train = Y_full_train[ids_to_take, :]\n",
    "                \n",
    "                if BATCH_SIZE < len(X_train):\n",
    "                    ops['attractor_regularization_lambda'] = ops['attractor_regularization_lambda']/(len(X_train)*1.0/BATCH_SIZE)\n",
    "                    print(ops['attractor_regularization_lambda'])\n",
    "                \n",
    "                X_val, Y_val = X_full_train[ids_for_val,:], Y_full_train[ids_for_val,:]\n",
    "                \n",
    "                N_TRAIN = len(X_train)\n",
    "                print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)\n",
    "\n",
    "                # Log Path init-n:\n",
    "                COMMENT = 'dataset_starvation_experiment'\n",
    "                MODEL_NAME_FILE = '{}_(att_iter{}__bidir{}__drop{})_{}.txt'.format(ops['problem_type'],\n",
    "                                                                                   ops['n_attractor_iterations'],\n",
    "                                                                                   ops['bidirectional'],\n",
    "                                                                                   ops['dropout'],\n",
    "                                                                                   COMMENT)\n",
    "                LOG_DIRECTORY = 'experiments/logs/{}'.format(MODEL_NAME_FILE)\n",
    "                MODEL_DIRECTORY = 'experiments/logs/{}_{}'.format(datetime.date.today(), MODEL_NAME_FILE)\n",
    "                print_into_log(LOG_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN))\n",
    "                print_into_log(MODEL_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN), supress=True)\n",
    "\n",
    "                sess.run(init)  # Run the initializer\n",
    "\n",
    "                train_prediction_loss = True\n",
    "                best_train_acc = -1000.\n",
    "                best_test_acc = 0\n",
    "                best_entropy = 0.0\n",
    "                best_att_loss = 0\n",
    "                best_train_loss = 0\n",
    "                best_val_loss = 0.0\n",
    "                best_val_acc = 0.0\n",
    "                best_epoch = 0\n",
    "                for epoch in range(1, TRAINING_EPOCHS + 2):\n",
    "                    if (epoch - 1) % DISPLAY_EPOCH == 0:\n",
    "                        # TRAIN set:\n",
    "                        ploss, train_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        # TEST set:\n",
    "                        test_acc = batch_tensor_collect(sess, [accuracy], X, Y, X_test, Y_test, BATCH_SIZE)[0]\n",
    "    \n",
    "                        # Validation set & Early stopping:\n",
    "                        ploss_val, val_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                  X, Y, X_val, Y_val, BATCH_SIZE)\n",
    "            \n",
    "                        # Precistion/Recall:\n",
    "                        if ops['problem_type'] == 'ner_german':\n",
    "                            y_pred, y_true, mask_val = batch_tensor_collect(sess, [Y_, Y, mask_op],\n",
    "                                                                X, Y, X_test, Y_test, BATCH_SIZE)\n",
    "                            y_pred = np.argmax(y_pred, axis=2)\n",
    "                            \n",
    "                            Y_pred_flat = np.extract(mask_val.astype(bool), y_pred)\n",
    "                            Y_test_flat = np.extract(mask_val.astype(bool), y_true)\n",
    "                            print(\"PRECISION:\",compute_f1(Y_pred_flat, Y_test_flat, maps['id2tag']))\n",
    "                            \n",
    "                        print(early_stopper.patience, early_stopper.best, ploss_val)\n",
    "                        early_stopper.update(ploss_val)\n",
    "                        if early_stopper.patience_ran_out():\n",
    "                            print_into_log(LOG_DIRECTORY, \"STOPPED EARLY AT {}\".format(epoch))\n",
    "                            break\n",
    "\n",
    "                        # ATTRACTOR(s) LOSS\n",
    "                        aloss = {}\n",
    "                        entropy = 0\n",
    "                        hid_vals_arr = batch_tensor_collect(sess, [A['h_net_seq_flat'] for att_name, A in\n",
    "                                                                   G_attractors.items()],\n",
    "                                                            X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        h_clean_val_arr = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            A = G_attractors[attractor_name]\n",
    "                            a_loss_val = []\n",
    "                            n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "                            for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "                                a_loss_val.append(\n",
    "                                    sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "                            aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "\n",
    "#                             entropy[attractor_name] = \"{:.4f}\".format(\n",
    "#                                 compute_entropy_fullvec(h_clean_val_arr[i], ops, n_bins=8))\n",
    "\n",
    "                        # Print training information:\n",
    "                        print_into_log(LOG_DIRECTORY, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + get_training_progress_comment(epoch, ploss, aloss, ploss_val, val_acc, train_acc,\n",
    "                                                                     test_acc, entropy))\n",
    "                        # Update the logs:\n",
    "                       \n",
    "                        #                 if ops['record_mutual_information']:\n",
    "                        # #                     h_attractor_val, h_clean_val = sess.run([h_attractor_collection, h_clean_seq_flat],\n",
    "                        # #                                                                    feed_dict={X: X_train, Y: Y_train})\n",
    "                        #                     # TODO: h_attractor_collection reshapeing masking.\n",
    "                        #                     h_attractor_val = None\n",
    "                        #                     h_clean_val = batch_tensor_collect(sess, [h_clean_seq_flat],\n",
    "                        #                                                                         X, Y, X_train, Y_train, BATCH_SIZE)[0]\n",
    "                        #                     MIS.update(ploss, aloss, train_acc, test_acc, np.tanh(hid_vals), h_attractor_val, h_clean_val)\n",
    "\n",
    "                        if (val_acc > best_val_acc):\n",
    "                            best_train_acc = train_acc\n",
    "                            best_test_acc = test_acc\n",
    "                            best_att_loss = aloss\n",
    "                            best_epoch = epoch\n",
    "                            best_val_acc = val_acc\n",
    "\n",
    "                            best_val_loss = ploss_val\n",
    "                            best_train_loss = ploss\n",
    "                            if ops['save_best_model']:\n",
    "                                save_path = saver.save(sess, MODEL_DIRECTORY)\n",
    "                            best_entropy = entropy\n",
    "                        if (1.0 - 1e-15 < 0.0):\n",
    "                            print('reached_peak')\n",
    "                            break\n",
    "\n",
    "                    if epoch > 1 and LOSS_SWITCH_FREQ > 0 \\\n",
    "                            and (epoch - 1) % LOSS_SWITCH_FREQ == 0:\n",
    "                        train_prediction_loss = not train_prediction_loss\n",
    "\n",
    "                    # MODEL TRAINING\n",
    "                    batches = get_batches(BATCH_SIZE, X_train, Y_train)\n",
    "                    for (batch_x, batch_y) in batches:\n",
    "                        if (LOSS_SWITCH_FREQ == 0 or train_prediction_loss):\n",
    "                            # Optimize all parameters except for attractor weights\n",
    "                            _ = sess.run([pred_train_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y})\n",
    "                        # Attractor:\n",
    "                        if (N_ATTRACTOR_STEPS > 0):\n",
    "                            batch_hid_vals = sess.run([A['h_net_seq_flat'] for att_name, A in G_attractors.items()],\n",
    "                                                      feed_dict={X:batch_x,  Y:batch_y})\n",
    "\n",
    "                            for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                A = G_attractors[attractor_name]\n",
    "                                _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: batch_hid_vals[i]})\n",
    "                                \n",
    "                            \n",
    "                            # TODO: redo how you did it before in the other version\n",
    "                            # Don't stop until the att_loss is below 1\n",
    "#                             print(aloss.values()[0])\n",
    "#                             while float(aloss.values()[0]) > 1.0:\n",
    "#                                 for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "#                                     A = G_attractors[attractor_name]\n",
    "#                                     a_loss_val = []\n",
    "#                                     n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "#                                     for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "#                                         a_loss_val.append(\n",
    "#                                             sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "#                                     aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "#                                 print(aloss.values()[0])\n",
    "#                                 # training procedure\n",
    "#                                 batch_hid_vals = sess.run([A['h_net_seq_flat'] for att_name, A in G_attractors.items()],\n",
    "#                                                           feed_dict={X:batch_x,  Y:batch_y})\n",
    "\n",
    "#                                 for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "#                                     A = G_attractors[attractor_name]\n",
    "#                                     _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: batch_hid_vals[i]})\n",
    "                            \n",
    "                print(\"Optimization Finished!\")\n",
    "\n",
    "                if (REPORT_BEST_TRAIN_PERFORMANCE):\n",
    "                    saved_train_acc.append(best_train_acc)\n",
    "                    saved_test_acc.append(best_test_acc)\n",
    "                    saved_att_loss.append(best_att_loss)\n",
    "                    saved_entropy_final.append(best_entropy)\n",
    "                    saved_epoch.append(best_epoch)\n",
    "\n",
    "                    saved_val_acc.append(best_val_acc)\n",
    "                    saved_val_loss.append(best_val_loss)\n",
    "                    saved_traini_loss.append(best_train_loss)\n",
    "                else:\n",
    "                    saved_train_acc.append(train_acc)\n",
    "                    saved_test_acc.append(test_acc)\n",
    "                    #             saved_att_loss.append(aloss)\n",
    "\n",
    "            save_results(ops, saved_epoch, saved_train_acc, saved_test_acc, saved_att_loss, saved_entropy_final, saved_val_acc,\n",
    "                 saved_val_loss, saved_traini_loss, N_TRAIN, N_TEST, SEQ_LEN, comment=COMMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = GRU_attractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'X' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7b6358abf769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/denis/Documents/attractor_net_notebooks/graph_init.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ops, inputs, direction, suffix)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                       \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attractor_tgt_net'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# tensor inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'X' referenced before assignment"
     ]
    }
   ],
   "source": [
    "cell(1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
