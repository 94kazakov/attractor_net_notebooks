{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "L2 norm\n",
      "********** replication  0  **********\n",
      "parity\n",
      "(32, 5, 1) (32, 1) (0, 5, 1) (0, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tGRU bidir(False), task: parity\n",
      "    hid: \t\t\t10,\n",
      "    h_hid: \t\t\t15\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.2\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_norm(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t32/32, SEQ_LEN: 5\n",
      "Logged Successfully: \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-aa9637d2bfb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    398\u001b[0m                         \u001b[0;31m# Validation set & Early stopping:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                         ploss_val, val_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n\u001b[0;32m--> 400\u001b[0;31m                                                                   X, Y, X_val, Y_val, BATCH_SIZE)\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                         \u001b[0;31m# Precistion/Recall:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/denis/Documents/attractor_net_notebooks/tensorflow_helpers.pyc\u001b[0m in \u001b[0;36mbatch_tensor_collect\u001b[0;34m(sess, input_tensors, X, Y, X_data, Y_data, batch_size)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for actual tensor collections, merge batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for just values, find the average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#!/usr/local/bin/python\n",
    "# This version of the code trains the attractor connections with a separate\n",
    "# objective function than the objective function used to train all other weights\n",
    "# in the network (on the prediction task).\n",
    "\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload\n",
    "\n",
    "from tensorflow_helpers import *\n",
    "from data_generator import generate_examples, pick_task\n",
    "\n",
    "from helper_functions import get_batches, load_pretrained_embeddings, \\\n",
    "    get_model_type_str, translate_ids_to_words, \\\n",
    "    save_results, print_into_log, print_some_translated_sentences, \\\n",
    "    get_training_progress_comment\n",
    "from graph_init import GRU_attractor\n",
    "\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, patience_max, disp_epoch, min_delta = 0.00):\n",
    "        self.best = 1e10\n",
    "        self.patience = 0  # our patience\n",
    "        self.patience_max = patience_max\n",
    "        self.display_epoch = disp_epoch\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def update(self, current):\n",
    "        if self.best > current:\n",
    "            self.best = current\n",
    "            self.patience = 0\n",
    "        elif abs(self.best - current) > self.min_delta:\n",
    "            self.patience += 1\n",
    "\n",
    "    def patience_ran_out(self):\n",
    "        if self.patience*self.display_epoch > self.patience_max:\n",
    "            return True\n",
    "        else:\n",
    "            False\n",
    "            \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "ops = {\n",
    "    'model_type': \"TANH\",  # OPTIONS: vanilla, LSTM_raw, LSTM_tensorflow, LSTM_attractor\n",
    "    'hid': 50,\n",
    "    'in': None,  # TBD\n",
    "    'out': 1,\n",
    "    #         'batch_size':n_examples, #since the sequences are 1-dimensional it's easier to just run them all at once\n",
    "    'n_attractor_iterations': 15,\n",
    "    'attractor_dynamics': \"projection2\",  # OPTIONS:  \"\" (for no attractor dynamics),\n",
    "    #           \"direct\" (simple attractor weights applied to hidden states directly, trained with noise addition)\n",
    "    #           \"projection\" (project the hidden state into a separate space via weights, do attraction, project back)\n",
    "    #           \"helper_hidden\" (hidden-hidden neurons) - IMPORTANT: don't forget to add h_hid number\n",
    "    'h_hid': 100,  # helper hidden for \"helper hidden\" \"attractory_dynamics\" mode\n",
    "    'attractor_noise_level': 0.5,\n",
    "    'attractor_noise_type': \"bernoilli\",  # OPTIONS: \"gaussian\", \"dropout\", \"random_drop\"\n",
    "\n",
    "    'train_attr_weights_on_pred': True,  \n",
    "\n",
    "    'attractor_regularization': \"l2_regularization\",  # OPTIONS: \"l2_regularization\", \"l2_norm\"\n",
    "    'attractor_regularization_lambda': 0.0,\n",
    "\n",
    "    'record_mutual_information': True,\n",
    "    'problem_type': \"pos_brown\",  # OPTIONS: parity, parity_length, majority, reber, kazakov, pos_brown, ner_german, sentiment_imdb, topic_classification\n",
    "    'masking': True,#\"seq\", \"final\"\n",
    "    'prediction_type': 'seq', #'seq', 'final', 'final_class'\n",
    "    'seq_len': None,\n",
    "\n",
    "    'save_best_model': True,\n",
    "    'reshuffle_data_each_replication': False,  # relevant for POS datasets (since they are loaded from files)\n",
    "    'test_partition': 0.3,\n",
    "    'lrate': 0.001,  # was 0.008\n",
    "\n",
    "    # NLP related (pos_brown task)\n",
    "    'bidirectional': False,\n",
    "    'embedding_size': 100,\n",
    "    'load_word_embeddings': True,\n",
    "    'train_word_embeddings': False,\n",
    "    'trainable_logic_symbols': 2, #make first *N* embeddings trainable(Pad, unknown, start symbols make it a separate matrix and trainable)\n",
    "    'input_type': \"embed\",  # embed&prior, embed, prior\n",
    "    'dropout': 0.2  # in range(0,1)\n",
    "}\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# SEQ_LEN = 12 # number of bits in input sequence\n",
    "N_HIDDEN = ops['hid']  # number of hidden units\n",
    "N_H_HIDDEN = ops['h_hid']\n",
    "TASK = ops['problem_type']\n",
    "ARCH = ops['model_type']  # hidden layer type: 'GRU' or 'tanh'\n",
    "NOISE_LEVEL = ops['attractor_noise_level']\n",
    "# noise in training attractor net\n",
    "# if >=0, Gaussian with std dev NOISE_LEVEL\n",
    "# if < 0, Bernoulli dropout proportion -NOISE_LEVEL\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "INPUT_NOISE_LEVEL = 0.1\n",
    "ATTRACTOR_TYPE = ops['attractor_dynamics']\n",
    "N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "# number of time steps in attractor dynamics\n",
    "# if = 0, then no attractor net\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# ATTR_WEIGHT_CONSTRAINTS = True\n",
    "# True: make attractor weights symmetric and have zero diag\n",
    "# False: unconstrained\n",
    "TRAIN_ATTR_WEIGHTS_ON_PREDICTION = False\n",
    "# True: train attractor weights on attractor net _and_ prediction\n",
    "REPORT_BEST_TRAIN_PERFORMANCE = True\n",
    "# True: save the train/test perf on the epoch for which train perf was best\n",
    "LOSS_SWITCH_FREQ = 1\n",
    "# how often (in epochs) to switch between attractor\n",
    "# and prediction loss\n",
    "\n",
    "ops, SEQ_LEN, N_INPUT, N_CLASSES, N_TRAIN, N_TEST = pick_task(ops['problem_type'],\n",
    "                                                              ops)  # task (parity, majority, reber, kazakov)\n",
    "\n",
    "# Training Parameters\n",
    "\n",
    "TRAINING_EPOCHS = 5000\n",
    "N_REPLICATIONS = 5\n",
    "BATCH_SIZE = 8500\n",
    "DISPLAY_EPOCH = 1\n",
    "EARLY_STOPPING_THRESH = 0.0 # 1e-3 for POS, 0.03 for Sentiment\n",
    "EARLY_STOPPING_PATIENCE = 50  # in epochs\n",
    "EARLY_STOPPING_MINIMUM_EPOCH = 0\n",
    "\n",
    "# NOTEBOOK CODE\n",
    "\n",
    "######### MAIN CODE #############################################################\n",
    "#0.02, 0.05, 0.1, 0.2, 0.35, 0.5, \n",
    "for dataset_part in [0.062289, 0.186869, 0.24915, 0.498]: # 2, 4, 8, 16\n",
    "    for attractor_steps in [15, 0]:\n",
    "#     for att_reg in [0.0]:\n",
    "        # the tf seed needs to be within the context of the graph.\n",
    "        tf.reset_default_graph()\n",
    "        np.random.seed(11)\n",
    "        tf.set_random_seed(11)\n",
    "        ops['n_attractor_iterations'] = attractor_steps\n",
    "        N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "\n",
    "        #\n",
    "        # PLACEHOLDERS\n",
    "        #\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            # last dimension is left singular, tensorflow will expect it to be an id number, not 1-hot embed\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN], name='Y')\n",
    "        elif ops['problem_type'] == 'sentiment_imdb':\n",
    "             # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            Y = tf.placeholder(\"int64\", [None, N_CLASSES], name='Y')\n",
    "        elif ops['problem_type'] == 'topic_classification':\n",
    "             # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            Y = tf.placeholder(\"int64\", [None, 1], name='Y')\n",
    "        elif ops['problem_type'] == 'ner_german':\n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN])\n",
    "        else:  # single output \n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, N_CLASSES])\n",
    "        attractor_tgt_net = tf.placeholder(\"float\", [None, N_HIDDEN], name='attractor_tgt')\n",
    "\n",
    "        # Embedding matrix initialization\n",
    "        if 'pos' in ops['problem_type'] or 'sentiment' in ops['problem_type'] or ops['problem_type'] == \"topic_classification\":\n",
    "            [_, _, _, _, _, _, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                         INPUT_NOISE_LEVEL, TASK, ops)\n",
    "\n",
    "            if ops['load_word_embeddings']:\n",
    "                embeddings_loaded, _ = load_pretrained_embeddings('data/glove.6B.{}d.txt'.format(ops['embedding_size']),\n",
    "                                                               maps, ops)\n",
    "                if ops['trainable_logic_symbols'] > 0:\n",
    "                    with tf.variable_scope(\"TASK_WEIGHTS\"):\n",
    "                        symbols_embedding = tf.get_variable(\"symb_embedding\",\n",
    "                                                initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                                shape=[ops['trainable_logic_symbols'], ops['embedding_size']],\n",
    "                                                dtype=tf.float32,\n",
    "                                                trainable=True)\n",
    "                    \n",
    "                word_embedding = tf.get_variable(\"embedding\",\n",
    "                                            initializer=embeddings_loaded,\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=ops['train_word_embeddings'])\n",
    "                if ops['trainable_logic_symbols'] > 0:\n",
    "                    embedding = tf.concat([symbols_embedding, word_embedding], axis=0)\n",
    "                else:\n",
    "                    embedding = word_embedding\n",
    "            else:  # initialize randomly\n",
    "                embedding = tf.get_variable(\"embedding\",\n",
    "                                            initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                            shape=[ops['vocab_size'], ops['embedding_size']],\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=ops['train_word_embeddings'])\n",
    "            embed_lookup = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "            # load priors information\n",
    "            if ops['input_type'] == 'prior' or ops['input_type'] == 'embed&prior':\n",
    "                id2prior = maps['id2prior']\n",
    "                word2id = maps['word2id']\n",
    "                priors = np.zeros([len(id2prior), len(id2prior[0])]).astype(\"float32\")\n",
    "                for id, prior in id2prior.items():\n",
    "                    priors[id] = prior\n",
    "                priors_op = tf.get_variable(\"priors\",\n",
    "                                            initializer=priors,\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=False)\n",
    "                prior_lookup = tf.nn.embedding_lookup(priors_op, X)\n",
    "\n",
    "            if ops['input_type'] == 'embed':\n",
    "                embed = embed_lookup\n",
    "            elif ops['input_type'] == 'prior':\n",
    "                embed = prior_lookup\n",
    "            elif ops['input_type'] == 'embed&prior':\n",
    "                embed = tf.concat([embed_lookup, prior_lookup], axis=2)\n",
    "\n",
    "        # Graph + all the training variables\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            net_inputs = {'X': embed, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        elif ops['problem_type'] == 'sentiment_imdb' or ops['problem_type'] == 'topic_classification':\n",
    "            net_inputs = {'X': embed, 'mask': X, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        else:\n",
    "            net_inputs = {'X': X, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "\n",
    "        if ops['bidirectional']:\n",
    "            G_attractors = {'forw': [], 'back': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            # Backward:\n",
    "            G_back = GRU_attractor(ops, inputs=net_inputs, direction='backward', suffix=names[1])\n",
    "            attr_loss_op_back = G_back.attr_loss_op\n",
    "            attr_train_op_back = G_back.attr_train_op\n",
    "            h_clean_seq_flat_back = G_back.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_back = G_back.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['back'] = {'attr_loss_op': attr_loss_op_back, \"attr_train_op\": attr_train_op_back,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_back, 'h_net_seq_flat': h_net_seq_flat_back}\n",
    "            G_back_output = G_back.output\n",
    "\n",
    "            \n",
    "            \n",
    "            # Merge: [seq_len, batch_size, n_hid*2]\n",
    "            # Note that we reverse the backward cell's output to align with original direction\n",
    "            # note in \"final\" only prediction, one less dimension\n",
    "            if 'final' in ops['prediction_type']:\n",
    "                merge_index = 1\n",
    "            else:\n",
    "                merge_index = 2\n",
    "            output = tf.concat([G_forw_output, tf.reverse(G_back_output, axis=[0])], axis=merge_index)\n",
    "    \n",
    "            if ops['dropout'] > 0.0:\n",
    "                # note keep_prob = 1.0 - drop_probability (not sure why they implemented it this way)\n",
    "                # tensorflow implementation scales by 1/keep_prob automatically\n",
    "                output = tf.nn.dropout(output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output = output\n",
    "\n",
    "            input_size_final_projection = 2 * ops['hid']\n",
    "            Y_ =  project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "            \n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            \n",
    "            if ops['train_attr_weights_on_pred']:\n",
    "                prediction_parameters += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"ATTRACTOR_WEIGHTS\")\n",
    "                \n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "        else:\n",
    "            G_attractors = {'forw': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            input_size_final_projection = ops['hid']\n",
    "            \n",
    "            if ops['dropout'] > 0.0:\n",
    "                output = tf.nn.dropout(G_forw_output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output = G_forw_output\n",
    "            \n",
    "            Y_ = project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "\n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            \n",
    "            if ops['train_attr_weights_on_pred']:\n",
    "                prediction_parameters += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"ATTRACTOR_WEIGHTS\")\n",
    "            \n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "\n",
    "            \n",
    "        mask_op = tf.cast(tf.sign(Y), dtype=tf.float32)\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "            # TODO: make a class for all \"best\" quantities (a lot of space)\n",
    "            saved_train_acc = []\n",
    "            saved_test_acc = []\n",
    "            saved_epoch = []\n",
    "            saved_att_loss = []\n",
    "            saved_entropy_final = []\n",
    "            saved_val_acc = []\n",
    "            saved_val_loss = []\n",
    "            saved_traini_loss = []\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # Start training\n",
    "            for replication in range(N_REPLICATIONS):\n",
    "                print(\"********** replication \", replication, \" **********\")\n",
    "                early_stopper = EarlyStopper(EARLY_STOPPING_PATIENCE, DISPLAY_EPOCH)\n",
    "                [X_full_train, Y_full_train, X_test, Y_test, X_val, Y_val, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                                                           INPUT_NOISE_LEVEL, TASK, ops)\n",
    "                # Take Only part of dataset:\n",
    "                all_ids = range(len(X_full_train))\n",
    "                np.random.shuffle(all_ids)\n",
    "                train_part = int(dataset_part * len(X_full_train))\n",
    "                ids_to_take = all_ids[0:train_part]\n",
    "                ids_for_val = all_ids[train_part:int(train_part + 0.2*train_part)]\n",
    "                if len(ids_to_take) > X_full_train.shape[0]:\n",
    "                    ids_to_take = range(X_full_train.shape[0])\n",
    "                X_train = X_full_train[ids_to_take, :]\n",
    "                Y_train = Y_full_train[ids_to_take, :]\n",
    "                \n",
    "                if BATCH_SIZE < len(X_train):\n",
    "                    ops['attractor_regularization_lambda'] = ops['attractor_regularization_lambda']/(len(X_train)*1.0/BATCH_SIZE)\n",
    "                    print(ops['attractor_regularization_lambda'])\n",
    "                \n",
    "                X_val, Y_val = X_full_train[ids_for_val,:], Y_full_train[ids_for_val,:]\n",
    "                \n",
    "                N_TRAIN = len(X_train)\n",
    "                print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)\n",
    "\n",
    "                # Log Path init-n:\n",
    "                COMMENT = 'dataset_starvation_experiment'\n",
    "                MODEL_NAME_FILE = '{}_(att_iter{}__bidir{}__drop{})_{}.txt'.format(ops['problem_type'],\n",
    "                                                                                   ops['n_attractor_iterations'],\n",
    "                                                                                   ops['bidirectional'],\n",
    "                                                                                   ops['dropout'],\n",
    "                                                                                   COMMENT)\n",
    "                LOG_DIRECTORY = 'experiments/logs/{}'.format(MODEL_NAME_FILE)\n",
    "                MODEL_DIRECTORY = 'experiments/logs/{}_{}'.format(datetime.date.today(), MODEL_NAME_FILE)\n",
    "                print_into_log(LOG_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN))\n",
    "                print_into_log(MODEL_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN), supress=True)\n",
    "\n",
    "                sess.run(init)  # Run the initializer\n",
    "\n",
    "                train_prediction_loss = True\n",
    "                best_train_acc = -1000.\n",
    "                best_test_acc = 0\n",
    "                best_entropy = 0.0\n",
    "                best_att_loss = 0\n",
    "                best_train_loss = 0\n",
    "                best_val_loss = 0.0\n",
    "                best_val_acc = 0.0\n",
    "                best_epoch = 0\n",
    "                for epoch in range(1, TRAINING_EPOCHS + 2):\n",
    "                    if (epoch - 1) % DISPLAY_EPOCH == 0:\n",
    "                        # TRAIN set:\n",
    "                        ploss, train_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        # TEST set:\n",
    "                        test_acc = batch_tensor_collect(sess, [accuracy], X, Y, X_test, Y_test, BATCH_SIZE)[0]\n",
    "    \n",
    "                        # Validation set & Early stopping:\n",
    "                        ploss_val, val_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                  X, Y, X_val, Y_val, BATCH_SIZE)\n",
    "            \n",
    "                        # Precistion/Recall:\n",
    "                        if ops['problem_type'] == 'ner_german':\n",
    "                            y_pred, y_true, mask_val = batch_tensor_collect(sess, [Y_, Y, mask_op],\n",
    "                                                                X, Y, X_test, Y_test, BATCH_SIZE)\n",
    "                            y_pred = np.argmax(y_pred, axis=2)\n",
    "                            \n",
    "                            Y_pred_flat = np.extract(mask_val.astype(bool), y_pred)\n",
    "                            Y_test_flat = np.extract(mask_val.astype(bool), y_true)\n",
    "                            print(\"PRECISION:\",compute_f1(Y_pred_flat, Y_test_flat, maps['id2tag']))\n",
    "                            \n",
    "                        print(early_stopper.patience, early_stopper.best, ploss_val)\n",
    "                        early_stopper.update(ploss_val)\n",
    "                        if early_stopper.patience_ran_out():\n",
    "                            print_into_log(LOG_DIRECTORY, \"STOPPED EARLY AT {}\".format(epoch))\n",
    "                            break\n",
    "\n",
    "                        # ATTRACTOR(s) LOSS\n",
    "                        aloss = {}\n",
    "                        entropy = 0\n",
    "                        hid_vals_arr = batch_tensor_collect(sess, [A['h_net_seq_flat'] for att_name, A in\n",
    "                                                                   G_attractors.items()],\n",
    "                                                            X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        h_clean_val_arr = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            A = G_attractors[attractor_name]\n",
    "                            a_loss_val = []\n",
    "                            n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "                            for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "                                a_loss_val.append(\n",
    "                                    sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "                            aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "\n",
    "#                             entropy[attractor_name] = \"{:.4f}\".format(\n",
    "#                                 compute_entropy_fullvec(h_clean_val_arr[i], ops, n_bins=8))\n",
    "\n",
    "                        # Print training information:\n",
    "                        print_into_log(LOG_DIRECTORY, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + get_training_progress_comment(epoch, ploss, aloss, ploss_val, val_acc, train_acc,\n",
    "                                                                     test_acc, entropy))\n",
    "                        # Update the logs:\n",
    "                       \n",
    "                        #                 if ops['record_mutual_information']:\n",
    "                        # #                     h_attractor_val, h_clean_val = sess.run([h_attractor_collection, h_clean_seq_flat],\n",
    "                        # #                                                                    feed_dict={X: X_train, Y: Y_train})\n",
    "                        #                     # TODO: h_attractor_collection reshapeing masking.\n",
    "                        #                     h_attractor_val = None\n",
    "                        #                     h_clean_val = batch_tensor_collect(sess, [h_clean_seq_flat],\n",
    "                        #                                                                         X, Y, X_train, Y_train, BATCH_SIZE)[0]\n",
    "                        #                     MIS.update(ploss, aloss, train_acc, test_acc, np.tanh(hid_vals), h_attractor_val, h_clean_val)\n",
    "\n",
    "                        if (val_acc > best_val_acc):\n",
    "                            best_train_acc = train_acc\n",
    "                            best_test_acc = test_acc\n",
    "                            best_att_loss = aloss\n",
    "                            best_epoch = epoch\n",
    "                            best_val_acc = val_acc\n",
    "\n",
    "                            best_val_loss = ploss_val\n",
    "                            best_train_loss = ploss\n",
    "                            if ops['save_best_model']:\n",
    "                                save_path = saver.save(sess, MODEL_DIRECTORY)\n",
    "                            best_entropy = entropy\n",
    "                        if (1.0 - 1e-15 < 0.0):\n",
    "                            print('reached_peak')\n",
    "                            break\n",
    "\n",
    "                    if epoch > 1 and LOSS_SWITCH_FREQ > 0 \\\n",
    "                            and (epoch - 1) % LOSS_SWITCH_FREQ == 0:\n",
    "                        train_prediction_loss = not train_prediction_loss\n",
    "\n",
    "                    # MODEL TRAINING\n",
    "                    batches = get_batches(BATCH_SIZE, X_train, Y_train)\n",
    "                    for (batch_x, batch_y) in batches:\n",
    "                        if (LOSS_SWITCH_FREQ == 0 or train_prediction_loss):\n",
    "                            # Optimize all parameters except for attractor weights\n",
    "                            _ = sess.run([pred_train_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y})\n",
    "                            \n",
    "                        \n",
    "                        # Attractor:\n",
    "                        if (N_ATTRACTOR_STEPS > 0):\n",
    "                            # training procedure\n",
    "                            batch_hid_vals = sess.run([A['h_net_seq_flat'] for att_name, A in G_attractors.items()],\n",
    "                                                      feed_dict={X:batch_x,  Y:batch_y})\n",
    "\n",
    "                            for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                A = G_attractors[attractor_name]\n",
    "                                _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: batch_hid_vals[i]})\n",
    "\n",
    "                            \n",
    "                            # TODO: redo how you did it before in the other version\n",
    "                            # Don't stop until the att_loss is below 1\n",
    "                            print(aloss.values()[0])\n",
    "                            while float(aloss.values()[0]) > 1.0:\n",
    "                                for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                    A = G_attractors[attractor_name]\n",
    "                                    a_loss_val = []\n",
    "                                    n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "                                    for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "                                        a_loss_val.append(\n",
    "                                            sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "                                    aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "                                print(aloss.values()[0])\n",
    "                                # training procedure\n",
    "                                batch_hid_vals = sess.run([A['h_net_seq_flat'] for att_name, A in G_attractors.items()],\n",
    "                                                          feed_dict={X:batch_x,  Y:batch_y})\n",
    "\n",
    "                                for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                    A = G_attractors[attractor_name]\n",
    "                                    _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: batch_hid_vals[i]})\n",
    "                print(\"Optimization Finished!\")\n",
    "\n",
    "                if (REPORT_BEST_TRAIN_PERFORMANCE):\n",
    "                    saved_train_acc.append(best_train_acc)\n",
    "                    saved_test_acc.append(best_test_acc)\n",
    "                    saved_att_loss.append(best_att_loss)\n",
    "                    saved_entropy_final.append(best_entropy)\n",
    "                    saved_epoch.append(best_epoch)\n",
    "\n",
    "                    saved_val_acc.append(best_val_acc)\n",
    "                    saved_val_loss.append(best_val_loss)\n",
    "                    saved_traini_loss.append(best_train_loss)\n",
    "                else:\n",
    "                    saved_train_acc.append(train_acc)\n",
    "                    saved_test_acc.append(test_acc)\n",
    "                    #             saved_att_loss.append(aloss)\n",
    "\n",
    "            save_results(ops, saved_epoch, saved_train_acc, saved_test_acc, saved_att_loss, saved_entropy_final, saved_val_acc,\n",
    "                 saved_val_loss, saved_traini_loss, N_TRAIN, N_TEST, SEQ_LEN, comment=COMMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/corpus_brown/data_params.pickle\", 'rb') as handle:\n",
    "    dataset_params = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params['total_examples']\n",
    "dataset_params['n_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':3}\n",
    "print(a.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
