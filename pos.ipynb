{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "L2 reg-n\n",
      "********** replication  0  **********\n",
      "majority\n",
      "(256, 10, 1) (256, 1) (0, 10, 1) (0, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: majority\n",
      "    hid: \t\t\t10,\n",
      "    h_hid: \t\t\t20\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t256/768, SEQ_LEN: 10\n",
      "Logged Successfully: \n",
      "0 10000000000.0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:25epoch=0; Loss Pred=1.0499; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.9791'}; Train Acc=0.465; Test Acc=0.5026; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:28epoch=100; Loss Pred=0.1074; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.6204'}; Train Acc=0.988; Test Acc=0.9336; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:30epoch=200; Loss Pred=0.0303; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.2984'}; Train Acc=1.000; Test Acc=0.9492; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:33epoch=300; Loss Pred=0.0506; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0163'}; Train Acc=0.992; Test Acc=0.9544; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:35epoch=400; Loss Pred=0.0170; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.8581'}; Train Acc=1.000; Test Acc=0.9570; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:38epoch=500; Loss Pred=0.1958; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.9390'}; Train Acc=0.922; Test Acc=0.8919; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:41epoch=600; Loss Pred=0.0172; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6178'}; Train Acc=1.000; Test Acc=0.9792; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:43epoch=700; Loss Pred=0.6268; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0073'}; Train Acc=0.812; Test Acc=0.8438; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:45epoch=800; Loss Pred=0.2250; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0254'}; Train Acc=0.938; Test Acc=0.9310; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:48epoch=900; Loss Pred=0.0217; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6146'}; Train Acc=1.000; Test Acc=0.9727; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:50epoch=1000; Loss Pred=0.0142; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4949'}; Train Acc=1.000; Test Acc=0.9740; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:52epoch=1100; Loss Pred=0.0117; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4393'}; Train Acc=1.000; Test Acc=0.9701; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:55epoch=1200; Loss Pred=0.0098; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3893'}; Train Acc=1.000; Test Acc=0.9740; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:29:57epoch=1300; Loss Pred=0.0079; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3619'}; Train Acc=1.000; Test Acc=0.9792; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:00epoch=1400; Loss Pred=0.6145; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3602'}; Train Acc=0.824; Test Acc=0.8646; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:02epoch=1500; Loss Pred=0.0470; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3993'}; Train Acc=0.984; Test Acc=0.9557; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:05epoch=1600; Loss Pred=0.0075; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3288'}; Train Acc=1.000; Test Acc=0.9831; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:07epoch=1700; Loss Pred=0.0066; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2850'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:11epoch=1800; Loss Pred=0.0061; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2729'}; Train Acc=1.000; Test Acc=0.9831; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:13epoch=1900; Loss Pred=0.0055; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2545'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:16epoch=2000; Loss Pred=0.0053; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2401'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:18epoch=2100; Loss Pred=0.2560; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4245'}; Train Acc=0.918; Test Acc=0.9089; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:21epoch=2200; Loss Pred=0.0095; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2560'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:23epoch=2300; Loss Pred=0.0059; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2324'}; Train Acc=1.000; Test Acc=0.9896; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:26epoch=2400; Loss Pred=0.0047; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2198'}; Train Acc=1.000; Test Acc=0.9935; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:28epoch=2500; Loss Pred=0.0038; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2131'}; Train Acc=1.000; Test Acc=0.9935; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:31epoch=2600; Loss Pred=0.0033; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2076'}; Train Acc=1.000; Test Acc=0.9909; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:33epoch=2700; Loss Pred=0.0030; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2172'}; Train Acc=1.000; Test Acc=0.9909; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:36epoch=2800; Loss Pred=0.0027; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1976'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:38epoch=2900; Loss Pred=0.0024; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2140'}; Train Acc=1.000; Test Acc=0.9896; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:41epoch=3000; Loss Pred=0.0024; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2064'}; Train Acc=1.000; Test Acc=0.9896; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:44epoch=3100; Loss Pred=0.0021; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2110'}; Train Acc=1.000; Test Acc=0.9896; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:48epoch=3200; Loss Pred=0.0020; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2002'}; Train Acc=1.000; Test Acc=0.9896; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:51epoch=3300; Loss Pred=1.4432; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '7.5691'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:53epoch=3400; Loss Pred=0.9426; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.7643'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:56epoch=3500; Loss Pred=0.9295; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '10.6374'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:30:58epoch=3600; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.5241'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:01epoch=3700; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.4608'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:05epoch=3800; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.5383'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:08epoch=3900; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.7065'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:11epoch=4000; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.8401'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:14epoch=4100; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.4722'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:18epoch=4200; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.3783'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:20epoch=4300; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.5440'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:23epoch=4400; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.8583'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:25epoch=4500; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.3623'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:28epoch=4600; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.5177'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:30epoch=4700; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.6572'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:34epoch=4800; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.7553'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:37epoch=4900; Loss Pred=0.9294; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '11.7272'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:40epoch=5000; Loss Pred=0.9298; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '10.4847'}; Train Acc=0.633; Test Acc=0.6211; Entropy=0; Entropy_Test=\n",
      "\n",
      "Optimization Finished!\n",
      "********** replication  1  **********\n",
      "majority\n",
      "(256, 10, 1) (256, 1) (0, 10, 1) (0, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: majority\n",
      "    hid: \t\t\t10,\n",
      "    h_hid: \t\t\t20\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t256/768, SEQ_LEN: 10\n",
      "Logged Successfully: \n",
      "0 10000000000.0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:40epoch=0; Loss Pred=1.1192; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0507'}; Train Acc=0.340; Test Acc=0.3893; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:43epoch=100; Loss Pred=0.3193; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.4895'}; Train Acc=0.871; Test Acc=0.8333; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:46epoch=200; Loss Pred=0.0670; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0296'}; Train Acc=0.992; Test Acc=0.9609; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:49epoch=300; Loss Pred=0.0431; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.8044'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:51epoch=400; Loss Pred=0.6700; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.7369'}; Train Acc=0.789; Test Acc=0.7370; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:54epoch=500; Loss Pred=0.0368; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6795'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:56epoch=600; Loss Pred=1.4703; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.6824'}; Train Acc=0.605; Test Acc=0.6016; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:31:59epoch=700; Loss Pred=0.8340; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.3066'}; Train Acc=0.660; Test Acc=0.6107; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:01epoch=800; Loss Pred=0.8647; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '2.4592'}; Train Acc=0.660; Test Acc=0.6107; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:03epoch=900; Loss Pred=0.4263; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '2.0672'}; Train Acc=0.883; Test Acc=0.8555; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:06epoch=1000; Loss Pred=0.3486; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.2554'}; Train Acc=0.867; Test Acc=0.8385; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:08epoch=1100; Loss Pred=0.1984; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.9738'}; Train Acc=0.926; Test Acc=0.9076; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:11epoch=1200; Loss Pred=0.0411; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.7176'}; Train Acc=1.000; Test Acc=0.9727; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:13epoch=1300; Loss Pred=0.1704; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.6197'}; Train Acc=0.945; Test Acc=0.9284; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:16epoch=1400; Loss Pred=0.2907; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.7675'}; Train Acc=0.863; Test Acc=0.8919; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:18epoch=1500; Loss Pred=0.0447; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4790'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:21epoch=1600; Loss Pred=0.2960; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4212'}; Train Acc=0.902; Test Acc=0.8893; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:23epoch=1700; Loss Pred=0.0280; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3844'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:25epoch=1800; Loss Pred=0.0267; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3721'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:28epoch=1900; Loss Pred=0.0201; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3392'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:30epoch=2000; Loss Pred=0.1129; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.5317'}; Train Acc=0.945; Test Acc=0.9414; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:33epoch=2100; Loss Pred=0.0162; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3167'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:35epoch=2200; Loss Pred=0.0187; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3102'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:38epoch=2300; Loss Pred=0.0548; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.3291'}; Train Acc=0.984; Test Acc=0.9701; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:40epoch=2400; Loss Pred=0.0147; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2777'}; Train Acc=1.000; Test Acc=0.9883; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:43epoch=2500; Loss Pred=0.0134; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2826'}; Train Acc=1.000; Test Acc=0.9896; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:45epoch=2600; Loss Pred=0.0112; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2755'}; Train Acc=1.000; Test Acc=0.9896; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:48epoch=2700; Loss Pred=0.0101; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2722'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:50epoch=2800; Loss Pred=0.0083; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2729'}; Train Acc=1.000; Test Acc=0.9909; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:52epoch=2900; Loss Pred=0.2327; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.4337'}; Train Acc=0.930; Test Acc=0.9115; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:55epoch=3000; Loss Pred=0.0101; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2670'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:32:57epoch=3100; Loss Pred=0.0067; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2489'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:00epoch=3200; Loss Pred=0.0051; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2415'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:02epoch=3300; Loss Pred=0.0042; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2302'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:05epoch=3400; Loss Pred=0.0039; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2302'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:07epoch=3500; Loss Pred=0.0035; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2224'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:09epoch=3600; Loss Pred=0.0031; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2195'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:12epoch=3700; Loss Pred=0.0029; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2291'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:14epoch=3800; Loss Pred=0.0026; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2187'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:17epoch=3900; Loss Pred=0.0024; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2149'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:19epoch=4000; Loss Pred=0.0023; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2034'}; Train Acc=1.000; Test Acc=0.9909; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:21epoch=4100; Loss Pred=0.0020; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2107'}; Train Acc=1.000; Test Acc=0.9935; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:24epoch=4200; Loss Pred=0.0019; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.2018'}; Train Acc=1.000; Test Acc=0.9935; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:26epoch=4300; Loss Pred=0.0017; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1959'}; Train Acc=1.000; Test Acc=0.9961; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:29epoch=4400; Loss Pred=0.0017; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1941'}; Train Acc=1.000; Test Acc=0.9961; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:31epoch=4500; Loss Pred=0.0015; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1935'}; Train Acc=1.000; Test Acc=0.9961; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:33epoch=4600; Loss Pred=0.0015; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1941'}; Train Acc=1.000; Test Acc=0.9935; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:36epoch=4700; Loss Pred=0.0014; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1856'}; Train Acc=1.000; Test Acc=0.9961; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:38epoch=4800; Loss Pred=0.0012; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1839'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:41epoch=4900; Loss Pred=0.0013; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1880'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:43epoch=5000; Loss Pred=0.0012; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '0.1930'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "Optimization Finished!\n",
      "Saved Results Successfully\n",
      "L2 reg-n\n",
      "********** replication  0  **********\n",
      "majority\n",
      "(256, 10, 1) (256, 1) (0, 10, 1) (0, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: majority\n",
      "    hid: \t\t\t10,\n",
      "    h_hid: \t\t\t20\n",
      "    n_attractor_iterations: \t0,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t256/768, SEQ_LEN: 10\n",
      "Logged Successfully: \n",
      "0 10000000000.0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:45epoch=0; Loss Pred=0.9645; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=0.633; Test Acc=0.6198; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:45epoch=100; Loss Pred=0.2115; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=0.945; Test Acc=0.9062; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:46epoch=200; Loss Pred=0.0310; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:46epoch=300; Loss Pred=0.0063; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:47epoch=400; Loss Pred=0.0028; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:47epoch=500; Loss Pred=0.0017; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:47epoch=600; Loss Pred=0.0011; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:48epoch=700; Loss Pred=0.0008; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:48epoch=800; Loss Pred=0.0006; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:49epoch=900; Loss Pred=0.0005; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:49epoch=1000; Loss Pred=0.0004; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:50epoch=1100; Loss Pred=0.0004; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:50epoch=1200; Loss Pred=0.0003; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:50epoch=1300; Loss Pred=0.0003; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:51epoch=1400; Loss Pred=0.0002; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:51epoch=1500; Loss Pred=0.0002; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:52epoch=1600; Loss Pred=0.0002; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:52epoch=1700; Loss Pred=0.0002; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:52epoch=1800; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:53epoch=1900; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:53epoch=2000; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:54epoch=2100; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:54epoch=2200; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:54epoch=2300; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:55epoch=2400; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:55epoch=2500; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:56epoch=2600; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:56epoch=2700; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:56epoch=2800; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:57epoch=2900; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:57epoch=3000; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:58epoch=3100; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:58epoch=3200; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:58epoch=3300; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:59epoch=3400; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:33:59epoch=3500; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:00epoch=3600; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:00epoch=3700; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:00epoch=3800; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:01epoch=3900; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9883; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:01epoch=4000; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9883; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:02epoch=4100; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:02epoch=4200; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:03epoch=4300; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:03epoch=4400; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:03epoch=4500; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:04epoch=4600; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:04epoch=4700; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:05epoch=4800; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:05epoch=4900; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:05epoch=5000; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "Optimization Finished!\n",
      "********** replication  1  **********\n",
      "majority\n",
      "(256, 10, 1) (256, 1) (0, 10, 1) (0, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: majority\n",
      "    hid: \t\t\t10,\n",
      "    h_hid: \t\t\t20\n",
      "    n_attractor_iterations: \t0,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.0\n",
      "    TRAIN/TEST_SIZE: \t256/768, SEQ_LEN: 10\n",
      "Logged Successfully: \n",
      "0 10000000000.0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:05epoch=0; Loss Pred=1.0278; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=0.660; Test Acc=0.6042; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:06epoch=100; Loss Pred=0.3230; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=0.938; Test Acc=0.9167; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:06epoch=200; Loss Pred=0.0500; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=0.992; Test Acc=0.9909; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:07epoch=300; Loss Pred=0.0130; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9961; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:07epoch=400; Loss Pred=0.0054; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9961; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:07epoch=500; Loss Pred=0.0030; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9961; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:08epoch=600; Loss Pred=0.0020; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9961; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:08epoch=700; Loss Pred=0.0015; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9961; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:09epoch=800; Loss Pred=0.0011; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9961; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:09epoch=900; Loss Pred=0.0009; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9948; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:09epoch=1000; Loss Pred=0.0008; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9935; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:10epoch=1100; Loss Pred=0.0006; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:10epoch=1200; Loss Pred=0.0006; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9922; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:11epoch=1300; Loss Pred=0.0005; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9909; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:11epoch=1400; Loss Pred=0.0004; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9896; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:11epoch=1500; Loss Pred=0.0004; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9883; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:12epoch=1600; Loss Pred=0.0003; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:12epoch=1700; Loss Pred=0.0003; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:13epoch=1800; Loss Pred=0.0003; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:13epoch=1900; Loss Pred=0.0003; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9870; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:14epoch=2000; Loss Pred=0.0002; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9857; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:14epoch=2100; Loss Pred=0.0002; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:14epoch=2200; Loss Pred=0.0002; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9844; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:15epoch=2300; Loss Pred=0.0002; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9831; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:15epoch=2400; Loss Pred=0.0002; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:16epoch=2500; Loss Pred=0.0002; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:16epoch=2600; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:16epoch=2700; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:17epoch=2800; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:17epoch=2900; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:18epoch=3000; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:18epoch=3100; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:18epoch=3200; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:19epoch=3300; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:19epoch=3400; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:20epoch=3500; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:20epoch=3600; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:20epoch=3700; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:21epoch=3800; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:21epoch=3900; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:22epoch=4000; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:22epoch=4100; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9805; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:22epoch=4200; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:23epoch=4300; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:23epoch=4400; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:24epoch=4500; Loss Pred=0.0001; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:24epoch=4600; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:25epoch=4700; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:25epoch=4800; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:25epoch=4900; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "0 0 0\n",
      "Logged Successfully: \n",
      "2018-07-27 13:34:26epoch=5000; Loss Pred=0.0000; Val Loss=0.0000; Val Acc=0.0000; Loss Att={'forw': '1.0000'}; Train Acc=1.000; Test Acc=0.9818; Entropy=0; Entropy_Test=\n",
      "\n",
      "Optimization Finished!\n",
      "Saved Results Successfully\n"
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#!/usr/local/bin/python\n",
    "# This version of the code trains the attractor connections with a separate\n",
    "# objective function than the objective function used to train all other weights\n",
    "# in the network (on the prediction task).\n",
    "\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload\n",
    "\n",
    "from tensorflow_helpers import *\n",
    "from data_generator import generate_examples, pick_task\n",
    "\n",
    "from helper_functions import get_batches, load_pretrained_embeddings, \\\n",
    "    get_model_type_str, translate_ids_to_words, \\\n",
    "    save_results, print_into_log, print_some_translated_sentences, \\\n",
    "    get_training_progress_comment\n",
    "from graph_init import GRU_attractor\n",
    "\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, patience_max, disp_epoch, min_delta = 0.00):\n",
    "        self.best = 1e10\n",
    "        self.patience = 0  # our patience\n",
    "        self.patience_max = patience_max\n",
    "        self.display_epoch = disp_epoch\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def update(self, current):\n",
    "        if self.best > current:\n",
    "            self.best = current\n",
    "            self.patience = 0\n",
    "        elif abs(self.best - current) > self.min_delta:\n",
    "            self.patience += 1\n",
    "\n",
    "    def patience_ran_out(self):\n",
    "        if self.patience*self.display_epoch > self.patience_max:\n",
    "            return True\n",
    "        else:\n",
    "            False\n",
    "            \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "ops = {\n",
    "    'model_type': \"TANH\",  # OPTIONS: vanilla, LSTM_raw, LSTM_tensorflow, LSTM_attractor\n",
    "    'hid': 10,\n",
    "    'in': None,  # TBD\n",
    "    'out': 1,\n",
    "    #         'batch_size':n_examples, #since the sequences are 1-dimensional it's easier to just run them all at once\n",
    "    'n_attractor_iterations': 15,\n",
    "    'attractor_dynamics': \"projection2\", \n",
    "    'h_hid': 20,  # helper hidden for \"helper hidden\" \"attractory_dynamics\" mode\n",
    "    'attractor_noise_level': 0.5,\n",
    "    'attractor_noise_type': \"bernoilli\",  # OPTIONS: \"gaussian\", \"dropout\", \"random_drop\"\n",
    "\n",
    "    'train_attr_weights_on_pred': True,\n",
    "    'force_att_loss_below_1': False,\n",
    "    \n",
    "\n",
    "    'attractor_regularization': \"l2_regularization\",  # OPTIONS: \"l2_regularization\", \"l2_norm\"\n",
    "    'attractor_regularization_lambda': 0.0,\n",
    "\n",
    "    'record_mutual_information': True,\n",
    "    'problem_type': \"majority\",  # OPTIONS: parity, parity_length, majority, reber, kazakov, pos_brown, ner_german, sentiment_imdb, topic_classification\n",
    "    'masking': False,#\"seq\", \"final\"\n",
    "    'prediction_type': 'final', #'seq', 'final', 'final_class'\n",
    "    'seq_len': None,\n",
    "\n",
    "    'save_best_model': True,\n",
    "    'reshuffle_data_each_replication': False,  # relevant for POS datasets (since they are loaded from files)\n",
    "    'test_partition': 0.3, # TODO: Irrelevant most of the time, move into the code for making the dataset\n",
    "    'lrate': 0.001,  # was 0.008\n",
    "\n",
    "    # NLP related (pos_brown task)\n",
    "    'bidirectional': False,\n",
    "    'embedding_size': 100,\n",
    "    'load_word_embeddings': False,\n",
    "    'train_word_embeddings': False,\n",
    "    'trainable_logic_symbols': 2, #make first *N* embeddings trainable(Pad, unknown, start symbols make it a separate matrix and trainable)\n",
    "    'input_type': \"embed\",  # embed&prior, embed, prior\n",
    "    'dropout': 0.0  # in range(0,1)\n",
    "}\n",
    "\n",
    "ops['input_noise_level'] = 0.1\n",
    "\n",
    "ops, SEQ_LEN, N_INPUT, N_CLASSES, N_TRAIN, N_TEST = pick_task(ops['problem_type'],\n",
    "                                                              ops)  # task (parity, majority, reber, kazakov)\n",
    "ops['seq_len'] = SEQ_LEN\n",
    "ops['n_classes'] = N_CLASSES\n",
    "ops['in'] = N_INPUT\n",
    "\n",
    "\n",
    "# TRAINING PARAMS:\n",
    "# True: train attractor weights on attractor net _and_ prediction\n",
    "REPORT_BEST_TRAIN_PERFORMANCE = True\n",
    "# True: save the train/test perf on the epoch for which train perf was best\n",
    "LOSS_SWITCH_FREQ = 1\n",
    "\n",
    "TRAINING_EPOCHS = 5000\n",
    "N_REPLICATIONS = 2\n",
    "BATCH_SIZE = 256\n",
    "DISPLAY_EPOCH = 100\n",
    "EARLY_STOPPING_THRESH = 0.0 # 1e-3 for POS, 0.03 for Sentiment\n",
    "EARLY_STOPPING_PATIENCE = 50  # in epochs\n",
    "EARLY_STOPPING_MINIMUM_EPOCH = 0\n",
    "\n",
    "# NOTEBOOK CODE\n",
    "\n",
    "######### MAIN CODE #############################################################\n",
    "#0.02, 0.05, 0.1, 0.2, 0.35, 0.5, \n",
    "for dataset_part in [1.0]: \n",
    "    for attractor_steps in [15, 0]:\n",
    "#     for att_reg in [0.0]:\n",
    "        # the tf seed needs to be within the context of the graph.\n",
    "        tf.reset_default_graph()\n",
    "        np.random.seed(11)\n",
    "        tf.set_random_seed(11)\n",
    "        ops['n_attractor_iterations'] = attractor_steps\n",
    "        ops['n_attractor_iterations'] = ops['n_attractor_iterations']\n",
    "\n",
    "        # PLACEHOLDERS\n",
    "        X, Y, attractor_tgt_net = init_placeholders(ops)\n",
    "        \n",
    "        # EMBEDDING\n",
    "        maps = None\n",
    "        if 'pos' in ops['problem_type'] or 'sentiment' in ops['problem_type'] or ops[\n",
    "            'problem_type'] == \"topic_classification\":\n",
    "            [_, _, _, _, _, _, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                     ops['input_noise_level'], ops['problem_type'], ops)\n",
    "            embed = init_embedding_lookup(ops, X, maps)\n",
    "\n",
    "        \n",
    "        #\n",
    "        # Graph + all the training variables\n",
    "        #\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            net_inputs = {'X': embed, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        elif ops['problem_type'] == 'sentiment_imdb' or ops['problem_type'] == 'topic_classification':\n",
    "            net_inputs = {'X': embed, 'mask': X, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        else:\n",
    "            net_inputs = {'X': X, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "\n",
    "        if ops['bidirectional']:\n",
    "            G_attractors = {'forw': [], 'back': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            # Backward:\n",
    "            G_back = GRU_attractor(ops, inputs=net_inputs, direction='backward', suffix=names[1])\n",
    "            attr_loss_op_back = G_back.attr_loss_op\n",
    "            attr_train_op_back = G_back.attr_train_op\n",
    "            h_clean_seq_flat_back = G_back.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_back = G_back.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['back'] = {'attr_loss_op': attr_loss_op_back, \"attr_train_op\": attr_train_op_back,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_back, 'h_net_seq_flat': h_net_seq_flat_back}\n",
    "            G_back_output = G_back.output\n",
    "            \n",
    "            # Merge: [seq_len, batch_size, n_hid*2]\n",
    "            # Note that we reverse the backward cell's output to align with original direction\n",
    "            # note in \"final\" only prediction, one less dimension\n",
    "            if 'final' in ops['prediction_type']:\n",
    "                merge_index = 1\n",
    "            else:\n",
    "                merge_index = 2\n",
    "            output = tf.concat([G_forw_output, tf.reverse(G_back_output, axis=[0])], axis=merge_index)\n",
    "            input_size_final_projection = 2 * ops['hid']\n",
    "        else:\n",
    "            G_attractors = {'forw': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = GRU_attractor(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            output = G_forw.output\n",
    "\n",
    "            input_size_final_projection = ops['hid']\n",
    "            \n",
    "            \n",
    "        # the same from here on:   \n",
    "        if ops['dropout'] > 0.0:\n",
    "            # note keep_prob = 1.0 - drop_probability (not sure why they implemented it this way)\n",
    "            # tensorflow implementation scales by 1/keep_prob automatically\n",
    "            output = tf.nn.dropout(output, keep_prob=1.0 - ops['dropout'])\n",
    "        else:\n",
    "            output = output\n",
    "\n",
    "        Y_ =  project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "\n",
    "        # LOSS, ACC, & TRAIN OPS\n",
    "        pred_loss_op = task_loss(Y, Y_, ops)\n",
    "        optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "        prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "\n",
    "        if ops['train_attr_weights_on_pred']:\n",
    "            prediction_parameters += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"ATTRACTOR_WEIGHTS\")\n",
    "\n",
    "        pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "        accuracy = task_accuracy(Y, Y_, ops)\n",
    "\n",
    "        # \n",
    "        mask_op = tf.cast(tf.sign(Y), dtype=tf.float32)\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # SESSION + TRAINING\n",
    "        #\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "            # TODO: make a class for all \"best\" quantities (a lot of space)\n",
    "            saved_train_acc = []\n",
    "            saved_test_acc = []\n",
    "            saved_epoch = []\n",
    "            saved_att_loss = []\n",
    "            saved_entropy_final = []\n",
    "            saved_val_acc = []\n",
    "            saved_val_loss = []\n",
    "            saved_traini_loss = []\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # Start training\n",
    "            for replication in range(N_REPLICATIONS):\n",
    "                print(\"********** replication \", replication, \" **********\")\n",
    "                early_stopper = EarlyStopper(EARLY_STOPPING_PATIENCE, DISPLAY_EPOCH)\n",
    "                if ops['problem_type'] in ['parity, parity_length, majority, reber, kazakov']:\n",
    "                    [X_train, Y_train, X_test, Y_test, X_val, Y_val, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                                                               ops['input_noise_level'], ops['problem_type'], ops)\n",
    "\n",
    "                    # X_val, Y_val are just None\n",
    "                else:\n",
    "                    [X_full_train, Y_full_train, X_test, Y_test, X_val, Y_val, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                                                               ops['input_noise_level'], ops['problem_type'], ops)\n",
    "\n",
    "                    # Take Only part of dataset:\n",
    "                    all_ids = range(len(X_full_train))\n",
    "                    np.random.shuffle(all_ids)\n",
    "                    train_part = int(dataset_part * len(X_full_train))\n",
    "                    ids_to_take = all_ids[0:train_part]\n",
    "                    ids_for_val = all_ids[train_part:int(train_part + 0.2*train_part)]\n",
    "                    if len(ids_to_take) > X_full_train.shape[0]:\n",
    "                        ids_to_take = range(X_full_train.shape[0])\n",
    "                    X_train = X_full_train[ids_to_take, :]\n",
    "                    Y_train = Y_full_train[ids_to_take, :]\n",
    "\n",
    "                    if BATCH_SIZE < len(X_train):\n",
    "                        ops['attractor_regularization_lambda'] = ops['attractor_regularization_lambda']/(len(X_train)*1.0/BATCH_SIZE)\n",
    "                        print(ops['attractor_regularization_lambda'])\n",
    "\n",
    "                    X_val, Y_val = X_full_train[ids_for_val,:], Y_full_train[ids_for_val,:]\n",
    "\n",
    "                    N_TRAIN = len(X_train)\n",
    "                print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)\n",
    "\n",
    "                # Log Path init-n:\n",
    "                COMMENT = 'dataset_starvation_experiment'\n",
    "                MODEL_NAME_FILE = '{}_(att_iter{}__bidir{}__drop{})_{}.txt'.format(ops['problem_type'],\n",
    "                                                                                   ops['n_attractor_iterations'],\n",
    "                                                                                   ops['bidirectional'],\n",
    "                                                                                   ops['dropout'],\n",
    "                                                                                   COMMENT)\n",
    "                LOG_DIRECTORY = 'experiments/logs/{}'.format(MODEL_NAME_FILE)\n",
    "                MODEL_DIRECTORY = 'experiments/logs/{}_{}'.format(datetime.date.today(), MODEL_NAME_FILE)\n",
    "                print_into_log(LOG_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN))\n",
    "                print_into_log(MODEL_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN), supress=True)\n",
    "\n",
    "                sess.run(init)  # Run the initializer\n",
    "\n",
    "                train_prediction_loss = True\n",
    "                best_train_acc = -1000.\n",
    "                best_test_acc = 0\n",
    "                best_entropy = 0.0\n",
    "                best_att_loss = 0\n",
    "                best_train_loss = 0\n",
    "                best_val_loss = 0.0\n",
    "                best_val_acc = 0.0\n",
    "                best_epoch = 0\n",
    "                ploss_val, val_acc = 0, 0\n",
    "                for epoch in range(1, TRAINING_EPOCHS + 2):\n",
    "                    if (epoch - 1) % DISPLAY_EPOCH == 0:\n",
    "                        # TRAIN set:\n",
    "                        ploss, train_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        # TEST set:\n",
    "                        test_acc = batch_tensor_collect(sess, [accuracy], X, Y, X_test, Y_test, BATCH_SIZE)[0]\n",
    "    \n",
    "                        if X_val != None:\n",
    "                            # Validation set & Early stopping:\n",
    "                            ploss_val, val_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                      X, Y, X_val, Y_val, BATCH_SIZE)\n",
    "            \n",
    "                        # Precistion/Recall:\n",
    "                        if ops['problem_type'] == 'ner_german':\n",
    "                            y_pred, y_true, mask_val = batch_tensor_collect(sess, [Y_, Y, mask_op],\n",
    "                                                                X, Y, X_test, Y_test, BATCH_SIZE)\n",
    "                            y_pred = np.argmax(y_pred, axis=2)\n",
    "                            \n",
    "                            Y_pred_flat = np.extract(mask_val.astype(bool), y_pred)\n",
    "                            Y_test_flat = np.extract(mask_val.astype(bool), y_true)\n",
    "                            print(\"PRECISION:\",compute_f1(Y_pred_flat, Y_test_flat, maps['id2tag']))\n",
    "                            \n",
    "                        print(early_stopper.patience, early_stopper.best, ploss_val)\n",
    "                        early_stopper.update(ploss_val)\n",
    "                        if early_stopper.patience_ran_out():\n",
    "                            print_into_log(LOG_DIRECTORY, \"STOPPED EARLY AT {}\".format(epoch))\n",
    "                            break\n",
    "\n",
    "                        # ATTRACTOR(s) LOSS\n",
    "                        aloss = {}\n",
    "                        entropy = 0\n",
    "                        hid_vals_arr = batch_tensor_collect(sess, [A['h_net_seq_flat'] for att_name, A in\n",
    "                                                                   G_attractors.items()],\n",
    "                                                            X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        h_clean_val_arr = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            A = G_attractors[attractor_name]\n",
    "                            a_loss_val = []\n",
    "                            n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "                            for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "                                a_loss_val.append(\n",
    "                                    sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "                            aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "\n",
    "#                             entropy[attractor_name] = \"{:.4f}\".format(\n",
    "#                                 compute_entropy_fullvec(h_clean_val_arr[i], ops, n_bins=8))\n",
    "\n",
    "                        # Print training information:\n",
    "                        print_into_log(LOG_DIRECTORY, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + get_training_progress_comment(epoch, ploss, aloss, ploss_val, val_acc, train_acc,\n",
    "                                                                     test_acc, entropy))\n",
    "                        # Update the logs:\n",
    "                       \n",
    "                        #                 if ops['record_mutual_information']:\n",
    "                        # #                     h_attractor_val, h_clean_val = sess.run([h_attractor_collection, h_clean_seq_flat],\n",
    "                        # #                                                                    feed_dict={X: X_train, Y: Y_train})\n",
    "                        #                     # TODO: h_attractor_collection reshapeing masking.\n",
    "                        #                     h_attractor_val = None\n",
    "                        #                     h_clean_val = batch_tensor_collect(sess, [h_clean_seq_flat],\n",
    "                        #                                                                         X, Y, X_train, Y_train, BATCH_SIZE)[0]\n",
    "                        #                     MIS.update(ploss, aloss, train_acc, test_acc, np.tanh(hid_vals), h_attractor_val, h_clean_val)\n",
    "\n",
    "                        if (val_acc > best_val_acc):\n",
    "                            best_train_acc = train_acc\n",
    "                            best_test_acc = test_acc\n",
    "                            best_att_loss = aloss\n",
    "                            best_epoch = epoch\n",
    "                            best_val_acc = val_acc\n",
    "\n",
    "                            best_val_loss = ploss_val\n",
    "                            best_train_loss = ploss\n",
    "                            if ops['save_best_model']:\n",
    "                                save_path = saver.save(sess, MODEL_DIRECTORY)\n",
    "                            best_entropy = entropy\n",
    "                        if (1.0 - 1e-15 < 0.0):\n",
    "                            print('reached_peak')\n",
    "                            break\n",
    "\n",
    "                    if epoch > 1 and LOSS_SWITCH_FREQ > 0 \\\n",
    "                            and (epoch - 1) % LOSS_SWITCH_FREQ == 0:\n",
    "                        train_prediction_loss = not train_prediction_loss\n",
    "\n",
    "                    # MODEL TRAINING\n",
    "                    batches = get_batches(BATCH_SIZE, X_train, Y_train)\n",
    "                    for (batch_x, batch_y) in batches:\n",
    "                        if (LOSS_SWITCH_FREQ == 0 or train_prediction_loss):\n",
    "                            # Optimize all parameters except for attractor weights\n",
    "                            _ = sess.run([pred_train_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y})\n",
    "                            \n",
    "                        \n",
    "                        # Attractor:\n",
    "                        if (ops['n_attractor_iterations'] > 0):\n",
    "                            # training procedure\n",
    "                            batch_hid_vals = sess.run([A['h_net_seq_flat'] for att_name, A in G_attractors.items()],\n",
    "                                                      feed_dict={X:batch_x,  Y:batch_y})\n",
    "\n",
    "                            for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                A = G_attractors[attractor_name]\n",
    "                                _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: batch_hid_vals[i]})\n",
    "\n",
    "                            \n",
    "                            # TODO: redo how you did it before in the other version\n",
    "                            # Don't stop until the att_loss is below 1\n",
    "                            if ops['force_att_loss_below_1']:\n",
    "                                while float(aloss.values()[0]) > 1.0:\n",
    "                                    for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                        A = G_attractors[attractor_name]\n",
    "                                        a_loss_val = []\n",
    "                                        n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "                                        for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "                                            a_loss_val.append(\n",
    "                                                sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "                                        aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "                                    print(aloss.values()[0])\n",
    "                                    # training procedure\n",
    "                                    batch_hid_vals = sess.run([A['h_net_seq_flat'] for att_name, A in G_attractors.items()],\n",
    "                                                              feed_dict={X:batch_x,  Y:batch_y})\n",
    "\n",
    "                                    for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                        A = G_attractors[attractor_name]\n",
    "                                        _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: batch_hid_vals[i]})\n",
    "                print(\"Optimization Finished!\")\n",
    "\n",
    "                if (REPORT_BEST_TRAIN_PERFORMANCE):\n",
    "                    saved_train_acc.append(best_train_acc)\n",
    "                    saved_test_acc.append(best_test_acc)\n",
    "                    saved_att_loss.append(best_att_loss)\n",
    "                    saved_entropy_final.append(best_entropy)\n",
    "                    saved_epoch.append(best_epoch)\n",
    "\n",
    "                    saved_val_acc.append(best_val_acc)\n",
    "                    saved_val_loss.append(best_val_loss)\n",
    "                    saved_traini_loss.append(best_train_loss)\n",
    "                else:\n",
    "                    saved_train_acc.append(train_acc)\n",
    "                    saved_test_acc.append(test_acc)\n",
    "                    #             saved_att_loss.append(aloss)\n",
    "\n",
    "            save_results(ops, saved_epoch, saved_train_acc, saved_test_acc, saved_att_loss, saved_entropy_final, saved_val_acc,\n",
    "                 saved_val_loss, saved_traini_loss, N_TRAIN, N_TEST, SEQ_LEN, comment=COMMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'TASK_WEIGHTS/symb_embedding:0' shape=(2, 100) dtype=float32_ref>, <tf.Variable 'TASK_WEIGHTS/forw/W_in_stack:0' shape=(100, 150) dtype=float32_ref>, <tf.Variable 'TASK_WEIGHTS/forw/W_rec_stack:0' shape=(50, 150) dtype=float32_ref>, <tf.Variable 'TASK_WEIGHTS/forw/b_stack:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_W_in:0' shape=(50, 100) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_Wout:0' shape=(100, 50) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_b_out:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_W:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_b:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'ATTRACTOR_WEIGHTS/forw/attractor_scale:0' shape=(1,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_params['total_examples']\n",
    "dataset_params['n_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = {'a':3}\n",
    "print(a.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
