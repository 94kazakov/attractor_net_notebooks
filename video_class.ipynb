{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "L2 reg-n\n",
      "********** replication  0  **********\n",
      "video_classification\n",
      "0.0\n",
      "(499, 40, 2048) (499, 1) (99, 40, 2048) (99, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: video_classification\n",
      "    hid: \t\t\t100,\n",
      "    h_hid: \t\t\t200\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.4\n",
      "    TRAIN/TEST_SIZE: \t499/0, SEQ_LEN: 40\n",
      "Logged Successfully: \n",
      "3.2328699\n",
      "0 10000000000.0 3.2328699\n",
      "Logged Successfully: \n",
      "2018-05-22 09:58:22epoch=0; Loss Pred=3.2220; Val Loss=3.2329; Val Acc=0.0179; Loss Att={'forw': '1.0866'}; Train Acc=0.033; Test Acc=0.0491; Entropy={'forw': '9.4661'}; Entropy_Test=\n",
      "\n",
      "3.3331544\n",
      "0 3.2328699 3.3331544\n",
      "Logged Successfully: \n",
      "2018-05-22 09:58:30epoch=1; Loss Pred=3.1693; Val Loss=3.3332; Val Acc=0.0536; Loss Att={'forw': '10.1620'}; Train Acc=0.102; Test Acc=0.0949; Entropy={'forw': '6.9565'}; Entropy_Test=\n",
      "\n",
      "3.3374372\n",
      "1 3.2328699 3.3374372\n",
      "Logged Successfully: \n",
      "2018-05-22 09:58:37epoch=2; Loss Pred=3.1759; Val Loss=3.3374; Val Acc=0.0625; Loss Att={'forw': '3.7641'}; Train Acc=0.107; Test Acc=0.0893; Entropy={'forw': '6.6135'}; Entropy_Test=\n",
      "\n",
      "3.2438247\n",
      "2 3.2328699 3.2438247\n",
      "Logged Successfully: \n",
      "2018-05-22 09:58:44epoch=3; Loss Pred=3.0270; Val Loss=3.2438; Val Acc=0.1190; Loss Att={'forw': '6.9964'}; Train Acc=0.175; Test Acc=0.1083; Entropy={'forw': '7.8891'}; Entropy_Test=\n",
      "\n",
      "3.2941825\n",
      "3 3.2328699 3.2941825\n",
      "Logged Successfully: \n",
      "2018-05-22 09:58:49epoch=4; Loss Pred=3.0200; Val Loss=3.2942; Val Acc=0.0982; Loss Att={'forw': '3.8885'}; Train Acc=0.150; Test Acc=0.1060; Entropy={'forw': '7.8459'}; Entropy_Test=\n",
      "\n",
      "3.0557694\n",
      "4 3.2328699 3.0557694\n",
      "Logged Successfully: \n",
      "2018-05-22 09:58:55epoch=5; Loss Pred=2.8312; Val Loss=3.0558; Val Acc=0.1280; Loss Att={'forw': '12.9685'}; Train Acc=0.184; Test Acc=0.1542; Entropy={'forw': '8.0796'}; Entropy_Test=\n",
      "\n",
      "2.8616748\n",
      "0 3.0557694 2.8616748\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:00epoch=6; Loss Pred=2.8865; Val Loss=2.8617; Val Acc=0.2113; Loss Att={'forw': '4.8993'}; Train Acc=0.163; Test Acc=0.1239; Entropy={'forw': '8.2227'}; Entropy_Test=\n",
      "\n",
      "3.248658\n",
      "0 2.8616748 3.248658\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:07epoch=7; Loss Pred=2.9244; Val Loss=3.2487; Val Acc=0.0625; Loss Att={'forw': '21.5156'}; Train Acc=0.184; Test Acc=0.1507; Entropy={'forw': '8.1946'}; Entropy_Test=\n",
      "\n",
      "3.3506873\n",
      "1 2.8616748 3.3506873\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:12epoch=8; Loss Pred=2.9596; Val Loss=3.3507; Val Acc=0.1071; Loss Att={'forw': '2.6286'}; Train Acc=0.168; Test Acc=0.1496; Entropy={'forw': '7.7870'}; Entropy_Test=\n",
      "\n",
      "3.263446\n",
      "2 2.8616748 3.263446\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:18epoch=9; Loss Pred=2.9285; Val Loss=3.2634; Val Acc=0.1458; Loss Att={'forw': '12.7292'}; Train Acc=0.176; Test Acc=0.1768; Entropy={'forw': '5.8239'}; Entropy_Test=\n",
      "\n",
      "3.0773816\n",
      "3 2.8616748 3.0773816\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:23epoch=10; Loss Pred=2.9342; Val Loss=3.0774; Val Acc=0.1726; Loss Att={'forw': '3.6149'}; Train Acc=0.210; Test Acc=0.1667; Entropy={'forw': '6.0980'}; Entropy_Test=\n",
      "\n",
      "3.145376\n",
      "4 2.8616748 3.145376\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:30epoch=11; Loss Pred=2.7699; Val Loss=3.1454; Val Acc=0.0804; Loss Att={'forw': '8.4214'}; Train Acc=0.176; Test Acc=0.1567; Entropy={'forw': '4.5535'}; Entropy_Test=\n",
      "\n",
      "3.132267\n",
      "5 2.8616748 3.132267\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:35epoch=12; Loss Pred=2.7104; Val Loss=3.1323; Val Acc=0.0804; Loss Att={'forw': '3.8492'}; Train Acc=0.166; Test Acc=0.1699; Entropy={'forw': '4.7326'}; Entropy_Test=\n",
      "\n",
      "2.8578804\n",
      "6 2.8616748 2.8578804\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:41epoch=13; Loss Pred=2.8307; Val Loss=2.8579; Val Acc=0.1726; Loss Att={'forw': '17.7838'}; Train Acc=0.154; Test Acc=0.1612; Entropy={'forw': '5.6567'}; Entropy_Test=\n",
      "\n",
      "3.0958111\n",
      "0 2.8578804 3.0958111\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:46epoch=14; Loss Pred=2.8290; Val Loss=3.0958; Val Acc=0.1726; Loss Att={'forw': '11.7215'}; Train Acc=0.182; Test Acc=0.1496; Entropy={'forw': '5.8164'}; Entropy_Test=\n",
      "\n",
      "2.783377\n",
      "1 2.8578804 2.783377\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:52epoch=15; Loss Pred=2.7552; Val Loss=2.7834; Val Acc=0.1905; Loss Att={'forw': '34.6572'}; Train Acc=0.217; Test Acc=0.1473; Entropy={'forw': '5.4734'}; Entropy_Test=\n",
      "\n",
      "2.8921394\n",
      "0 2.783377 2.8921394\n",
      "Logged Successfully: \n",
      "2018-05-22 09:59:57epoch=16; Loss Pred=2.7834; Val Loss=2.8921; Val Acc=0.0893; Loss Att={'forw': '6.0389'}; Train Acc=0.191; Test Acc=0.1688; Entropy={'forw': '5.3366'}; Entropy_Test=\n",
      "\n",
      "3.1162887\n",
      "1 2.783377 3.1162887\n",
      "Logged Successfully: \n",
      "2018-05-22 10:00:04epoch=17; Loss Pred=2.7458; Val Loss=3.1163; Val Acc=0.1071; Loss Att={'forw': '18.3679'}; Train Acc=0.190; Test Acc=0.1529; Entropy={'forw': '5.7621'}; Entropy_Test=\n",
      "\n",
      "2.7194934\n",
      "2 2.783377 2.7194934\n",
      "Logged Successfully: \n",
      "2018-05-22 10:00:09epoch=18; Loss Pred=2.7674; Val Loss=2.7195; Val Acc=0.1815; Loss Att={'forw': '5.3988'}; Train Acc=0.176; Test Acc=0.1507; Entropy={'forw': '5.9053'}; Entropy_Test=\n",
      "\n",
      "2.8433018\n",
      "0 2.7194934 2.8433018\n",
      "Logged Successfully: \n",
      "2018-05-22 10:00:15epoch=19; Loss Pred=2.7693; Val Loss=2.8433; Val Acc=0.1429; Loss Att={'forw': '9.9748'}; Train Acc=0.188; Test Acc=0.1451; Entropy={'forw': '6.4961'}; Entropy_Test=\n",
      "\n",
      "3.0650127\n",
      "1 2.7194934 3.0650127\n",
      "Logged Successfully: \n",
      "2018-05-22 10:00:20epoch=20; Loss Pred=2.7885; Val Loss=3.0650; Val Acc=0.1161; Loss Att={'forw': '3.9621'}; Train Acc=0.160; Test Acc=0.1507; Entropy={'forw': '6.5532'}; Entropy_Test=\n",
      "\n",
      "3.215086\n",
      "2 2.7194934 3.215086\n",
      "Logged Successfully: \n",
      "2018-05-22 10:00:27epoch=21; Loss Pred=2.8732; Val Loss=3.2151; Val Acc=0.0625; Loss Att={'forw': '19.9362'}; Train Acc=0.150; Test Acc=0.1286; Entropy={'forw': '5.9953'}; Entropy_Test=\n",
      "\n",
      "3.1387725\n",
      "3 2.7194934 3.1387725\n",
      "Logged Successfully: \n",
      "2018-05-22 10:00:31epoch=22; Loss Pred=2.7663; Val Loss=3.1388; Val Acc=0.0714; Loss Att={'forw': '6.3779'}; Train Acc=0.174; Test Acc=0.1373; Entropy={'forw': '5.9376'}; Entropy_Test=\n",
      "\n",
      "3.1035762\n",
      "4 2.7194934 3.1035762\n",
      "Logged Successfully: \n",
      "2018-05-22 10:00:38epoch=23; Loss Pred=2.6787; Val Loss=3.1036; Val Acc=0.1637; Loss Att={'forw': '17.0884'}; Train Acc=0.195; Test Acc=0.1641; Entropy={'forw': '5.5308'}; Entropy_Test=\n",
      "\n",
      "3.0879939\n",
      "5 2.7194934 3.0879939\n",
      "Logged Successfully: \n",
      "2018-05-22 10:00:43epoch=24; Loss Pred=2.6353; Val Loss=3.0880; Val Acc=0.1369; Loss Att={'forw': '4.4120'}; Train Acc=0.202; Test Acc=0.1754; Entropy={'forw': '5.6120'}; Entropy_Test=\n",
      "\n",
      "2.8500144\n",
      "6 2.7194934 2.8500144\n",
      "Logged Successfully: \n",
      "2018-05-22 10:00:49epoch=25; Loss Pred=2.5154; Val Loss=2.8500; Val Acc=0.1429; Loss Att={'forw': '14.5703'}; Train Acc=0.213; Test Acc=0.1674; Entropy={'forw': '4.4587'}; Entropy_Test=\n",
      "\n",
      "2.977895\n",
      "7 2.7194934 2.977895\n",
      "Logged Successfully: \n",
      "2018-05-22 10:00:54epoch=26; Loss Pred=2.5244; Val Loss=2.9779; Val Acc=0.1607; Loss Att={'forw': '3.7597'}; Train Acc=0.212; Test Acc=0.1857; Entropy={'forw': '4.4911'}; Entropy_Test=\n",
      "\n",
      "2.8003628\n",
      "8 2.7194934 2.8003628\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:01epoch=27; Loss Pred=2.6685; Val Loss=2.8004; Val Acc=0.2202; Loss Att={'forw': '9.1069'}; Train Acc=0.212; Test Acc=0.1629; Entropy={'forw': '3.6897'}; Entropy_Test=\n",
      "\n",
      "2.7593417\n",
      "9 2.7194934 2.7593417\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:06epoch=28; Loss Pred=2.6331; Val Loss=2.7593; Val Acc=0.1905; Loss Att={'forw': '5.5173'}; Train Acc=0.168; Test Acc=0.1743; Entropy={'forw': '3.7243'}; Entropy_Test=\n",
      "\n",
      "3.0039344\n",
      "10 2.7194934 3.0039344\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:12epoch=29; Loss Pred=2.6837; Val Loss=3.0039; Val Acc=0.2292; Loss Att={'forw': '16.5810'}; Train Acc=0.195; Test Acc=0.1743; Entropy={'forw': '6.1684'}; Entropy_Test=\n",
      "\n",
      "3.1418264\n",
      "11 2.7194934 3.1418264\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:18epoch=30; Loss Pred=2.6362; Val Loss=3.1418; Val Acc=0.0625; Loss Att={'forw': '9.1494'}; Train Acc=0.203; Test Acc=0.1676; Entropy={'forw': '6.2858'}; Entropy_Test=\n",
      "\n",
      "2.8486826\n",
      "12 2.7194934 2.8486826\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:24epoch=31; Loss Pred=2.5618; Val Loss=2.8487; Val Acc=0.1458; Loss Att={'forw': '6.7646'}; Train Acc=0.192; Test Acc=0.1551; Entropy={'forw': '5.0234'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8486102\n",
      "13 2.7194934 2.8486102\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:29epoch=32; Loss Pred=2.5516; Val Loss=2.8486; Val Acc=0.0804; Loss Att={'forw': '3.7346'}; Train Acc=0.197; Test Acc=0.1980; Entropy={'forw': '5.0198'}; Entropy_Test=\n",
      "\n",
      "2.9485803\n",
      "14 2.7194934 2.9485803\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:35epoch=33; Loss Pred=2.6357; Val Loss=2.9486; Val Acc=0.1548; Loss Att={'forw': '8.6387'}; Train Acc=0.200; Test Acc=0.1578; Entropy={'forw': '5.4875'}; Entropy_Test=\n",
      "\n",
      "3.0110316\n",
      "15 2.7194934 3.0110316\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:40epoch=34; Loss Pred=2.6627; Val Loss=3.0110; Val Acc=0.2083; Loss Att={'forw': '4.5085'}; Train Acc=0.215; Test Acc=0.1933; Entropy={'forw': '5.5474'}; Entropy_Test=\n",
      "\n",
      "3.3046725\n",
      "16 2.7194934 3.3046725\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:47epoch=35; Loss Pred=2.7478; Val Loss=3.3047; Val Acc=0.1071; Loss Att={'forw': '18.9311'}; Train Acc=0.165; Test Acc=0.1429; Entropy={'forw': '5.9592'}; Entropy_Test=\n",
      "\n",
      "3.0689638\n",
      "17 2.7194934 3.0689638\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:52epoch=36; Loss Pred=2.5587; Val Loss=3.0690; Val Acc=0.1429; Loss Att={'forw': '7.5082'}; Train Acc=0.195; Test Acc=0.1799; Entropy={'forw': '5.8409'}; Entropy_Test=\n",
      "\n",
      "2.4863794\n",
      "18 2.7194934 2.4863794\n",
      "Logged Successfully: \n",
      "2018-05-22 10:01:58epoch=37; Loss Pred=2.4929; Val Loss=2.4864; Val Acc=0.1786; Loss Att={'forw': '17.0243'}; Train Acc=0.233; Test Acc=0.1833; Entropy={'forw': '4.3729'}; Entropy_Test=\n",
      "\n",
      "2.596346\n",
      "0 2.4863794 2.596346\n",
      "Logged Successfully: \n",
      "2018-05-22 10:02:03epoch=38; Loss Pred=2.4468; Val Loss=2.5963; Val Acc=0.1071; Loss Att={'forw': '6.5134'}; Train Acc=0.231; Test Acc=0.1641; Entropy={'forw': '4.3349'}; Entropy_Test=\n",
      "\n",
      "2.649987\n",
      "1 2.4863794 2.649987\n",
      "Logged Successfully: \n",
      "2018-05-22 10:02:09epoch=39; Loss Pred=2.4721; Val Loss=2.6500; Val Acc=0.1429; Loss Att={'forw': '10.3353'}; Train Acc=0.195; Test Acc=0.2069; Entropy={'forw': '4.0906'}; Entropy_Test=\n",
      "\n",
      "2.933136\n",
      "2 2.4863794 2.933136\n",
      "Logged Successfully: \n",
      "2018-05-22 10:02:14epoch=40; Loss Pred=2.3573; Val Loss=2.9331; Val Acc=0.1339; Loss Att={'forw': '5.1321'}; Train Acc=0.241; Test Acc=0.1585; Entropy={'forw': '4.1211'}; Entropy_Test=\n",
      "\n",
      "2.9203515\n",
      "3 2.4863794 2.9203515\n",
      "Logged Successfully: \n",
      "2018-05-22 10:02:21epoch=41; Loss Pred=2.5661; Val Loss=2.9204; Val Acc=0.1548; Loss Att={'forw': '9.5538'}; Train Acc=0.221; Test Acc=0.1824; Entropy={'forw': '4.7567'}; Entropy_Test=\n",
      "\n",
      "2.7513046\n",
      "4 2.4863794 2.7513046\n",
      "Logged Successfully: \n",
      "2018-05-22 10:02:26epoch=42; Loss Pred=2.5916; Val Loss=2.7513; Val Acc=0.1250; Loss Att={'forw': '4.5981'}; Train Acc=0.213; Test Acc=0.1652; Entropy={'forw': '4.7109'}; Entropy_Test=\n",
      "\n",
      "2.6779044\n",
      "5 2.4863794 2.6779044\n",
      "Logged Successfully: \n",
      "2018-05-22 10:02:32epoch=43; Loss Pred=2.3222; Val Loss=2.6779; Val Acc=0.1964; Loss Att={'forw': '6.8106'}; Train Acc=0.231; Test Acc=0.2092; Entropy={'forw': '4.9791'}; Entropy_Test=\n",
      "\n",
      "2.3449132\n",
      "6 2.4863794 2.3449132\n",
      "Logged Successfully: \n",
      "2018-05-22 10:02:37epoch=44; Loss Pred=2.3382; Val Loss=2.3449; Val Acc=0.2083; Loss Att={'forw': '4.2680'}; Train Acc=0.213; Test Acc=0.2054; Entropy={'forw': '4.9584'}; Entropy_Test=\n",
      "\n",
      "2.8608978\n",
      "0 2.3449132 2.8608978\n",
      "Logged Successfully: \n",
      "2018-05-22 10:02:43epoch=45; Loss Pred=2.5388; Val Loss=2.8609; Val Acc=0.1429; Loss Att={'forw': '12.5951'}; Train Acc=0.195; Test Acc=0.1596; Entropy={'forw': '5.3161'}; Entropy_Test=\n",
      "\n",
      "2.7992694\n",
      "1 2.3449132 2.7992694\n",
      "Logged Successfully: \n",
      "2018-05-22 10:02:48epoch=46; Loss Pred=2.4914; Val Loss=2.7993; Val Acc=0.1429; Loss Att={'forw': '7.9439'}; Train Acc=0.217; Test Acc=0.1942; Entropy={'forw': '5.2751'}; Entropy_Test=\n",
      "\n",
      "2.600247\n",
      "2 2.3449132 2.600247\n",
      "Logged Successfully: \n",
      "2018-05-22 10:02:55epoch=47; Loss Pred=2.3921; Val Loss=2.6002; Val Acc=0.1786; Loss Att={'forw': '9.8771'}; Train Acc=0.229; Test Acc=0.1741; Entropy={'forw': '5.0358'}; Entropy_Test=\n",
      "\n",
      "2.4938202\n",
      "3 2.3449132 2.4938202\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:00epoch=48; Loss Pred=2.3312; Val Loss=2.4938; Val Acc=0.2143; Loss Att={'forw': '5.9894'}; Train Acc=0.240; Test Acc=0.2098; Entropy={'forw': '5.0356'}; Entropy_Test=\n",
      "\n",
      "2.81246\n",
      "4 2.3449132 2.81246\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:06epoch=49; Loss Pred=2.4237; Val Loss=2.8125; Val Acc=0.1518; Loss Att={'forw': '12.2326'}; Train Acc=0.223; Test Acc=0.1944; Entropy={'forw': '5.3261'}; Entropy_Test=\n",
      "\n",
      "2.7884965\n",
      "5 2.3449132 2.7884965\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:11epoch=50; Loss Pred=2.4221; Val Loss=2.7885; Val Acc=0.1815; Loss Att={'forw': '4.3015'}; Train Acc=0.221; Test Acc=0.2067; Entropy={'forw': '5.3430'}; Entropy_Test=\n",
      "\n",
      "2.5677326\n",
      "6 2.3449132 2.5677326\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:17epoch=51; Loss Pred=2.3951; Val Loss=2.5677; Val Acc=0.2083; Loss Att={'forw': '10.7174'}; Train Acc=0.234; Test Acc=0.2158; Entropy={'forw': '4.6338'}; Entropy_Test=\n",
      "\n",
      "2.6974397\n",
      "7 2.3449132 2.6974397\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:22epoch=52; Loss Pred=2.3352; Val Loss=2.6974; Val Acc=0.2083; Loss Att={'forw': '4.7584'}; Train Acc=0.242; Test Acc=0.2228; Entropy={'forw': '4.6269'}; Entropy_Test=\n",
      "\n",
      "2.8702219\n",
      "8 2.3449132 2.8702219\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:29epoch=53; Loss Pred=2.1757; Val Loss=2.8702; Val Acc=0.1696; Loss Att={'forw': '8.5630'}; Train Acc=0.285; Test Acc=0.2194; Entropy={'forw': '4.5207'}; Entropy_Test=\n",
      "\n",
      "2.4233782\n",
      "9 2.3449132 2.4233782\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:34epoch=54; Loss Pred=2.1968; Val Loss=2.4234; Val Acc=0.1905; Loss Att={'forw': '5.2472'}; Train Acc=0.281; Test Acc=0.2156; Entropy={'forw': '4.5978'}; Entropy_Test=\n",
      "\n",
      "2.3889081\n",
      "10 2.3449132 2.3889081\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:40epoch=55; Loss Pred=2.1336; Val Loss=2.3889; Val Acc=0.2173; Loss Att={'forw': '24.5867'}; Train Acc=0.306; Test Acc=0.2310; Entropy={'forw': '4.8380'}; Entropy_Test=\n",
      "\n",
      "2.5622182\n",
      "11 2.3449132 2.5622182\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:45epoch=56; Loss Pred=2.2241; Val Loss=2.5622; Val Acc=0.2530; Loss Att={'forw': '10.2912'}; Train Acc=0.291; Test Acc=0.2315; Entropy={'forw': '5.3537'}; Entropy_Test=\n",
      "\n",
      "2.430368\n",
      "12 2.3449132 2.430368\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:52epoch=57; Loss Pred=2.1133; Val Loss=2.4304; Val Acc=0.2351; Loss Att={'forw': '7.2332'}; Train Acc=0.302; Test Acc=0.2188; Entropy={'forw': '5.0625'}; Entropy_Test=\n",
      "\n",
      "2.3399646\n",
      "13 2.3449132 2.3399646\n",
      "Logged Successfully: \n",
      "2018-05-22 10:03:57epoch=58; Loss Pred=2.0866; Val Loss=2.3400; Val Acc=0.2083; Loss Att={'forw': '4.3054'}; Train Acc=0.277; Test Acc=0.2212; Entropy={'forw': '5.0219'}; Entropy_Test=\n",
      "\n",
      "2.8585033\n",
      "0 2.3399646 2.8585033\n",
      "Logged Successfully: \n",
      "2018-05-22 10:04:03epoch=59; Loss Pred=2.4291; Val Loss=2.8585; Val Acc=0.1429; Loss Att={'forw': '13.9598'}; Train Acc=0.225; Test Acc=0.1866; Entropy={'forw': '4.8108'}; Entropy_Test=\n",
      "\n",
      "2.7110748\n",
      "1 2.3399646 2.7110748\n",
      "Logged Successfully: \n",
      "2018-05-22 10:04:08epoch=60; Loss Pred=2.3735; Val Loss=2.7111; Val Acc=0.1429; Loss Att={'forw': '6.7191'}; Train Acc=0.258; Test Acc=0.1900; Entropy={'forw': '4.7811'}; Entropy_Test=\n",
      "\n",
      "2.9180427\n",
      "2 2.3399646 2.9180427\n",
      "Logged Successfully: \n",
      "2018-05-22 10:04:15epoch=61; Loss Pred=2.4255; Val Loss=2.9180; Val Acc=0.1071; Loss Att={'forw': '11.1926'}; Train Acc=0.207; Test Acc=0.1888; Entropy={'forw': '4.9933'}; Entropy_Test=\n",
      "\n",
      "2.724878\n",
      "3 2.3399646 2.724878\n",
      "Logged Successfully: \n",
      "2018-05-22 10:04:20epoch=62; Loss Pred=2.3270; Val Loss=2.7249; Val Acc=0.1339; Loss Att={'forw': '6.2902'}; Train Acc=0.229; Test Acc=0.1786; Entropy={'forw': '4.9571'}; Entropy_Test=\n",
      "\n",
      "2.6096737\n",
      "4 2.3399646 2.6096737\n",
      "Logged Successfully: \n",
      "2018-05-22 10:04:26epoch=63; Loss Pred=2.1672; Val Loss=2.6097; Val Acc=0.1994; Loss Att={'forw': '5.7960'}; Train Acc=0.273; Test Acc=0.1933; Entropy={'forw': '6.0036'}; Entropy_Test=\n",
      "\n",
      "2.3872151\n",
      "5 2.3399646 2.3872151\n",
      "Logged Successfully: \n",
      "2018-05-22 10:04:31epoch=64; Loss Pred=2.2310; Val Loss=2.3872; Val Acc=0.3482; Loss Att={'forw': '3.7965'}; Train Acc=0.251; Test Acc=0.1766; Entropy={'forw': '6.0698'}; Entropy_Test=\n",
      "\n",
      "2.4332597\n",
      "6 2.3399646 2.4332597\n",
      "Logged Successfully: \n",
      "2018-05-22 10:04:38epoch=65; Loss Pred=2.2177; Val Loss=2.4333; Val Acc=0.2530; Loss Att={'forw': '10.1210'}; Train Acc=0.261; Test Acc=0.2058; Entropy={'forw': '4.7686'}; Entropy_Test=\n",
      "\n",
      "2.774686\n",
      "7 2.3399646 2.774686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Successfully: \n",
      "2018-05-22 10:04:42epoch=66; Loss Pred=2.2431; Val Loss=2.7747; Val Acc=0.1518; Loss Att={'forw': '4.7031'}; Train Acc=0.236; Test Acc=0.2100; Entropy={'forw': '4.8885'}; Entropy_Test=\n",
      "\n",
      "2.3943079\n",
      "8 2.3399646 2.3943079\n",
      "Logged Successfully: \n",
      "2018-05-22 10:04:49epoch=67; Loss Pred=2.2868; Val Loss=2.3943; Val Acc=0.1964; Loss Att={'forw': '17.2595'}; Train Acc=0.273; Test Acc=0.2176; Entropy={'forw': '5.8543'}; Entropy_Test=\n",
      "\n",
      "2.284056\n",
      "9 2.3399646 2.284056\n",
      "Logged Successfully: \n",
      "2018-05-22 10:04:54epoch=68; Loss Pred=2.3872; Val Loss=2.2841; Val Acc=0.2827; Loss Att={'forw': '6.2527'}; Train Acc=0.247; Test Acc=0.2232; Entropy={'forw': '5.9889'}; Entropy_Test=\n",
      "\n",
      "2.4005127\n",
      "0 2.284056 2.4005127\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:00epoch=69; Loss Pred=2.0448; Val Loss=2.4005; Val Acc=0.2351; Loss Att={'forw': '13.7690'}; Train Acc=0.295; Test Acc=0.2426; Entropy={'forw': '5.8518'}; Entropy_Test=\n",
      "\n",
      "2.4894235\n",
      "1 2.284056 2.4894235\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:05epoch=70; Loss Pred=2.0339; Val Loss=2.4894; Val Acc=0.1786; Loss Att={'forw': '5.4147'}; Train Acc=0.276; Test Acc=0.2502; Entropy={'forw': '5.9927'}; Entropy_Test=\n",
      "\n",
      "2.4113414\n",
      "2 2.284056 2.4113414\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:12epoch=71; Loss Pred=2.1509; Val Loss=2.4113; Val Acc=0.2232; Loss Att={'forw': '11.8925'}; Train Acc=0.309; Test Acc=0.2033; Entropy={'forw': '6.1205'}; Entropy_Test=\n",
      "\n",
      "2.393681\n",
      "3 2.284056 2.393681\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:17epoch=72; Loss Pred=2.0224; Val Loss=2.3937; Val Acc=0.2798; Loss Att={'forw': '5.5118'}; Train Acc=0.347; Test Acc=0.2424; Entropy={'forw': '6.0489'}; Entropy_Test=\n",
      "\n",
      "2.775699\n",
      "4 2.284056 2.775699\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:23epoch=73; Loss Pred=2.1333; Val Loss=2.7757; Val Acc=0.1964; Loss Att={'forw': '10.5933'}; Train Acc=0.305; Test Acc=0.2371; Entropy={'forw': '5.6334'}; Entropy_Test=\n",
      "\n",
      "2.499764\n",
      "5 2.284056 2.499764\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:28epoch=74; Loss Pred=2.0989; Val Loss=2.4998; Val Acc=0.2321; Loss Att={'forw': '5.2861'}; Train Acc=0.329; Test Acc=0.2504; Entropy={'forw': '5.5913'}; Entropy_Test=\n",
      "\n",
      "2.7404284\n",
      "6 2.284056 2.7404284\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:35epoch=75; Loss Pred=2.5310; Val Loss=2.7404; Val Acc=0.1905; Loss Att={'forw': '28.2805'}; Train Acc=0.225; Test Acc=0.1734; Entropy={'forw': '6.4693'}; Entropy_Test=\n",
      "\n",
      "2.7391274\n",
      "7 2.284056 2.7391274\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:39epoch=76; Loss Pred=2.4344; Val Loss=2.7391; Val Acc=0.1726; Loss Att={'forw': '11.8090'}; Train Acc=0.253; Test Acc=0.1935; Entropy={'forw': '6.5798'}; Entropy_Test=\n",
      "\n",
      "2.9543538\n",
      "8 2.284056 2.9543538\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:46epoch=77; Loss Pred=2.2030; Val Loss=2.9544; Val Acc=0.1696; Loss Att={'forw': '15.5013'}; Train Acc=0.280; Test Acc=0.2183; Entropy={'forw': '6.3716'}; Entropy_Test=\n",
      "\n",
      "2.5851228\n",
      "9 2.284056 2.5851228\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:50epoch=78; Loss Pred=2.2638; Val Loss=2.5851; Val Acc=0.2262; Loss Att={'forw': '6.8451'}; Train Acc=0.250; Test Acc=0.2047; Entropy={'forw': '6.3901'}; Entropy_Test=\n",
      "\n",
      "2.602008\n",
      "10 2.284056 2.602008\n",
      "Logged Successfully: \n",
      "2018-05-22 10:05:57epoch=79; Loss Pred=2.3647; Val Loss=2.6020; Val Acc=0.1786; Loss Att={'forw': '11.0140'}; Train Acc=0.271; Test Acc=0.2313; Entropy={'forw': '6.8591'}; Entropy_Test=\n",
      "\n",
      "2.6471138\n",
      "11 2.284056 2.6471138\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:02epoch=80; Loss Pred=2.3548; Val Loss=2.6471; Val Acc=0.1518; Loss Att={'forw': '6.6454'}; Train Acc=0.277; Test Acc=0.2301; Entropy={'forw': '6.8602'}; Entropy_Test=\n",
      "\n",
      "2.640222\n",
      "12 2.284056 2.640222\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:08epoch=81; Loss Pred=2.0896; Val Loss=2.6402; Val Acc=0.2262; Loss Att={'forw': '10.7096'}; Train Acc=0.305; Test Acc=0.2455; Entropy={'forw': '5.7580'}; Entropy_Test=\n",
      "\n",
      "2.4894638\n",
      "13 2.284056 2.4894638\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:13epoch=82; Loss Pred=2.1895; Val Loss=2.4895; Val Acc=0.2500; Loss Att={'forw': '7.5224'}; Train Acc=0.301; Test Acc=0.2489; Entropy={'forw': '5.7715'}; Entropy_Test=\n",
      "\n",
      "2.2894611\n",
      "14 2.284056 2.2894611\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:19epoch=83; Loss Pred=2.1023; Val Loss=2.2895; Val Acc=0.2619; Loss Att={'forw': '9.5185'}; Train Acc=0.316; Test Acc=0.2446; Entropy={'forw': '6.0349'}; Entropy_Test=\n",
      "\n",
      "2.1164296\n",
      "15 2.284056 2.1164296\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:24epoch=84; Loss Pred=1.9970; Val Loss=2.1164; Val Acc=0.2946; Loss Att={'forw': '5.8759'}; Train Acc=0.326; Test Acc=0.2614; Entropy={'forw': '5.9635'}; Entropy_Test=\n",
      "\n",
      "2.7607014\n",
      "0 2.1164296 2.7607014\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:31epoch=85; Loss Pred=2.2941; Val Loss=2.7607; Val Acc=0.2351; Loss Att={'forw': '12.3997'}; Train Acc=0.311; Test Acc=0.1998; Entropy={'forw': '5.4863'}; Entropy_Test=\n",
      "\n",
      "2.4633954\n",
      "1 2.1164296 2.4633954\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:36epoch=86; Loss Pred=2.2614; Val Loss=2.4634; Val Acc=0.2530; Loss Att={'forw': '7.0669'}; Train Acc=0.309; Test Acc=0.2313; Entropy={'forw': '5.4947'}; Entropy_Test=\n",
      "\n",
      "2.3921888\n",
      "2 2.1164296 2.3921888\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:42epoch=87; Loss Pred=1.9334; Val Loss=2.3922; Val Acc=0.2708; Loss Att={'forw': '8.4130'}; Train Acc=0.361; Test Acc=0.2511; Entropy={'forw': '5.4435'}; Entropy_Test=\n",
      "\n",
      "2.6426342\n",
      "3 2.1164296 2.6426342\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:47epoch=88; Loss Pred=1.9587; Val Loss=2.6426; Val Acc=0.1875; Loss Att={'forw': '6.1184'}; Train Acc=0.337; Test Acc=0.2547; Entropy={'forw': '5.4168'}; Entropy_Test=\n",
      "\n",
      "2.703585\n",
      "4 2.1164296 2.703585\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:53epoch=89; Loss Pred=2.0308; Val Loss=2.7036; Val Acc=0.1964; Loss Att={'forw': '8.6729'}; Train Acc=0.337; Test Acc=0.2502; Entropy={'forw': '6.1653'}; Entropy_Test=\n",
      "\n",
      "2.5255206\n",
      "5 2.1164296 2.5255206\n",
      "Logged Successfully: \n",
      "2018-05-22 10:06:58epoch=90; Loss Pred=2.0393; Val Loss=2.5255; Val Acc=0.2917; Loss Att={'forw': '5.7028'}; Train Acc=0.324; Test Acc=0.2467; Entropy={'forw': '6.1504'}; Entropy_Test=\n",
      "\n",
      "2.3915665\n",
      "6 2.1164296 2.3915665\n",
      "Logged Successfully: \n",
      "2018-05-22 10:07:05epoch=91; Loss Pred=2.1501; Val Loss=2.3916; Val Acc=0.2738; Loss Att={'forw': '30.7222'}; Train Acc=0.279; Test Acc=0.2210; Entropy={'forw': '6.0550'}; Entropy_Test=\n",
      "\n",
      "2.5977972\n",
      "7 2.1164296 2.5977972\n",
      "Logged Successfully: \n",
      "2018-05-22 10:07:09epoch=92; Loss Pred=2.1137; Val Loss=2.5978; Val Acc=0.1875; Loss Att={'forw': '27.3242'}; Train Acc=0.312; Test Acc=0.2393; Entropy={'forw': '6.0733'}; Entropy_Test=\n",
      "\n",
      "2.7565353\n",
      "8 2.1164296 2.7565353\n",
      "Logged Successfully: \n",
      "2018-05-22 10:07:16epoch=93; Loss Pred=2.1581; Val Loss=2.7565; Val Acc=0.1964; Loss Att={'forw': '32.7543'}; Train Acc=0.273; Test Acc=0.2500; Entropy={'forw': '5.5212'}; Entropy_Test=\n",
      "\n",
      "2.5052893\n",
      "9 2.1164296 2.5052893\n",
      "Logged Successfully: \n",
      "2018-05-22 10:07:21epoch=94; Loss Pred=2.1101; Val Loss=2.5053; Val Acc=0.2232; Loss Att={'forw': '29.5277'}; Train Acc=0.309; Test Acc=0.2634; Entropy={'forw': '5.5675'}; Entropy_Test=\n",
      "\n",
      "2.6685927\n",
      "10 2.1164296 2.6685927\n",
      "Logged Successfully: \n",
      "2018-05-22 10:07:27epoch=95; Loss Pred=2.3603; Val Loss=2.6686; Val Acc=0.2917; Loss Att={'forw': '28.9143'}; Train Acc=0.287; Test Acc=0.2379; Entropy={'forw': '6.9302'}; Entropy_Test=\n",
      "\n",
      "2.8090584\n",
      "11 2.1164296 2.8090584\n",
      "Logged Successfully: \n",
      "2018-05-22 10:07:32epoch=96; Loss Pred=2.3779; Val Loss=2.8091; Val Acc=0.2798; Loss Att={'forw': '11.9460'}; Train Acc=0.292; Test Acc=0.2221; Entropy={'forw': '6.7903'}; Entropy_Test=\n",
      "\n",
      "2.3624635\n",
      "12 2.1164296 2.3624635\n",
      "Logged Successfully: \n",
      "2018-05-22 10:07:38epoch=97; Loss Pred=1.9384; Val Loss=2.3625; Val Acc=0.2143; Loss Att={'forw': '15.0351'}; Train Acc=0.343; Test Acc=0.2806; Entropy={'forw': '6.2825'}; Entropy_Test=\n",
      "\n",
      "2.38591\n",
      "13 2.1164296 2.38591\n",
      "Logged Successfully: \n",
      "2018-05-22 10:07:43epoch=98; Loss Pred=1.9493; Val Loss=2.3859; Val Acc=0.2530; Loss Att={'forw': '7.5349'}; Train Acc=0.326; Test Acc=0.2681; Entropy={'forw': '6.1969'}; Entropy_Test=\n",
      "\n",
      "2.2979395\n",
      "14 2.1164296 2.2979395\n",
      "Logged Successfully: \n",
      "2018-05-22 10:07:49epoch=99; Loss Pred=1.9570; Val Loss=2.2979; Val Acc=0.2440; Loss Att={'forw': '8.2109'}; Train Acc=0.312; Test Acc=0.2315; Entropy={'forw': '5.1419'}; Entropy_Test=\n",
      "\n",
      "2.4519727\n",
      "15 2.1164296 2.4519727\n",
      "Logged Successfully: \n",
      "2018-05-22 10:07:54epoch=100; Loss Pred=1.9763; Val Loss=2.4520; Val Acc=0.2232; Loss Att={'forw': '5.7582'}; Train Acc=0.348; Test Acc=0.2437; Entropy={'forw': '5.0865'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3601432\n",
      "16 2.1164296 2.3601432\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:01epoch=101; Loss Pred=1.9228; Val Loss=2.3601; Val Acc=0.2500; Loss Att={'forw': '11.2589'}; Train Acc=0.319; Test Acc=0.2717; Entropy={'forw': '5.2838'}; Entropy_Test=\n",
      "\n",
      "2.487046\n",
      "17 2.1164296 2.487046\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:06epoch=102; Loss Pred=2.0565; Val Loss=2.4870; Val Acc=0.2143; Loss Att={'forw': '7.1008'}; Train Acc=0.339; Test Acc=0.2750; Entropy={'forw': '5.2413'}; Entropy_Test=\n",
      "\n",
      "2.2798092\n",
      "18 2.1164296 2.2798092\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:12epoch=103; Loss Pred=2.0612; Val Loss=2.2798; Val Acc=0.3065; Loss Att={'forw': '7.9199'}; Train Acc=0.362; Test Acc=0.2580; Entropy={'forw': '5.0286'}; Entropy_Test=\n",
      "\n",
      "2.3656616\n",
      "19 2.1164296 2.3656616\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:17epoch=104; Loss Pred=1.8290; Val Loss=2.3657; Val Acc=0.2708; Loss Att={'forw': '5.6139'}; Train Acc=0.408; Test Acc=0.2725; Entropy={'forw': '5.0191'}; Entropy_Test=\n",
      "\n",
      "2.1318185\n",
      "20 2.1164296 2.1318185\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:23epoch=105; Loss Pred=1.9673; Val Loss=2.1318; Val Acc=0.2589; Loss Att={'forw': '12.9984'}; Train Acc=0.378; Test Acc=0.2761; Entropy={'forw': '5.9820'}; Entropy_Test=\n",
      "\n",
      "2.274631\n",
      "21 2.1164296 2.274631\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:28epoch=106; Loss Pred=1.9481; Val Loss=2.2746; Val Acc=0.3244; Loss Att={'forw': '9.3770'}; Train Acc=0.367; Test Acc=0.2846; Entropy={'forw': '6.0185'}; Entropy_Test=\n",
      "\n",
      "2.4272406\n",
      "22 2.1164296 2.4272406\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:34epoch=107; Loss Pred=1.9929; Val Loss=2.4272; Val Acc=0.2321; Loss Att={'forw': '10.8615'}; Train Acc=0.317; Test Acc=0.2435; Entropy={'forw': '5.8954'}; Entropy_Test=\n",
      "\n",
      "2.3536117\n",
      "23 2.1164296 2.3536117\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:39epoch=108; Loss Pred=1.9551; Val Loss=2.3536; Val Acc=0.2798; Loss Att={'forw': '6.3774'}; Train Acc=0.344; Test Acc=0.2705; Entropy={'forw': '5.8502'}; Entropy_Test=\n",
      "\n",
      "2.6861894\n",
      "24 2.1164296 2.6861894\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:46epoch=109; Loss Pred=2.0083; Val Loss=2.6862; Val Acc=0.1786; Loss Att={'forw': '45.1540'}; Train Acc=0.368; Test Acc=0.2402; Entropy={'forw': '7.0552'}; Entropy_Test=\n",
      "\n",
      "2.9361882\n",
      "25 2.1164296 2.9361882\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:50epoch=110; Loss Pred=1.9976; Val Loss=2.9362; Val Acc=0.2232; Loss Att={'forw': '14.9302'}; Train Acc=0.322; Test Acc=0.2491; Entropy={'forw': '6.9762'}; Entropy_Test=\n",
      "\n",
      "2.2618773\n",
      "26 2.1164296 2.2618773\n",
      "Logged Successfully: \n",
      "2018-05-22 10:08:57epoch=111; Loss Pred=2.0979; Val Loss=2.2619; Val Acc=0.2054; Loss Att={'forw': '9.0456'}; Train Acc=0.303; Test Acc=0.2533; Entropy={'forw': '6.1988'}; Entropy_Test=\n",
      "\n",
      "2.4736829\n",
      "27 2.1164296 2.4736829\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:01epoch=112; Loss Pred=2.0651; Val Loss=2.4737; Val Acc=0.2351; Loss Att={'forw': '6.6217'}; Train Acc=0.300; Test Acc=0.2558; Entropy={'forw': '6.1926'}; Entropy_Test=\n",
      "\n",
      "2.5348442\n",
      "28 2.1164296 2.5348442\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:08epoch=113; Loss Pred=2.1170; Val Loss=2.5348; Val Acc=0.2589; Loss Att={'forw': '11.0756'}; Train Acc=0.311; Test Acc=0.2701; Entropy={'forw': '6.6267'}; Entropy_Test=\n",
      "\n",
      "2.6623554\n",
      "29 2.1164296 2.6623554\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:12epoch=114; Loss Pred=2.1040; Val Loss=2.6624; Val Acc=0.1964; Loss Att={'forw': '7.2391'}; Train Acc=0.328; Test Acc=0.2650; Entropy={'forw': '6.6549'}; Entropy_Test=\n",
      "\n",
      "2.2582545\n",
      "30 2.1164296 2.2582545\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:19epoch=115; Loss Pred=2.0792; Val Loss=2.2583; Val Acc=0.2530; Loss Att={'forw': '10.2491'}; Train Acc=0.327; Test Acc=0.2670; Entropy={'forw': '6.3561'}; Entropy_Test=\n",
      "\n",
      "2.31404\n",
      "31 2.1164296 2.31404\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:23epoch=116; Loss Pred=2.0915; Val Loss=2.3140; Val Acc=0.2351; Loss Att={'forw': '7.4062'}; Train Acc=0.289; Test Acc=0.2536; Entropy={'forw': '6.3376'}; Entropy_Test=\n",
      "\n",
      "2.428224\n",
      "32 2.1164296 2.428224\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:30epoch=117; Loss Pred=2.0203; Val Loss=2.4282; Val Acc=0.3244; Loss Att={'forw': '7.6599'}; Train Acc=0.325; Test Acc=0.2739; Entropy={'forw': '6.6550'}; Entropy_Test=\n",
      "\n",
      "2.454013\n",
      "33 2.1164296 2.454013\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:35epoch=118; Loss Pred=2.0192; Val Loss=2.4540; Val Acc=0.2321; Loss Att={'forw': '5.6584'}; Train Acc=0.327; Test Acc=0.2536; Entropy={'forw': '6.6448'}; Entropy_Test=\n",
      "\n",
      "2.2247655\n",
      "34 2.1164296 2.2247655\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:41epoch=119; Loss Pred=1.8977; Val Loss=2.2248; Val Acc=0.2976; Loss Att={'forw': '10.3026'}; Train Acc=0.316; Test Acc=0.2594; Entropy={'forw': '6.2117'}; Entropy_Test=\n",
      "\n",
      "2.4818454\n",
      "35 2.1164296 2.4818454\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:46epoch=120; Loss Pred=1.8972; Val Loss=2.4818; Val Acc=0.3065; Loss Att={'forw': '6.8594'}; Train Acc=0.342; Test Acc=0.2931; Entropy={'forw': '6.2115'}; Entropy_Test=\n",
      "\n",
      "2.5959966\n",
      "36 2.1164296 2.5959966\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:52epoch=121; Loss Pred=1.8152; Val Loss=2.5960; Val Acc=0.2798; Loss Att={'forw': '6.5707'}; Train Acc=0.371; Test Acc=0.2759; Entropy={'forw': '6.3535'}; Entropy_Test=\n",
      "\n",
      "2.3526013\n",
      "37 2.1164296 2.3526013\n",
      "Logged Successfully: \n",
      "2018-05-22 10:09:57epoch=122; Loss Pred=1.8528; Val Loss=2.3526; Val Acc=0.2589; Loss Att={'forw': '5.2293'}; Train Acc=0.398; Test Acc=0.2629; Entropy={'forw': '6.3460'}; Entropy_Test=\n",
      "\n",
      "2.5242493\n",
      "38 2.1164296 2.5242493\n",
      "Logged Successfully: \n",
      "2018-05-22 10:10:03epoch=123; Loss Pred=2.0312; Val Loss=2.5242; Val Acc=0.2411; Loss Att={'forw': '9.7892'}; Train Acc=0.344; Test Acc=0.2522; Entropy={'forw': '7.2278'}; Entropy_Test=\n",
      "\n",
      "2.2967622\n",
      "39 2.1164296 2.2967622\n",
      "Logged Successfully: \n",
      "2018-05-22 10:10:08epoch=124; Loss Pred=2.0619; Val Loss=2.2968; Val Acc=0.3274; Loss Att={'forw': '5.9530'}; Train Acc=0.327; Test Acc=0.2493; Entropy={'forw': '7.2252'}; Entropy_Test=\n",
      "\n",
      "2.3012924\n",
      "40 2.1164296 2.3012924\n",
      "Logged Successfully: \n",
      "2018-05-22 10:10:15epoch=125; Loss Pred=1.9026; Val Loss=2.3013; Val Acc=0.3363; Loss Att={'forw': '15.0061'}; Train Acc=0.374; Test Acc=0.2692; Entropy={'forw': '6.9869'}; Entropy_Test=\n",
      "\n",
      "2.3800812\n",
      "41 2.1164296 2.3800812\n",
      "Logged Successfully: \n",
      "2018-05-22 10:10:19epoch=126; Loss Pred=1.9695; Val Loss=2.3801; Val Acc=0.2411; Loss Att={'forw': '9.0181'}; Train Acc=0.334; Test Acc=0.2685; Entropy={'forw': '7.0477'}; Entropy_Test=\n",
      "\n",
      "2.3398764\n",
      "42 2.1164296 2.3398764\n",
      "Logged Successfully: \n",
      "2018-05-22 10:10:26epoch=127; Loss Pred=1.8579; Val Loss=2.3399; Val Acc=0.2589; Loss Att={'forw': '7.4162'}; Train Acc=0.383; Test Acc=0.3118; Entropy={'forw': '6.3088'}; Entropy_Test=\n",
      "\n",
      "2.0444846\n",
      "43 2.1164296 2.0444846\n",
      "Logged Successfully: \n",
      "2018-05-22 10:10:31epoch=128; Loss Pred=1.8297; Val Loss=2.0445; Val Acc=0.3512; Loss Att={'forw': '5.2942'}; Train Acc=0.375; Test Acc=0.3002; Entropy={'forw': '6.3400'}; Entropy_Test=\n",
      "\n",
      "2.3615913\n",
      "0 2.0444846 2.3615913\n",
      "Logged Successfully: \n",
      "2018-05-22 10:10:37epoch=129; Loss Pred=1.9726; Val Loss=2.3616; Val Acc=0.2143; Loss Att={'forw': '8.4072'}; Train Acc=0.352; Test Acc=0.3018; Entropy={'forw': '6.8789'}; Entropy_Test=\n",
      "\n",
      "2.351352\n",
      "1 2.0444846 2.351352\n",
      "Logged Successfully: \n",
      "2018-05-22 10:10:42epoch=130; Loss Pred=1.8101; Val Loss=2.3514; Val Acc=0.3899; Loss Att={'forw': '5.0761'}; Train Acc=0.382; Test Acc=0.3004; Entropy={'forw': '6.8725'}; Entropy_Test=\n",
      "\n",
      "2.8014169\n",
      "2 2.0444846 2.8014169\n",
      "Logged Successfully: \n",
      "2018-05-22 10:10:49epoch=131; Loss Pred=1.8553; Val Loss=2.8014; Val Acc=0.2708; Loss Att={'forw': '14.7131'}; Train Acc=0.389; Test Acc=0.2482; Entropy={'forw': '7.0314'}; Entropy_Test=\n",
      "\n",
      "2.5277328\n",
      "3 2.0444846 2.5277328\n",
      "Logged Successfully: \n",
      "2018-05-22 10:10:53epoch=132; Loss Pred=1.9379; Val Loss=2.5277; Val Acc=0.2232; Loss Att={'forw': '6.3640'}; Train Acc=0.362; Test Acc=0.2496; Entropy={'forw': '6.9915'}; Entropy_Test=\n",
      "\n",
      "2.4646835\n",
      "4 2.0444846 2.4646835\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:00epoch=133; Loss Pred=1.9524; Val Loss=2.4647; Val Acc=0.2798; Loss Att={'forw': '11.1740'}; Train Acc=0.367; Test Acc=0.2478; Entropy={'forw': '6.9819'}; Entropy_Test=\n",
      "\n",
      "2.6527665\n",
      "5 2.0444846 2.6527665\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:05epoch=134; Loss Pred=1.9214; Val Loss=2.6528; Val Acc=0.2262; Loss Att={'forw': '5.2280'}; Train Acc=0.342; Test Acc=0.2321; Entropy={'forw': '7.0057'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.276618\n",
      "6 2.0444846 2.276618\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:11epoch=135; Loss Pred=1.8576; Val Loss=2.2766; Val Acc=0.3363; Loss Att={'forw': '24.8637'}; Train Acc=0.388; Test Acc=0.3051; Entropy={'forw': '6.5852'}; Entropy_Test=\n",
      "\n",
      "2.5262802\n",
      "7 2.0444846 2.5262802\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:16epoch=136; Loss Pred=1.8384; Val Loss=2.5263; Val Acc=0.2321; Loss Att={'forw': '14.1179'}; Train Acc=0.378; Test Acc=0.2996; Entropy={'forw': '6.8417'}; Entropy_Test=\n",
      "\n",
      "2.530846\n",
      "8 2.0444846 2.530846\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:22epoch=137; Loss Pred=1.9961; Val Loss=2.5308; Val Acc=0.2887; Loss Att={'forw': '10.4843'}; Train Acc=0.380; Test Acc=0.2723; Entropy={'forw': '7.2362'}; Entropy_Test=\n",
      "\n",
      "2.3354514\n",
      "9 2.0444846 2.3354514\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:27epoch=138; Loss Pred=1.8724; Val Loss=2.3355; Val Acc=0.3244; Loss Att={'forw': '9.3214'}; Train Acc=0.374; Test Acc=0.2679; Entropy={'forw': '7.2260'}; Entropy_Test=\n",
      "\n",
      "2.4557178\n",
      "10 2.0444846 2.4557178\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:33epoch=139; Loss Pred=1.7633; Val Loss=2.4557; Val Acc=0.3036; Loss Att={'forw': '12.1644'}; Train Acc=0.395; Test Acc=0.2792; Entropy={'forw': '7.1249'}; Entropy_Test=\n",
      "\n",
      "2.299554\n",
      "11 2.0444846 2.299554\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:37epoch=140; Loss Pred=1.7284; Val Loss=2.2996; Val Acc=0.2530; Loss Att={'forw': '7.7611'}; Train Acc=0.413; Test Acc=0.2748; Entropy={'forw': '7.1539'}; Entropy_Test=\n",
      "\n",
      "2.295548\n",
      "12 2.0444846 2.295548\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:43epoch=141; Loss Pred=1.7108; Val Loss=2.2955; Val Acc=0.2619; Loss Att={'forw': '9.7397'}; Train Acc=0.405; Test Acc=0.2739; Entropy={'forw': '7.0073'}; Entropy_Test=\n",
      "\n",
      "2.358575\n",
      "13 2.0444846 2.358575\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:48epoch=142; Loss Pred=1.7115; Val Loss=2.3586; Val Acc=0.2173; Loss Att={'forw': '6.5774'}; Train Acc=0.389; Test Acc=0.2839; Entropy={'forw': '7.0223'}; Entropy_Test=\n",
      "\n",
      "2.091872\n",
      "14 2.0444846 2.091872\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:54epoch=143; Loss Pred=1.8053; Val Loss=2.0919; Val Acc=0.3244; Loss Att={'forw': '8.4041'}; Train Acc=0.350; Test Acc=0.3172; Entropy={'forw': '6.3877'}; Entropy_Test=\n",
      "\n",
      "2.0689683\n",
      "15 2.0444846 2.0689683\n",
      "Logged Successfully: \n",
      "2018-05-22 10:11:59epoch=144; Loss Pred=1.8253; Val Loss=2.0690; Val Acc=0.3214; Loss Att={'forw': '6.6218'}; Train Acc=0.377; Test Acc=0.3174; Entropy={'forw': '6.3912'}; Entropy_Test=\n",
      "\n",
      "2.2361784\n",
      "16 2.0444846 2.2361784\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:05epoch=145; Loss Pred=1.5923; Val Loss=2.2362; Val Acc=0.2976; Loss Att={'forw': '8.8427'}; Train Acc=0.430; Test Acc=0.2980; Entropy={'forw': '6.4355'}; Entropy_Test=\n",
      "\n",
      "1.9605196\n",
      "17 2.0444846 1.9605196\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:09epoch=146; Loss Pred=1.6689; Val Loss=1.9605; Val Acc=0.3780; Loss Att={'forw': '5.4658'}; Train Acc=0.427; Test Acc=0.3156; Entropy={'forw': '6.4044'}; Entropy_Test=\n",
      "\n",
      "2.3720076\n",
      "0 1.9605196 2.3720076\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:15epoch=147; Loss Pred=1.6744; Val Loss=2.3720; Val Acc=0.2679; Loss Att={'forw': '9.0018'}; Train Acc=0.413; Test Acc=0.3161; Entropy={'forw': '6.7856'}; Entropy_Test=\n",
      "\n",
      "2.2135952\n",
      "1 1.9605196 2.2135952\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:20epoch=148; Loss Pred=1.7209; Val Loss=2.2136; Val Acc=0.3899; Loss Att={'forw': '5.9196'}; Train Acc=0.415; Test Acc=0.3161; Entropy={'forw': '6.7824'}; Entropy_Test=\n",
      "\n",
      "2.5027566\n",
      "2 1.9605196 2.5027566\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:26epoch=149; Loss Pred=1.8682; Val Loss=2.5028; Val Acc=0.2679; Loss Att={'forw': '14.4607'}; Train Acc=0.378; Test Acc=0.2574; Entropy={'forw': '7.8295'}; Entropy_Test=\n",
      "\n",
      "2.3850083\n",
      "3 1.9605196 2.3850083\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:30epoch=150; Loss Pred=1.8103; Val Loss=2.3850; Val Acc=0.2530; Loss Att={'forw': '9.2677'}; Train Acc=0.384; Test Acc=0.2875; Entropy={'forw': '7.8255'}; Entropy_Test=\n",
      "\n",
      "2.3926651\n",
      "4 1.9605196 2.3926651\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:36epoch=151; Loss Pred=2.0379; Val Loss=2.3927; Val Acc=0.3631; Loss Att={'forw': '11.9474'}; Train Acc=0.337; Test Acc=0.2600; Entropy={'forw': '7.8794'}; Entropy_Test=\n",
      "\n",
      "2.2896438\n",
      "5 1.9605196 2.2896438\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:41epoch=152; Loss Pred=2.0487; Val Loss=2.2896; Val Acc=0.3631; Loss Att={'forw': '9.0576'}; Train Acc=0.354; Test Acc=0.2931; Entropy={'forw': '7.8881'}; Entropy_Test=\n",
      "\n",
      "2.0178025\n",
      "6 1.9605196 2.0178025\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:47epoch=153; Loss Pred=1.8643; Val Loss=2.0178; Val Acc=0.3244; Loss Att={'forw': '14.6889'}; Train Acc=0.372; Test Acc=0.2998; Entropy={'forw': '6.9565'}; Entropy_Test=\n",
      "\n",
      "2.0931487\n",
      "7 1.9605196 2.0931487\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:52epoch=154; Loss Pred=1.7749; Val Loss=2.0931; Val Acc=0.2679; Loss Att={'forw': '10.4320'}; Train Acc=0.393; Test Acc=0.2663; Entropy={'forw': '7.0139'}; Entropy_Test=\n",
      "\n",
      "2.3594563\n",
      "8 1.9605196 2.3594563\n",
      "Logged Successfully: \n",
      "2018-05-22 10:12:57epoch=155; Loss Pred=1.7085; Val Loss=2.3595; Val Acc=0.3036; Loss Att={'forw': '10.4580'}; Train Acc=0.446; Test Acc=0.2895; Entropy={'forw': '6.6885'}; Entropy_Test=\n",
      "\n",
      "2.4359372\n",
      "9 1.9605196 2.4359372\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:02epoch=156; Loss Pred=1.8075; Val Loss=2.4359; Val Acc=0.3155; Loss Att={'forw': '7.3621'}; Train Acc=0.380; Test Acc=0.2705; Entropy={'forw': '6.6727'}; Entropy_Test=\n",
      "\n",
      "2.5736938\n",
      "10 1.9605196 2.5736938\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:08epoch=157; Loss Pred=2.1103; Val Loss=2.5737; Val Acc=0.3006; Loss Att={'forw': '19.0183'}; Train Acc=0.321; Test Acc=0.2681; Entropy={'forw': '6.8169'}; Entropy_Test=\n",
      "\n",
      "2.7998738\n",
      "11 1.9605196 2.7998738\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:13epoch=158; Loss Pred=2.1393; Val Loss=2.7999; Val Acc=0.1875; Loss Att={'forw': '9.6014'}; Train Acc=0.342; Test Acc=0.2493; Entropy={'forw': '6.8634'}; Entropy_Test=\n",
      "\n",
      "2.7077258\n",
      "12 1.9605196 2.7077258\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:19epoch=159; Loss Pred=2.0287; Val Loss=2.7077; Val Acc=0.1964; Loss Att={'forw': '13.0075'}; Train Acc=0.376; Test Acc=0.2833; Entropy={'forw': '6.2665'}; Entropy_Test=\n",
      "\n",
      "2.303949\n",
      "13 1.9605196 2.303949\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:23epoch=160; Loss Pred=1.9593; Val Loss=2.3039; Val Acc=0.2708; Loss Att={'forw': '7.9068'}; Train Acc=0.386; Test Acc=0.2804; Entropy={'forw': '6.2121'}; Entropy_Test=\n",
      "\n",
      "2.3321052\n",
      "14 1.9605196 2.3321052\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:30epoch=161; Loss Pred=2.0788; Val Loss=2.3321; Val Acc=0.3274; Loss Att={'forw': '10.3698'}; Train Acc=0.328; Test Acc=0.2536; Entropy={'forw': '6.7346'}; Entropy_Test=\n",
      "\n",
      "2.3682158\n",
      "15 1.9605196 2.3682158\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:34epoch=162; Loss Pred=2.1186; Val Loss=2.3682; Val Acc=0.2500; Loss Att={'forw': '7.2046'}; Train Acc=0.346; Test Acc=0.2402; Entropy={'forw': '6.7053'}; Entropy_Test=\n",
      "\n",
      "2.8205647\n",
      "16 1.9605196 2.8205647\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:40epoch=163; Loss Pred=2.0084; Val Loss=2.8206; Val Acc=0.2530; Loss Att={'forw': '26.7942'}; Train Acc=0.348; Test Acc=0.2674; Entropy={'forw': '8.0720'}; Entropy_Test=\n",
      "\n",
      "2.6934757\n",
      "17 1.9605196 2.6934757\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:45epoch=164; Loss Pred=2.1946; Val Loss=2.6935; Val Acc=0.1964; Loss Att={'forw': '12.5666'}; Train Acc=0.317; Test Acc=0.2683; Entropy={'forw': '8.0224'}; Entropy_Test=\n",
      "\n",
      "2.6692035\n",
      "18 1.9605196 2.6692035\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:51epoch=165; Loss Pred=2.0997; Val Loss=2.6692; Val Acc=0.1786; Loss Att={'forw': '14.9528'}; Train Acc=0.312; Test Acc=0.2480; Entropy={'forw': '7.3452'}; Entropy_Test=\n",
      "\n",
      "2.72916\n",
      "19 1.9605196 2.72916\n",
      "Logged Successfully: \n",
      "2018-05-22 10:13:55epoch=166; Loss Pred=2.1990; Val Loss=2.7292; Val Acc=0.1875; Loss Att={'forw': '9.2798'}; Train Acc=0.326; Test Acc=0.2663; Entropy={'forw': '7.3074'}; Entropy_Test=\n",
      "\n",
      "3.1536481\n",
      "20 1.9605196 3.1536481\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:02epoch=167; Loss Pred=2.4921; Val Loss=3.1536; Val Acc=0.2143; Loss Att={'forw': '27.8788'}; Train Acc=0.296; Test Acc=0.2460; Entropy={'forw': '7.4907'}; Entropy_Test=\n",
      "\n",
      "3.2341094\n",
      "21 1.9605196 3.2341094\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:06epoch=168; Loss Pred=2.4949; Val Loss=3.2341; Val Acc=0.1875; Loss Att={'forw': '10.6039'}; Train Acc=0.273; Test Acc=0.2344; Entropy={'forw': '7.3349'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.842849\n",
      "22 1.9605196 2.842849\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:12epoch=169; Loss Pred=2.2936; Val Loss=2.8428; Val Acc=0.2054; Loss Att={'forw': '17.0950'}; Train Acc=0.286; Test Acc=0.2281; Entropy={'forw': '7.4877'}; Entropy_Test=\n",
      "\n",
      "2.8402874\n",
      "23 1.9605196 2.8402874\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:17epoch=170; Loss Pred=2.3337; Val Loss=2.8403; Val Acc=0.2232; Loss Att={'forw': '9.8271'}; Train Acc=0.274; Test Acc=0.2192; Entropy={'forw': '7.5391'}; Entropy_Test=\n",
      "\n",
      "2.1214914\n",
      "24 1.9605196 2.1214914\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:23epoch=171; Loss Pred=2.0280; Val Loss=2.1215; Val Acc=0.3810; Loss Att={'forw': '14.0352'}; Train Acc=0.352; Test Acc=0.2549; Entropy={'forw': '7.4075'}; Entropy_Test=\n",
      "\n",
      "2.218145\n",
      "25 1.9605196 2.218145\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:27epoch=172; Loss Pred=1.9780; Val Loss=2.2181; Val Acc=0.3452; Loss Att={'forw': '8.6287'}; Train Acc=0.356; Test Acc=0.2522; Entropy={'forw': '7.4464'}; Entropy_Test=\n",
      "\n",
      "2.224415\n",
      "26 1.9605196 2.224415\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:33epoch=173; Loss Pred=1.7513; Val Loss=2.2244; Val Acc=0.2887; Loss Att={'forw': '13.2057'}; Train Acc=0.417; Test Acc=0.2906; Entropy={'forw': '7.6734'}; Entropy_Test=\n",
      "\n",
      "2.2451215\n",
      "27 1.9605196 2.2451215\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:38epoch=174; Loss Pred=1.7560; Val Loss=2.2451; Val Acc=0.2143; Loss Att={'forw': '7.3785'}; Train Acc=0.406; Test Acc=0.2906; Entropy={'forw': '7.6975'}; Entropy_Test=\n",
      "\n",
      "2.3873935\n",
      "28 1.9605196 2.3873935\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:44epoch=175; Loss Pred=1.8255; Val Loss=2.3874; Val Acc=0.2708; Loss Att={'forw': '32.3519'}; Train Acc=0.393; Test Acc=0.2882; Entropy={'forw': '7.3214'}; Entropy_Test=\n",
      "\n",
      "2.1349242\n",
      "29 1.9605196 2.1349242\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:48epoch=176; Loss Pred=1.8266; Val Loss=2.1349; Val Acc=0.3601; Loss Att={'forw': '10.4204'}; Train Acc=0.394; Test Acc=0.2951; Entropy={'forw': '7.5772'}; Entropy_Test=\n",
      "\n",
      "2.6703982\n",
      "30 1.9605196 2.6703982\n",
      "Logged Successfully: \n",
      "2018-05-22 10:14:55epoch=177; Loss Pred=1.8805; Val Loss=2.6704; Val Acc=0.2946; Loss Att={'forw': '17.1294'}; Train Acc=0.374; Test Acc=0.3163; Entropy={'forw': '8.1535'}; Entropy_Test=\n",
      "\n",
      "2.0467026\n",
      "31 1.9605196 2.0467026\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:00epoch=178; Loss Pred=2.0232; Val Loss=2.0467; Val Acc=0.3720; Loss Att={'forw': '11.3222'}; Train Acc=0.354; Test Acc=0.3339; Entropy={'forw': '8.2112'}; Entropy_Test=\n",
      "\n",
      "2.2061305\n",
      "32 1.9605196 2.2061305\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:06epoch=179; Loss Pred=1.7264; Val Loss=2.2061; Val Acc=0.2976; Loss Att={'forw': '16.6635'}; Train Acc=0.421; Test Acc=0.3138; Entropy={'forw': '7.9603'}; Entropy_Test=\n",
      "\n",
      "2.2365813\n",
      "33 1.9605196 2.2365813\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:10epoch=180; Loss Pred=1.6362; Val Loss=2.2366; Val Acc=0.2500; Loss Att={'forw': '9.1652'}; Train Acc=0.426; Test Acc=0.3018; Entropy={'forw': '8.0809'}; Entropy_Test=\n",
      "\n",
      "2.1289454\n",
      "34 1.9605196 2.1289454\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:16epoch=181; Loss Pred=1.8847; Val Loss=2.1289; Val Acc=0.3065; Loss Att={'forw': '11.3063'}; Train Acc=0.355; Test Acc=0.3433; Entropy={'forw': '7.4912'}; Entropy_Test=\n",
      "\n",
      "2.3446705\n",
      "35 1.9605196 2.3446705\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:21epoch=182; Loss Pred=1.7907; Val Loss=2.3447; Val Acc=0.3631; Loss Att={'forw': '8.5774'}; Train Acc=0.387; Test Acc=0.3150; Entropy={'forw': '7.5078'}; Entropy_Test=\n",
      "\n",
      "2.3676677\n",
      "36 1.9605196 2.3676677\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:27epoch=183; Loss Pred=1.6787; Val Loss=2.3677; Val Acc=0.2887; Loss Att={'forw': '14.3611'}; Train Acc=0.457; Test Acc=0.2795; Entropy={'forw': '7.5991'}; Entropy_Test=\n",
      "\n",
      "2.349461\n",
      "37 1.9605196 2.349461\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:31epoch=184; Loss Pred=1.7418; Val Loss=2.3495; Val Acc=0.3333; Loss Att={'forw': '9.7631'}; Train Acc=0.434; Test Acc=0.2973; Entropy={'forw': '7.5863'}; Entropy_Test=\n",
      "\n",
      "2.2679305\n",
      "38 1.9605196 2.2679305\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:37epoch=185; Loss Pred=1.6667; Val Loss=2.2679; Val Acc=0.3155; Loss Att={'forw': '17.1594'}; Train Acc=0.456; Test Acc=0.3196; Entropy={'forw': '6.9396'}; Entropy_Test=\n",
      "\n",
      "2.3293297\n",
      "39 1.9605196 2.3293297\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:41epoch=186; Loss Pred=1.6879; Val Loss=2.3293; Val Acc=0.2440; Loss Att={'forw': '8.8899'}; Train Acc=0.430; Test Acc=0.3306; Entropy={'forw': '6.9771'}; Entropy_Test=\n",
      "\n",
      "2.1770747\n",
      "40 1.9605196 2.1770747\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:48epoch=187; Loss Pred=1.7295; Val Loss=2.1771; Val Acc=0.3036; Loss Att={'forw': '19.1713'}; Train Acc=0.458; Test Acc=0.3337; Entropy={'forw': '7.3220'}; Entropy_Test=\n",
      "\n",
      "2.0093844\n",
      "41 1.9605196 2.0093844\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:52epoch=188; Loss Pred=1.5930; Val Loss=2.0094; Val Acc=0.3899; Loss Att={'forw': '8.7246'}; Train Acc=0.480; Test Acc=0.3473; Entropy={'forw': '7.3336'}; Entropy_Test=\n",
      "\n",
      "2.2096019\n",
      "42 1.9605196 2.2096019\n",
      "Logged Successfully: \n",
      "2018-05-22 10:15:58epoch=189; Loss Pred=1.6695; Val Loss=2.2096; Val Acc=0.3452; Loss Att={'forw': '14.8564'}; Train Acc=0.400; Test Acc=0.3272; Entropy={'forw': '7.2279'}; Entropy_Test=\n",
      "\n",
      "2.204363\n",
      "43 1.9605196 2.204363\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:03epoch=190; Loss Pred=1.6602; Val Loss=2.2044; Val Acc=0.3065; Loss Att={'forw': '7.9797'}; Train Acc=0.410; Test Acc=0.3230; Entropy={'forw': '7.2224'}; Entropy_Test=\n",
      "\n",
      "2.3398912\n",
      "44 1.9605196 2.3398912\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:09epoch=191; Loss Pred=1.7969; Val Loss=2.3399; Val Acc=0.2589; Loss Att={'forw': '12.8667'}; Train Acc=0.428; Test Acc=0.2779; Entropy={'forw': '7.5202'}; Entropy_Test=\n",
      "\n",
      "2.346382\n",
      "45 1.9605196 2.346382\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:13epoch=192; Loss Pred=1.8319; Val Loss=2.3464; Val Acc=0.2887; Loss Att={'forw': '8.7457'}; Train Acc=0.380; Test Acc=0.3190; Entropy={'forw': '7.5373'}; Entropy_Test=\n",
      "\n",
      "2.4114888\n",
      "46 1.9605196 2.4114888\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:19epoch=193; Loss Pred=1.4913; Val Loss=2.4115; Val Acc=0.2679; Loss Att={'forw': '8.3646'}; Train Acc=0.476; Test Acc=0.3161; Entropy={'forw': '6.9693'}; Entropy_Test=\n",
      "\n",
      "2.2151363\n",
      "47 1.9605196 2.2151363\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:23epoch=194; Loss Pred=1.6845; Val Loss=2.2151; Val Acc=0.3214; Loss Att={'forw': '6.4388'}; Train Acc=0.404; Test Acc=0.3196; Entropy={'forw': '6.9628'}; Entropy_Test=\n",
      "\n",
      "2.069327\n",
      "48 1.9605196 2.069327\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:29epoch=195; Loss Pred=1.6603; Val Loss=2.0693; Val Acc=0.3214; Loss Att={'forw': '7.1895'}; Train Acc=0.433; Test Acc=0.3150; Entropy={'forw': '7.0835'}; Entropy_Test=\n",
      "\n",
      "1.8598106\n",
      "49 1.9605196 1.8598106\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:34epoch=196; Loss Pred=1.7145; Val Loss=1.8598; Val Acc=0.3869; Loss Att={'forw': '6.2941'}; Train Acc=0.442; Test Acc=0.3172; Entropy={'forw': '7.0917'}; Entropy_Test=\n",
      "\n",
      "2.3428745\n",
      "0 1.8598106 2.3428745\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:40epoch=197; Loss Pred=1.9419; Val Loss=2.3429; Val Acc=0.2976; Loss Att={'forw': '15.5035'}; Train Acc=0.348; Test Acc=0.2719; Entropy={'forw': '7.0153'}; Entropy_Test=\n",
      "\n",
      "2.2877197\n",
      "1 1.8598106 2.2877197\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:44epoch=198; Loss Pred=1.7362; Val Loss=2.2877; Val Acc=0.2619; Loss Att={'forw': '8.1326'}; Train Acc=0.441; Test Acc=0.2824; Entropy={'forw': '7.1261'}; Entropy_Test=\n",
      "\n",
      "1.9491893\n",
      "2 1.8598106 1.9491893\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:50epoch=199; Loss Pred=1.6527; Val Loss=1.9492; Val Acc=0.3333; Loss Att={'forw': '15.1001'}; Train Acc=0.426; Test Acc=0.3533; Entropy={'forw': '6.8823'}; Entropy_Test=\n",
      "\n",
      "2.4613242\n",
      "3 1.8598106 2.4613242\n",
      "Logged Successfully: \n",
      "2018-05-22 10:16:55epoch=200; Loss Pred=1.6790; Val Loss=2.4613; Val Acc=0.3065; Loss Att={'forw': '8.5496'}; Train Acc=0.430; Test Acc=0.3297; Entropy={'forw': '6.8589'}; Entropy_Test=\n",
      "\n",
      "2.4981406\n",
      "4 1.8598106 2.4981406\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:01epoch=201; Loss Pred=1.5727; Val Loss=2.4981; Val Acc=0.3571; Loss Att={'forw': '9.2321'}; Train Acc=0.448; Test Acc=0.3467; Entropy={'forw': '7.0566'}; Entropy_Test=\n",
      "\n",
      "2.4911191\n",
      "5 1.8598106 2.4911191\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:05epoch=202; Loss Pred=1.5964; Val Loss=2.4911; Val Acc=0.3333; Loss Att={'forw': '6.7029'}; Train Acc=0.450; Test Acc=0.3217; Entropy={'forw': '7.0636'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0198262\n",
      "6 1.8598106 2.0198262\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:11epoch=203; Loss Pred=1.4995; Val Loss=2.0198; Val Acc=0.3423; Loss Att={'forw': '10.9856'}; Train Acc=0.495; Test Acc=0.3442; Entropy={'forw': '7.3184'}; Entropy_Test=\n",
      "\n",
      "2.1937473\n",
      "7 1.8598106 2.1937473\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:16epoch=204; Loss Pred=1.4559; Val Loss=2.1937; Val Acc=0.3780; Loss Att={'forw': '6.7417'}; Train Acc=0.498; Test Acc=0.3694; Entropy={'forw': '7.3133'}; Entropy_Test=\n",
      "\n",
      "2.4561496\n",
      "8 1.8598106 2.4561496\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:22epoch=205; Loss Pred=1.7800; Val Loss=2.4561; Val Acc=0.2232; Loss Att={'forw': '19.3566'}; Train Acc=0.407; Test Acc=0.2915; Entropy={'forw': '7.6949'}; Entropy_Test=\n",
      "\n",
      "2.071942\n",
      "9 1.8598106 2.071942\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:26epoch=206; Loss Pred=1.8152; Val Loss=2.0719; Val Acc=0.4107; Loss Att={'forw': '15.3819'}; Train Acc=0.393; Test Acc=0.3125; Entropy={'forw': '7.6772'}; Entropy_Test=\n",
      "\n",
      "2.4246712\n",
      "10 1.8598106 2.4246712\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:32epoch=207; Loss Pred=1.9041; Val Loss=2.4247; Val Acc=0.2411; Loss Att={'forw': '34.9677'}; Train Acc=0.383; Test Acc=0.2703; Entropy={'forw': '6.6639'}; Entropy_Test=\n",
      "\n",
      "2.6419501\n",
      "11 1.8598106 2.6419501\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:37epoch=208; Loss Pred=1.8633; Val Loss=2.6420; Val Acc=0.2976; Loss Att={'forw': '14.8061'}; Train Acc=0.378; Test Acc=0.2717; Entropy={'forw': '6.8581'}; Entropy_Test=\n",
      "\n",
      "2.407397\n",
      "12 1.8598106 2.407397\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:43epoch=209; Loss Pred=1.7720; Val Loss=2.4074; Val Acc=0.2440; Loss Att={'forw': '22.1100'}; Train Acc=0.413; Test Acc=0.2741; Entropy={'forw': '7.1119'}; Entropy_Test=\n",
      "\n",
      "2.4209385\n",
      "13 1.8598106 2.4209385\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:47epoch=210; Loss Pred=1.9531; Val Loss=2.4209; Val Acc=0.2321; Loss Att={'forw': '18.1243'}; Train Acc=0.393; Test Acc=0.2859; Entropy={'forw': '7.0914'}; Entropy_Test=\n",
      "\n",
      "2.3363473\n",
      "14 1.8598106 2.3363473\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:53epoch=211; Loss Pred=1.7180; Val Loss=2.3363; Val Acc=0.3452; Loss Att={'forw': '12.1469'}; Train Acc=0.443; Test Acc=0.3241; Entropy={'forw': '7.5108'}; Entropy_Test=\n",
      "\n",
      "2.2564855\n",
      "15 1.8598106 2.2564855\n",
      "Logged Successfully: \n",
      "2018-05-22 10:17:58epoch=212; Loss Pred=1.7933; Val Loss=2.2565; Val Acc=0.3810; Loss Att={'forw': '10.4465'}; Train Acc=0.415; Test Acc=0.3335; Entropy={'forw': '7.4973'}; Entropy_Test=\n",
      "\n",
      "2.1503074\n",
      "16 1.8598106 2.1503074\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:04epoch=213; Loss Pred=1.5806; Val Loss=2.1503; Val Acc=0.3601; Loss Att={'forw': '15.3406'}; Train Acc=0.463; Test Acc=0.3533; Entropy={'forw': '7.5872'}; Entropy_Test=\n",
      "\n",
      "2.1195712\n",
      "17 1.8598106 2.1195712\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:08epoch=214; Loss Pred=1.6344; Val Loss=2.1196; Val Acc=0.3423; Loss Att={'forw': '12.1655'}; Train Acc=0.452; Test Acc=0.3643; Entropy={'forw': '7.5979'}; Entropy_Test=\n",
      "\n",
      "2.1527655\n",
      "18 1.8598106 2.1527655\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:14epoch=215; Loss Pred=1.5902; Val Loss=2.1528; Val Acc=0.3899; Loss Att={'forw': '13.7626'}; Train Acc=0.494; Test Acc=0.3250; Entropy={'forw': '7.7397'}; Entropy_Test=\n",
      "\n",
      "2.1316826\n",
      "19 1.8598106 2.1316826\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:18epoch=216; Loss Pred=1.6036; Val Loss=2.1317; Val Acc=0.3690; Loss Att={'forw': '9.0671'}; Train Acc=0.432; Test Acc=0.3315; Entropy={'forw': '7.7871'}; Entropy_Test=\n",
      "\n",
      "1.9701074\n",
      "20 1.8598106 1.9701074\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:24epoch=217; Loss Pred=1.5965; Val Loss=1.9701; Val Acc=0.3839; Loss Att={'forw': '11.7803'}; Train Acc=0.444; Test Acc=0.3051; Entropy={'forw': '7.3414'}; Entropy_Test=\n",
      "\n",
      "2.2339387\n",
      "21 1.8598106 2.2339387\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:29epoch=218; Loss Pred=1.5266; Val Loss=2.2339; Val Acc=0.3065; Loss Att={'forw': '7.7199'}; Train Acc=0.482; Test Acc=0.3364; Entropy={'forw': '7.3397'}; Entropy_Test=\n",
      "\n",
      "2.0731542\n",
      "22 1.8598106 2.0731542\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:35epoch=219; Loss Pred=1.7221; Val Loss=2.0732; Val Acc=0.3214; Loss Att={'forw': '8.3759'}; Train Acc=0.434; Test Acc=0.2781; Entropy={'forw': '7.0756'}; Entropy_Test=\n",
      "\n",
      "2.0996118\n",
      "23 1.8598106 2.0996118\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:39epoch=220; Loss Pred=1.7034; Val Loss=2.0996; Val Acc=0.3720; Loss Att={'forw': '7.0938'}; Train Acc=0.407; Test Acc=0.2723; Entropy={'forw': '7.0731'}; Entropy_Test=\n",
      "\n",
      "2.17808\n",
      "24 1.8598106 2.17808\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:45epoch=221; Loss Pred=1.6190; Val Loss=2.1781; Val Acc=0.3631; Loss Att={'forw': '14.8810'}; Train Acc=0.452; Test Acc=0.3554; Entropy={'forw': '7.6538'}; Entropy_Test=\n",
      "\n",
      "2.0914915\n",
      "25 1.8598106 2.0914915\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:50epoch=222; Loss Pred=1.6898; Val Loss=2.0915; Val Acc=0.4167; Loss Att={'forw': '7.3865'}; Train Acc=0.446; Test Acc=0.3576; Entropy={'forw': '7.6229'}; Entropy_Test=\n",
      "\n",
      "1.9633596\n",
      "26 1.8598106 1.9633596\n",
      "Logged Successfully: \n",
      "2018-05-22 10:18:56epoch=223; Loss Pred=1.4782; Val Loss=1.9634; Val Acc=0.3750; Loss Att={'forw': '10.2713'}; Train Acc=0.473; Test Acc=0.3333; Entropy={'forw': '7.3308'}; Entropy_Test=\n",
      "\n",
      "2.4564917\n",
      "27 1.8598106 2.4564917\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:00epoch=224; Loss Pred=1.5225; Val Loss=2.4565; Val Acc=0.3036; Loss Att={'forw': '7.1589'}; Train Acc=0.465; Test Acc=0.3228; Entropy={'forw': '7.3437'}; Entropy_Test=\n",
      "\n",
      "2.1706963\n",
      "28 1.8598106 2.1706963\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:06epoch=225; Loss Pred=1.5121; Val Loss=2.1707; Val Acc=0.3869; Loss Att={'forw': '13.5127'}; Train Acc=0.512; Test Acc=0.3411; Entropy={'forw': '7.3796'}; Entropy_Test=\n",
      "\n",
      "2.3642032\n",
      "29 1.8598106 2.3642032\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:11epoch=226; Loss Pred=1.5820; Val Loss=2.3642; Val Acc=0.3333; Loss Att={'forw': '6.7753'}; Train Acc=0.495; Test Acc=0.3429; Entropy={'forw': '7.3912'}; Entropy_Test=\n",
      "\n",
      "2.3014152\n",
      "30 1.8598106 2.3014152\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:17epoch=227; Loss Pred=1.7981; Val Loss=2.3014; Val Acc=0.3155; Loss Att={'forw': '12.3078'}; Train Acc=0.440; Test Acc=0.3074; Entropy={'forw': '7.5891'}; Entropy_Test=\n",
      "\n",
      "2.4763749\n",
      "31 1.8598106 2.4763749\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:21epoch=228; Loss Pred=1.7283; Val Loss=2.4764; Val Acc=0.3363; Loss Att={'forw': '8.6503'}; Train Acc=0.458; Test Acc=0.3246; Entropy={'forw': '7.5685'}; Entropy_Test=\n",
      "\n",
      "2.2091095\n",
      "32 1.8598106 2.2091095\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:27epoch=229; Loss Pred=1.8051; Val Loss=2.2091; Val Acc=0.2768; Loss Att={'forw': '14.8068'}; Train Acc=0.430; Test Acc=0.2906; Entropy={'forw': '7.2757'}; Entropy_Test=\n",
      "\n",
      "2.5109885\n",
      "33 1.8598106 2.5109885\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:31epoch=230; Loss Pred=1.7931; Val Loss=2.5110; Val Acc=0.2321; Loss Att={'forw': '9.7511'}; Train Acc=0.415; Test Acc=0.2824; Entropy={'forw': '7.2961'}; Entropy_Test=\n",
      "\n",
      "2.4748406\n",
      "34 1.8598106 2.4748406\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:37epoch=231; Loss Pred=1.7811; Val Loss=2.4748; Val Acc=0.2679; Loss Att={'forw': '10.5191'}; Train Acc=0.400; Test Acc=0.2790; Entropy={'forw': '7.7191'}; Entropy_Test=\n",
      "\n",
      "2.422348\n",
      "35 1.8598106 2.422348\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:42epoch=232; Loss Pred=1.8765; Val Loss=2.4223; Val Acc=0.2530; Loss Att={'forw': '9.1126'}; Train Acc=0.389; Test Acc=0.2750; Entropy={'forw': '7.7362'}; Entropy_Test=\n",
      "\n",
      "2.30104\n",
      "36 1.8598106 2.30104\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:48epoch=233; Loss Pred=1.6297; Val Loss=2.3010; Val Acc=0.3690; Loss Att={'forw': '8.9781'}; Train Acc=0.466; Test Acc=0.3585; Entropy={'forw': '7.0910'}; Entropy_Test=\n",
      "\n",
      "2.1462066\n",
      "37 1.8598106 2.1462066\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:52epoch=234; Loss Pred=1.7156; Val Loss=2.1462; Val Acc=0.3601; Loss Att={'forw': '7.4744'}; Train Acc=0.458; Test Acc=0.3433; Entropy={'forw': '7.0861'}; Entropy_Test=\n",
      "\n",
      "2.204717\n",
      "38 1.8598106 2.204717\n",
      "Logged Successfully: \n",
      "2018-05-22 10:19:58epoch=235; Loss Pred=1.5421; Val Loss=2.2047; Val Acc=0.3482; Loss Att={'forw': '9.1012'}; Train Acc=0.464; Test Acc=0.3641; Entropy={'forw': '7.0120'}; Entropy_Test=\n",
      "\n",
      "2.1073496\n",
      "39 1.8598106 2.1073496\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:03epoch=236; Loss Pred=1.5606; Val Loss=2.1073; Val Acc=0.4256; Loss Att={'forw': '7.0878'}; Train Acc=0.493; Test Acc=0.3891; Entropy={'forw': '7.0023'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.204966\n",
      "40 1.8598106 2.204966\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:09epoch=237; Loss Pred=1.5146; Val Loss=2.2050; Val Acc=0.2946; Loss Att={'forw': '12.7945'}; Train Acc=0.482; Test Acc=0.3342; Entropy={'forw': '7.6380'}; Entropy_Test=\n",
      "\n",
      "2.141689\n",
      "41 1.8598106 2.141689\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:13epoch=238; Loss Pred=1.5120; Val Loss=2.1417; Val Acc=0.3512; Loss Att={'forw': '9.0437'}; Train Acc=0.475; Test Acc=0.3355; Entropy={'forw': '7.6594'}; Entropy_Test=\n",
      "\n",
      "2.0214486\n",
      "42 1.8598106 2.0214486\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:19epoch=239; Loss Pred=1.4709; Val Loss=2.0214; Val Acc=0.4226; Loss Att={'forw': '12.5069'}; Train Acc=0.515; Test Acc=0.3645; Entropy={'forw': '7.6199'}; Entropy_Test=\n",
      "\n",
      "2.1681724\n",
      "43 1.8598106 2.1681724\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:24epoch=240; Loss Pred=1.4980; Val Loss=2.1682; Val Acc=0.3333; Loss Att={'forw': '6.8740'}; Train Acc=0.507; Test Acc=0.3621; Entropy={'forw': '7.6273'}; Entropy_Test=\n",
      "\n",
      "2.0346456\n",
      "44 1.8598106 2.0346456\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:30epoch=241; Loss Pred=1.4039; Val Loss=2.0346; Val Acc=0.3393; Loss Att={'forw': '9.3573'}; Train Acc=0.533; Test Acc=0.3893; Entropy={'forw': '7.6680'}; Entropy_Test=\n",
      "\n",
      "1.8241781\n",
      "45 1.8598106 1.8241781\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:34epoch=242; Loss Pred=1.3803; Val Loss=1.8242; Val Acc=0.4137; Loss Att={'forw': '7.0056'}; Train Acc=0.538; Test Acc=0.3855; Entropy={'forw': '7.6688'}; Entropy_Test=\n",
      "\n",
      "1.9847479\n",
      "0 1.8241781 1.9847479\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:40epoch=243; Loss Pred=1.4671; Val Loss=1.9847; Val Acc=0.3780; Loss Att={'forw': '7.7077'}; Train Acc=0.525; Test Acc=0.3799; Entropy={'forw': '8.1731'}; Entropy_Test=\n",
      "\n",
      "2.1347618\n",
      "1 1.8241781 2.1347618\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:44epoch=244; Loss Pred=1.5386; Val Loss=2.1348; Val Acc=0.3958; Loss Att={'forw': '7.0062'}; Train Acc=0.516; Test Acc=0.3835; Entropy={'forw': '8.1756'}; Entropy_Test=\n",
      "\n",
      "2.0551524\n",
      "2 1.8241781 2.0551524\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:50epoch=245; Loss Pred=1.6133; Val Loss=2.0552; Val Acc=0.4077; Loss Att={'forw': '10.8366'}; Train Acc=0.472; Test Acc=0.3748; Entropy={'forw': '7.9300'}; Entropy_Test=\n",
      "\n",
      "1.8557814\n",
      "3 1.8241781 1.8557814\n",
      "Logged Successfully: \n",
      "2018-05-22 10:20:55epoch=246; Loss Pred=1.6527; Val Loss=1.8558; Val Acc=0.3988; Loss Att={'forw': '7.3652'}; Train Acc=0.465; Test Acc=0.3676; Entropy={'forw': '7.9357'}; Entropy_Test=\n",
      "\n",
      "2.3373675\n",
      "4 1.8241781 2.3373675\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:01epoch=247; Loss Pred=1.7242; Val Loss=2.3374; Val Acc=0.3214; Loss Att={'forw': '10.0397'}; Train Acc=0.462; Test Acc=0.3397; Entropy={'forw': '8.1845'}; Entropy_Test=\n",
      "\n",
      "2.1744368\n",
      "5 1.8241781 2.1744368\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:05epoch=248; Loss Pred=1.7149; Val Loss=2.1744; Val Acc=0.3423; Loss Att={'forw': '7.5533'}; Train Acc=0.484; Test Acc=0.3629; Entropy={'forw': '8.1650'}; Entropy_Test=\n",
      "\n",
      "2.1135285\n",
      "6 1.8241781 2.1135285\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:11epoch=249; Loss Pred=1.5214; Val Loss=2.1135; Val Acc=0.3571; Loss Att={'forw': '11.0626'}; Train Acc=0.459; Test Acc=0.3442; Entropy={'forw': '7.8926'}; Entropy_Test=\n",
      "\n",
      "2.3322453\n",
      "7 1.8241781 2.3322453\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:15epoch=250; Loss Pred=1.4341; Val Loss=2.3322; Val Acc=0.3214; Loss Att={'forw': '7.0274'}; Train Acc=0.486; Test Acc=0.3489; Entropy={'forw': '7.9225'}; Entropy_Test=\n",
      "\n",
      "2.2765152\n",
      "8 1.8241781 2.2765152\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:21epoch=251; Loss Pred=1.4845; Val Loss=2.2765; Val Acc=0.3750; Loss Att={'forw': '9.3888'}; Train Acc=0.503; Test Acc=0.3853; Entropy={'forw': '8.0900'}; Entropy_Test=\n",
      "\n",
      "2.214084\n",
      "9 1.8241781 2.214084\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:26epoch=252; Loss Pred=1.4857; Val Loss=2.2141; Val Acc=0.3601; Loss Att={'forw': '7.7294'}; Train Acc=0.522; Test Acc=0.3741; Entropy={'forw': '8.0964'}; Entropy_Test=\n",
      "\n",
      "1.6480058\n",
      "10 1.8241781 1.6480058\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:32epoch=253; Loss Pred=1.2628; Val Loss=1.6480; Val Acc=0.5179; Loss Att={'forw': '8.0474'}; Train Acc=0.560; Test Acc=0.3790; Entropy={'forw': '8.2162'}; Entropy_Test=\n",
      "\n",
      "1.6648537\n",
      "0 1.6480058 1.6648537\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:36epoch=254; Loss Pred=1.3371; Val Loss=1.6649; Val Acc=0.4435; Loss Att={'forw': '6.6086'}; Train Acc=0.536; Test Acc=0.3741; Entropy={'forw': '8.2151'}; Entropy_Test=\n",
      "\n",
      "2.1615832\n",
      "1 1.6480058 2.1615832\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:42epoch=255; Loss Pred=1.4521; Val Loss=2.1616; Val Acc=0.3423; Loss Att={'forw': '11.6729'}; Train Acc=0.500; Test Acc=0.3725; Entropy={'forw': '8.0798'}; Entropy_Test=\n",
      "\n",
      "1.6799604\n",
      "2 1.6480058 1.6799604\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:47epoch=256; Loss Pred=1.3996; Val Loss=1.6800; Val Acc=0.3780; Loss Att={'forw': '6.9579'}; Train Acc=0.539; Test Acc=0.3685; Entropy={'forw': '8.0365'}; Entropy_Test=\n",
      "\n",
      "1.8727213\n",
      "3 1.6480058 1.8727213\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:53epoch=257; Loss Pred=1.2905; Val Loss=1.8727; Val Acc=0.4970; Loss Att={'forw': '9.2678'}; Train Acc=0.552; Test Acc=0.4020; Entropy={'forw': '8.2588'}; Entropy_Test=\n",
      "\n",
      "2.1305215\n",
      "4 1.6480058 2.1305215\n",
      "Logged Successfully: \n",
      "2018-05-22 10:21:57epoch=258; Loss Pred=1.2935; Val Loss=2.1305; Val Acc=0.4494; Loss Att={'forw': '6.1664'}; Train Acc=0.560; Test Acc=0.3920; Entropy={'forw': '8.2582'}; Entropy_Test=\n",
      "\n",
      "1.8743517\n",
      "5 1.6480058 1.8743517\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:03epoch=259; Loss Pred=1.2137; Val Loss=1.8744; Val Acc=0.3869; Loss Att={'forw': '10.1948'}; Train Acc=0.590; Test Acc=0.4031; Entropy={'forw': '8.2823'}; Entropy_Test=\n",
      "\n",
      "1.7226542\n",
      "6 1.6480058 1.7226542\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:08epoch=260; Loss Pred=1.2281; Val Loss=1.7227; Val Acc=0.4613; Loss Att={'forw': '7.1261'}; Train Acc=0.585; Test Acc=0.4272; Entropy={'forw': '8.2855'}; Entropy_Test=\n",
      "\n",
      "1.9071782\n",
      "7 1.6480058 1.9071782\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:13epoch=261; Loss Pred=1.3141; Val Loss=1.9072; Val Acc=0.4226; Loss Att={'forw': '10.9203'}; Train Acc=0.540; Test Acc=0.3944; Entropy={'forw': '8.0976'}; Entropy_Test=\n",
      "\n",
      "2.2433655\n",
      "8 1.6480058 2.2433655\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:18epoch=262; Loss Pred=1.2892; Val Loss=2.2434; Val Acc=0.3571; Loss Att={'forw': '7.1025'}; Train Acc=0.546; Test Acc=0.3748; Entropy={'forw': '8.1180'}; Entropy_Test=\n",
      "\n",
      "2.1799319\n",
      "9 1.6480058 2.1799319\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:24epoch=263; Loss Pred=1.4503; Val Loss=2.1799; Val Acc=0.3423; Loss Att={'forw': '13.1434'}; Train Acc=0.550; Test Acc=0.3475; Entropy={'forw': '8.3142'}; Entropy_Test=\n",
      "\n",
      "2.4487855\n",
      "10 1.6480058 2.4487855\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:28epoch=264; Loss Pred=1.5148; Val Loss=2.4488; Val Acc=0.3929; Loss Att={'forw': '9.2400'}; Train Acc=0.521; Test Acc=0.3426; Entropy={'forw': '8.3530'}; Entropy_Test=\n",
      "\n",
      "2.1938548\n",
      "11 1.6480058 2.1938548\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:34epoch=265; Loss Pred=1.4684; Val Loss=2.1939; Val Acc=0.3690; Loss Att={'forw': '16.0294'}; Train Acc=0.492; Test Acc=0.3721; Entropy={'forw': '8.2588'}; Entropy_Test=\n",
      "\n",
      "2.4519045\n",
      "12 1.6480058 2.4519045\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:38epoch=266; Loss Pred=1.2575; Val Loss=2.4519; Val Acc=0.4137; Loss Att={'forw': '10.4575'}; Train Acc=0.562; Test Acc=0.3705; Entropy={'forw': '8.2505'}; Entropy_Test=\n",
      "\n",
      "1.8260844\n",
      "13 1.6480058 1.8260844\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:44epoch=267; Loss Pred=1.3096; Val Loss=1.8261; Val Acc=0.4702; Loss Att={'forw': '10.2591'}; Train Acc=0.581; Test Acc=0.3980; Entropy={'forw': '8.3957'}; Entropy_Test=\n",
      "\n",
      "1.6762892\n",
      "14 1.6480058 1.6762892\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:49epoch=268; Loss Pred=1.2974; Val Loss=1.6763; Val Acc=0.4792; Loss Att={'forw': '7.4657'}; Train Acc=0.550; Test Acc=0.4065; Entropy={'forw': '8.4076'}; Entropy_Test=\n",
      "\n",
      "1.6981574\n",
      "15 1.6480058 1.6981574\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:55epoch=269; Loss Pred=1.3464; Val Loss=1.6982; Val Acc=0.4405; Loss Att={'forw': '7.7083'}; Train Acc=0.551; Test Acc=0.3781; Entropy={'forw': '8.1313'}; Entropy_Test=\n",
      "\n",
      "1.6904752\n",
      "16 1.6480058 1.6904752\n",
      "Logged Successfully: \n",
      "2018-05-22 10:22:59epoch=270; Loss Pred=1.3065; Val Loss=1.6905; Val Acc=0.4435; Loss Att={'forw': '6.5206'}; Train Acc=0.548; Test Acc=0.3674; Entropy={'forw': '8.1347'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3782198\n",
      "17 1.6480058 2.3782198\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:05epoch=271; Loss Pred=1.5375; Val Loss=2.3782; Val Acc=0.4345; Loss Att={'forw': '13.1970'}; Train Acc=0.490; Test Acc=0.3654; Entropy={'forw': '8.3379'}; Entropy_Test=\n",
      "\n",
      "2.4192028\n",
      "18 1.6480058 2.4192028\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:10epoch=272; Loss Pred=1.6880; Val Loss=2.4192; Val Acc=0.3601; Loss Att={'forw': '7.2922'}; Train Acc=0.501; Test Acc=0.3353; Entropy={'forw': '8.3240'}; Entropy_Test=\n",
      "\n",
      "1.843755\n",
      "19 1.6480058 1.843755\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:15epoch=273; Loss Pred=1.3706; Val Loss=1.8438; Val Acc=0.3780; Loss Att={'forw': '9.5104'}; Train Acc=0.553; Test Acc=0.3844; Entropy={'forw': '8.5441'}; Entropy_Test=\n",
      "\n",
      "2.0553403\n",
      "20 1.6480058 2.0553403\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:20epoch=274; Loss Pred=1.4004; Val Loss=2.0553; Val Acc=0.4792; Loss Att={'forw': '6.8786'}; Train Acc=0.544; Test Acc=0.3835; Entropy={'forw': '8.5549'}; Entropy_Test=\n",
      "\n",
      "1.3933364\n",
      "21 1.6480058 1.3933364\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:26epoch=275; Loss Pred=1.0039; Val Loss=1.3933; Val Acc=0.5952; Loss Att={'forw': '8.1338'}; Train Acc=0.613; Test Acc=0.4058; Entropy={'forw': '8.3542'}; Entropy_Test=\n",
      "\n",
      "1.4753542\n",
      "0 1.3933364 1.4753542\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:30epoch=276; Loss Pred=1.0718; Val Loss=1.4754; Val Acc=0.5417; Loss Att={'forw': '6.7205'}; Train Acc=0.602; Test Acc=0.4031; Entropy={'forw': '8.3445'}; Entropy_Test=\n",
      "\n",
      "1.6887052\n",
      "1 1.3933364 1.6887052\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:36epoch=277; Loss Pred=1.2978; Val Loss=1.6887; Val Acc=0.4792; Loss Att={'forw': '9.0476'}; Train Acc=0.534; Test Acc=0.4087; Entropy={'forw': '8.1908'}; Entropy_Test=\n",
      "\n",
      "1.5550287\n",
      "2 1.3933364 1.5550287\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:41epoch=278; Loss Pred=1.2515; Val Loss=1.5550; Val Acc=0.5714; Loss Att={'forw': '7.4292'}; Train Acc=0.546; Test Acc=0.4004; Entropy={'forw': '8.2043'}; Entropy_Test=\n",
      "\n",
      "1.9155513\n",
      "3 1.3933364 1.9155513\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:47epoch=279; Loss Pred=1.3601; Val Loss=1.9156; Val Acc=0.3988; Loss Att={'forw': '13.8003'}; Train Acc=0.525; Test Acc=0.3627; Entropy={'forw': '8.3729'}; Entropy_Test=\n",
      "\n",
      "1.8686504\n",
      "4 1.3933364 1.8686504\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:51epoch=280; Loss Pred=1.3844; Val Loss=1.8687; Val Acc=0.4226; Loss Att={'forw': '10.0385'}; Train Acc=0.509; Test Acc=0.3509; Entropy={'forw': '8.3822'}; Entropy_Test=\n",
      "\n",
      "2.026586\n",
      "5 1.3933364 2.026586\n",
      "Logged Successfully: \n",
      "2018-05-22 10:23:57epoch=281; Loss Pred=1.3379; Val Loss=2.0266; Val Acc=0.4167; Loss Att={'forw': '8.3994'}; Train Acc=0.533; Test Acc=0.3531; Entropy={'forw': '7.9400'}; Entropy_Test=\n",
      "\n",
      "2.0405793\n",
      "6 1.3933364 2.0405793\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:01epoch=282; Loss Pred=1.3929; Val Loss=2.0406; Val Acc=0.4286; Loss Att={'forw': '7.9559'}; Train Acc=0.508; Test Acc=0.3645; Entropy={'forw': '7.9508'}; Entropy_Test=\n",
      "\n",
      "1.9598967\n",
      "7 1.3933364 1.9598967\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:07epoch=283; Loss Pred=1.2999; Val Loss=1.9599; Val Acc=0.3601; Loss Att={'forw': '10.0566'}; Train Acc=0.549; Test Acc=0.3944; Entropy={'forw': '7.8721'}; Entropy_Test=\n",
      "\n",
      "1.7832195\n",
      "8 1.3933364 1.7832195\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:12epoch=284; Loss Pred=1.3755; Val Loss=1.7832; Val Acc=0.3958; Loss Att={'forw': '7.6263'}; Train Acc=0.507; Test Acc=0.4100; Entropy={'forw': '7.8505'}; Entropy_Test=\n",
      "\n",
      "2.0568023\n",
      "9 1.3933364 2.0568023\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:18epoch=285; Loss Pred=1.4234; Val Loss=2.0568; Val Acc=0.4048; Loss Att={'forw': '12.1717'}; Train Acc=0.523; Test Acc=0.3866; Entropy={'forw': '8.1373'}; Entropy_Test=\n",
      "\n",
      "2.2316554\n",
      "10 1.3933364 2.2316554\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:22epoch=286; Loss Pred=1.3796; Val Loss=2.2317; Val Acc=0.3423; Loss Att={'forw': '7.5063'}; Train Acc=0.512; Test Acc=0.3891; Entropy={'forw': '8.1282'}; Entropy_Test=\n",
      "\n",
      "1.8556709\n",
      "11 1.3933364 1.8556709\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:28epoch=287; Loss Pred=1.5802; Val Loss=1.8557; Val Acc=0.4613; Loss Att={'forw': '11.9288'}; Train Acc=0.465; Test Acc=0.3743; Entropy={'forw': '8.1812'}; Entropy_Test=\n",
      "\n",
      "2.1877944\n",
      "12 1.3933364 2.1877944\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:33epoch=288; Loss Pred=1.5129; Val Loss=2.1878; Val Acc=0.3631; Loss Att={'forw': '7.5603'}; Train Acc=0.474; Test Acc=0.3612; Entropy={'forw': '8.1635'}; Entropy_Test=\n",
      "\n",
      "1.9539139\n",
      "13 1.3933364 1.9539139\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:39epoch=289; Loss Pred=1.1651; Val Loss=1.9539; Val Acc=0.3304; Loss Att={'forw': '10.5239'}; Train Acc=0.577; Test Acc=0.3955; Entropy={'forw': '8.3812'}; Entropy_Test=\n",
      "\n",
      "2.1504872\n",
      "14 1.3933364 2.1504872\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:43epoch=290; Loss Pred=1.2218; Val Loss=2.1505; Val Acc=0.4226; Loss Att={'forw': '7.0646'}; Train Acc=0.576; Test Acc=0.3991; Entropy={'forw': '8.4034'}; Entropy_Test=\n",
      "\n",
      "1.8311787\n",
      "15 1.3933364 1.8311787\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:49epoch=291; Loss Pred=1.2502; Val Loss=1.8312; Val Acc=0.4315; Loss Att={'forw': '9.3714'}; Train Acc=0.591; Test Acc=0.4165; Entropy={'forw': '7.9226'}; Entropy_Test=\n",
      "\n",
      "1.8067957\n",
      "16 1.3933364 1.8067957\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:53epoch=292; Loss Pred=1.2220; Val Loss=1.8068; Val Acc=0.4940; Loss Att={'forw': '6.8356'}; Train Acc=0.589; Test Acc=0.4248; Entropy={'forw': '7.9349'}; Entropy_Test=\n",
      "\n",
      "1.773493\n",
      "17 1.3933364 1.773493\n",
      "Logged Successfully: \n",
      "2018-05-22 10:24:59epoch=293; Loss Pred=1.3621; Val Loss=1.7735; Val Acc=0.4702; Loss Att={'forw': '7.5243'}; Train Acc=0.557; Test Acc=0.4248; Entropy={'forw': '8.0347'}; Entropy_Test=\n",
      "\n",
      "1.7442272\n",
      "18 1.3933364 1.7442272\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:04epoch=294; Loss Pred=1.3027; Val Loss=1.7442; Val Acc=0.4494; Loss Att={'forw': '6.4956'}; Train Acc=0.560; Test Acc=0.4288; Entropy={'forw': '8.0326'}; Entropy_Test=\n",
      "\n",
      "1.6124717\n",
      "19 1.3933364 1.6124717\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:09epoch=295; Loss Pred=1.2722; Val Loss=1.6125; Val Acc=0.4613; Loss Att={'forw': '7.3994'}; Train Acc=0.559; Test Acc=0.4429; Entropy={'forw': '7.9308'}; Entropy_Test=\n",
      "\n",
      "2.4254184\n",
      "20 1.3933364 2.4254184\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:14epoch=296; Loss Pred=1.2636; Val Loss=2.4254; Val Acc=0.3482; Loss Att={'forw': '5.7592'}; Train Acc=0.566; Test Acc=0.4058; Entropy={'forw': '7.9246'}; Entropy_Test=\n",
      "\n",
      "1.67853\n",
      "21 1.3933364 1.67853\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:20epoch=297; Loss Pred=1.3749; Val Loss=1.6785; Val Acc=0.4583; Loss Att={'forw': '8.3051'}; Train Acc=0.534; Test Acc=0.3815; Entropy={'forw': '8.3157'}; Entropy_Test=\n",
      "\n",
      "2.1757112\n",
      "22 1.3933364 2.1757112\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:24epoch=298; Loss Pred=1.3788; Val Loss=2.1757; Val Acc=0.3571; Loss Att={'forw': '6.1065'}; Train Acc=0.528; Test Acc=0.3741; Entropy={'forw': '8.3072'}; Entropy_Test=\n",
      "\n",
      "1.9514347\n",
      "23 1.3933364 1.9514347\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:30epoch=299; Loss Pred=1.3827; Val Loss=1.9514; Val Acc=0.3869; Loss Att={'forw': '7.6616'}; Train Acc=0.525; Test Acc=0.3317; Entropy={'forw': '8.1516'}; Entropy_Test=\n",
      "\n",
      "2.1354039\n",
      "24 1.3933364 2.1354039\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:34epoch=300; Loss Pred=1.3210; Val Loss=2.1354; Val Acc=0.3036; Loss Att={'forw': '6.5250'}; Train Acc=0.542; Test Acc=0.3422; Entropy={'forw': '8.1579'}; Entropy_Test=\n",
      "\n",
      "1.8048887\n",
      "25 1.3933364 1.8048887\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:40epoch=301; Loss Pred=1.3850; Val Loss=1.8049; Val Acc=0.5149; Loss Att={'forw': '7.5240'}; Train Acc=0.526; Test Acc=0.3866; Entropy={'forw': '8.1035'}; Entropy_Test=\n",
      "\n",
      "1.852\n",
      "26 1.3933364 1.852\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:45epoch=302; Loss Pred=1.3942; Val Loss=1.8520; Val Acc=0.4613; Loss Att={'forw': '6.5347'}; Train Acc=0.553; Test Acc=0.4080; Entropy={'forw': '8.0950'}; Entropy_Test=\n",
      "\n",
      "2.3631113\n",
      "27 1.3933364 2.3631113\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:50epoch=303; Loss Pred=1.2745; Val Loss=2.3631; Val Acc=0.4286; Loss Att={'forw': '9.1516'}; Train Acc=0.529; Test Acc=0.3830; Entropy={'forw': '8.2099'}; Entropy_Test=\n",
      "\n",
      "1.8224409\n",
      "28 1.3933364 1.8224409\n",
      "Logged Successfully: \n",
      "2018-05-22 10:25:55epoch=304; Loss Pred=1.3113; Val Loss=1.8224; Val Acc=0.4583; Loss Att={'forw': '6.7939'}; Train Acc=0.563; Test Acc=0.3962; Entropy={'forw': '8.1969'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6704594\n",
      "29 1.3933364 1.6704594\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:01epoch=305; Loss Pred=1.2749; Val Loss=1.6705; Val Acc=0.4494; Loss Att={'forw': '10.4226'}; Train Acc=0.544; Test Acc=0.4076; Entropy={'forw': '8.1057'}; Entropy_Test=\n",
      "\n",
      "1.4636238\n",
      "30 1.3933364 1.4636238\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:05epoch=306; Loss Pred=1.1863; Val Loss=1.4636; Val Acc=0.5893; Loss Att={'forw': '6.7394'}; Train Acc=0.590; Test Acc=0.4266; Entropy={'forw': '8.0967'}; Entropy_Test=\n",
      "\n",
      "1.5548441\n",
      "31 1.3933364 1.5548441\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:11epoch=307; Loss Pred=1.1900; Val Loss=1.5548; Val Acc=0.5417; Loss Att={'forw': '8.5033'}; Train Acc=0.581; Test Acc=0.4134; Entropy={'forw': '7.9038'}; Entropy_Test=\n",
      "\n",
      "1.6174675\n",
      "32 1.3933364 1.6174675\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:15epoch=308; Loss Pred=1.2540; Val Loss=1.6175; Val Acc=0.5060; Loss Att={'forw': '6.7935'}; Train Acc=0.590; Test Acc=0.4199; Entropy={'forw': '7.8722'}; Entropy_Test=\n",
      "\n",
      "2.3355095\n",
      "33 1.3933364 2.3355095\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:21epoch=309; Loss Pred=1.1084; Val Loss=2.3355; Val Acc=0.3125; Loss Att={'forw': '8.8221'}; Train Acc=0.604; Test Acc=0.4074; Entropy={'forw': '7.9709'}; Entropy_Test=\n",
      "\n",
      "1.5879996\n",
      "34 1.3933364 1.5879996\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:25epoch=310; Loss Pred=1.0562; Val Loss=1.5880; Val Acc=0.4613; Loss Att={'forw': '6.3845'}; Train Acc=0.617; Test Acc=0.4335; Entropy={'forw': '7.9782'}; Entropy_Test=\n",
      "\n",
      "1.8854469\n",
      "35 1.3933364 1.8854469\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:31epoch=311; Loss Pred=1.2136; Val Loss=1.8854; Val Acc=0.4851; Loss Att={'forw': '9.8371'}; Train Acc=0.620; Test Acc=0.4092; Entropy={'forw': '7.8108'}; Entropy_Test=\n",
      "\n",
      "2.0848558\n",
      "36 1.3933364 2.0848558\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:36epoch=312; Loss Pred=1.2362; Val Loss=2.0849; Val Acc=0.4970; Loss Att={'forw': '6.8347'}; Train Acc=0.596; Test Acc=0.3815; Entropy={'forw': '7.8122'}; Entropy_Test=\n",
      "\n",
      "2.0941002\n",
      "37 1.3933364 2.0941002\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:42epoch=313; Loss Pred=1.2403; Val Loss=2.0941; Val Acc=0.4554; Loss Att={'forw': '8.1347'}; Train Acc=0.572; Test Acc=0.4145; Entropy={'forw': '7.9580'}; Entropy_Test=\n",
      "\n",
      "1.5686027\n",
      "38 1.3933364 1.5686027\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:46epoch=314; Loss Pred=1.2744; Val Loss=1.5686; Val Acc=0.5804; Loss Att={'forw': '6.4812'}; Train Acc=0.593; Test Acc=0.4121; Entropy={'forw': '7.9394'}; Entropy_Test=\n",
      "\n",
      "1.8475486\n",
      "39 1.3933364 1.8475486\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:52epoch=315; Loss Pred=1.2684; Val Loss=1.8475; Val Acc=0.4524; Loss Att={'forw': '9.5175'}; Train Acc=0.587; Test Acc=0.3935; Entropy={'forw': '8.1083'}; Entropy_Test=\n",
      "\n",
      "1.696525\n",
      "40 1.3933364 1.696525\n",
      "Logged Successfully: \n",
      "2018-05-22 10:26:57epoch=316; Loss Pred=1.1883; Val Loss=1.6965; Val Acc=0.4881; Loss Att={'forw': '6.8546'}; Train Acc=0.580; Test Acc=0.4413; Entropy={'forw': '8.0967'}; Entropy_Test=\n",
      "\n",
      "1.82325\n",
      "41 1.3933364 1.82325\n",
      "Logged Successfully: \n",
      "2018-05-22 10:27:03epoch=317; Loss Pred=1.2255; Val Loss=1.8233; Val Acc=0.4851; Loss Att={'forw': '10.6208'}; Train Acc=0.597; Test Acc=0.4125; Entropy={'forw': '8.1298'}; Entropy_Test=\n",
      "\n",
      "1.527969\n",
      "42 1.3933364 1.527969\n",
      "Logged Successfully: \n",
      "2018-05-22 10:27:07epoch=318; Loss Pred=1.2654; Val Loss=1.5280; Val Acc=0.5149; Loss Att={'forw': '7.0940'}; Train Acc=0.601; Test Acc=0.4270; Entropy={'forw': '8.1283'}; Entropy_Test=\n",
      "\n",
      "1.7347146\n",
      "43 1.3933364 1.7347146\n",
      "Logged Successfully: \n",
      "2018-05-22 10:27:14epoch=319; Loss Pred=1.3584; Val Loss=1.7347; Val Acc=0.4613; Loss Att={'forw': '8.8758'}; Train Acc=0.564; Test Acc=0.3790; Entropy={'forw': '8.0016'}; Entropy_Test=\n",
      "\n",
      "1.6733943\n",
      "44 1.3933364 1.6733943\n",
      "Logged Successfully: \n",
      "2018-05-22 10:27:18epoch=320; Loss Pred=1.2962; Val Loss=1.6734; Val Acc=0.4792; Loss Att={'forw': '7.1878'}; Train Acc=0.543; Test Acc=0.3786; Entropy={'forw': '7.9940'}; Entropy_Test=\n",
      "\n",
      "1.9053379\n",
      "45 1.3933364 1.9053379\n",
      "Logged Successfully: \n",
      "2018-05-22 10:27:24epoch=321; Loss Pred=1.2335; Val Loss=1.9053; Val Acc=0.4554; Loss Att={'forw': '7.9926'}; Train Acc=0.583; Test Acc=0.3893; Entropy={'forw': '7.9107'}; Entropy_Test=\n",
      "\n",
      "1.6673175\n",
      "46 1.3933364 1.6673175\n",
      "Logged Successfully: \n",
      "2018-05-22 10:27:29epoch=322; Loss Pred=1.1960; Val Loss=1.6673; Val Acc=0.4970; Loss Att={'forw': '6.7866'}; Train Acc=0.582; Test Acc=0.3855; Entropy={'forw': '7.9077'}; Entropy_Test=\n",
      "\n",
      "1.7300541\n",
      "47 1.3933364 1.7300541\n",
      "Logged Successfully: \n",
      "2018-05-22 10:27:35epoch=323; Loss Pred=1.0818; Val Loss=1.7301; Val Acc=0.5030; Loss Att={'forw': '12.5929'}; Train Acc=0.607; Test Acc=0.4254; Entropy={'forw': '8.1610'}; Entropy_Test=\n",
      "\n",
      "1.5480182\n",
      "48 1.3933364 1.5480182\n",
      "Logged Successfully: \n",
      "2018-05-22 10:27:39epoch=324; Loss Pred=1.2118; Val Loss=1.5480; Val Acc=0.5357; Loss Att={'forw': '6.4687'}; Train Acc=0.589; Test Acc=0.4377; Entropy={'forw': '8.1640'}; Entropy_Test=\n",
      "\n",
      "1.5178813\n",
      "49 1.3933364 1.5178813\n",
      "Logged Successfully: \n",
      "2018-05-22 10:27:46epoch=325; Loss Pred=0.9331; Val Loss=1.5179; Val Acc=0.4762; Loss Att={'forw': '7.6583'}; Train Acc=0.664; Test Acc=0.4839; Entropy={'forw': '7.9844'}; Entropy_Test=\n",
      "\n",
      "1.9921739\n",
      "50 1.3933364 1.9921739\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 327\n",
      "Optimization Finished!\n",
      "********** replication  1  **********\n",
      "video_classification\n",
      "0.0\n",
      "(499, 40, 2048) (499, 1) (99, 40, 2048) (99, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: video_classification\n",
      "    hid: \t\t\t100,\n",
      "    h_hid: \t\t\t200\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.4\n",
      "    TRAIN/TEST_SIZE: \t499/0, SEQ_LEN: 40\n",
      "Logged Successfully: \n",
      "3.2179012\n",
      "0 10000000000.0 3.2179012\n",
      "Logged Successfully: \n",
      "2018-05-22 10:29:25epoch=0; Loss Pred=3.2239; Val Loss=3.2179; Val Acc=0.0536; Loss Att={'forw': '1.0795'}; Train Acc=0.047; Test Acc=0.0513; Entropy={'forw': '9.2964'}; Entropy_Test=\n",
      "\n",
      "3.185659\n",
      "0 3.2179012 3.185659\n",
      "Logged Successfully: \n",
      "2018-05-22 10:29:32epoch=1; Loss Pred=3.0911; Val Loss=3.1857; Val Acc=0.1250; Loss Att={'forw': '11.7551'}; Train Acc=0.119; Test Acc=0.0915; Entropy={'forw': '8.4045'}; Entropy_Test=\n",
      "\n",
      "3.196275\n",
      "0 3.185659 3.196275\n",
      "Logged Successfully: \n",
      "2018-05-22 10:29:38epoch=2; Loss Pred=3.0871; Val Loss=3.1963; Val Acc=0.0893; Loss Att={'forw': '3.6186'}; Train Acc=0.106; Test Acc=0.0859; Entropy={'forw': '8.5111'}; Entropy_Test=\n",
      "\n",
      "3.3407958\n",
      "1 3.185659 3.3407958\n",
      "Logged Successfully: \n",
      "2018-05-22 10:29:45epoch=3; Loss Pred=3.0816; Val Loss=3.3408; Val Acc=0.1161; Loss Att={'forw': '5.1359'}; Train Acc=0.117; Test Acc=0.1004; Entropy={'forw': '7.2549'}; Entropy_Test=\n",
      "\n",
      "3.1485271\n",
      "2 3.185659 3.1485271\n",
      "Logged Successfully: \n",
      "2018-05-22 10:29:50epoch=4; Loss Pred=3.0928; Val Loss=3.1485; Val Acc=0.1429; Loss Att={'forw': '1.7144'}; Train Acc=0.125; Test Acc=0.0951; Entropy={'forw': '7.5138'}; Entropy_Test=\n",
      "\n",
      "3.3315637\n",
      "0 3.1485271 3.3315637\n",
      "Logged Successfully: \n",
      "2018-05-22 10:29:57epoch=5; Loss Pred=2.9264; Val Loss=3.3316; Val Acc=0.0804; Loss Att={'forw': '10.6614'}; Train Acc=0.150; Test Acc=0.1174; Entropy={'forw': '8.3719'}; Entropy_Test=\n",
      "\n",
      "2.9592054\n",
      "1 3.1485271 2.9592054\n",
      "Logged Successfully: \n",
      "2018-05-22 10:30:02epoch=6; Loss Pred=2.9312; Val Loss=2.9592; Val Acc=0.1280; Loss Att={'forw': '2.3968'}; Train Acc=0.128; Test Acc=0.1040; Entropy={'forw': '7.9343'}; Entropy_Test=\n",
      "\n",
      "2.816311\n",
      "0 2.9592054 2.816311\n",
      "Logged Successfully: \n",
      "2018-05-22 10:30:09epoch=7; Loss Pred=2.7162; Val Loss=2.8163; Val Acc=0.1845; Loss Att={'forw': '12.8222'}; Train Acc=0.172; Test Acc=0.1444; Entropy={'forw': '7.7437'}; Entropy_Test=\n",
      "\n",
      "2.9577544\n",
      "0 2.816311 2.9577544\n",
      "Logged Successfully: \n",
      "2018-05-22 10:30:15epoch=8; Loss Pred=2.7604; Val Loss=2.9578; Val Acc=0.1161; Loss Att={'forw': '2.3306'}; Train Acc=0.196; Test Acc=0.1431; Entropy={'forw': '7.8698'}; Entropy_Test=\n",
      "\n",
      "2.8733294\n",
      "1 2.816311 2.8733294\n",
      "Logged Successfully: \n",
      "2018-05-22 10:30:22epoch=9; Loss Pred=2.7687; Val Loss=2.8733; Val Acc=0.2649; Loss Att={'forw': '7.2173'}; Train Acc=0.180; Test Acc=0.1339; Entropy={'forw': '8.3073'}; Entropy_Test=\n",
      "\n",
      "2.9964488\n",
      "2 2.816311 2.9964488\n",
      "Logged Successfully: \n",
      "2018-05-22 10:30:29epoch=10; Loss Pred=2.7031; Val Loss=2.9964; Val Acc=0.1726; Loss Att={'forw': '2.9532'}; Train Acc=0.225; Test Acc=0.1451; Entropy={'forw': '8.2104'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8599021\n",
      "3 2.816311 2.8599021\n",
      "Logged Successfully: \n",
      "2018-05-22 10:30:35epoch=11; Loss Pred=2.7835; Val Loss=2.8599; Val Acc=0.2381; Loss Att={'forw': '22.6377'}; Train Acc=0.182; Test Acc=0.1699; Entropy={'forw': '6.9273'}; Entropy_Test=\n",
      "\n",
      "2.9504418\n",
      "4 2.816311 2.9504418\n",
      "Logged Successfully: \n",
      "2018-05-22 10:30:40epoch=12; Loss Pred=2.8180; Val Loss=2.9504; Val Acc=0.1815; Loss Att={'forw': '5.9842'}; Train Acc=0.203; Test Acc=0.1625; Entropy={'forw': '6.7821'}; Entropy_Test=\n",
      "\n",
      "2.8320231\n",
      "5 2.816311 2.8320231\n",
      "Logged Successfully: \n",
      "2018-05-22 10:30:46epoch=13; Loss Pred=2.8152; Val Loss=2.8320; Val Acc=0.1339; Loss Att={'forw': '6.9755'}; Train Acc=0.195; Test Acc=0.1529; Entropy={'forw': '6.1704'}; Entropy_Test=\n",
      "\n",
      "3.0536988\n",
      "6 2.816311 3.0536988\n",
      "Logged Successfully: \n",
      "2018-05-22 10:30:50epoch=14; Loss Pred=2.6646; Val Loss=3.0537; Val Acc=0.1161; Loss Att={'forw': '4.0086'}; Train Acc=0.210; Test Acc=0.1362; Entropy={'forw': '6.3736'}; Entropy_Test=\n",
      "\n",
      "3.0375865\n",
      "7 2.816311 3.0375865\n",
      "Logged Successfully: \n",
      "2018-05-22 10:30:57epoch=15; Loss Pred=2.8916; Val Loss=3.0376; Val Acc=0.1786; Loss Att={'forw': '9.4060'}; Train Acc=0.164; Test Acc=0.1324; Entropy={'forw': '5.5837'}; Entropy_Test=\n",
      "\n",
      "3.132202\n",
      "8 2.816311 3.132202\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:01epoch=16; Loss Pred=2.8920; Val Loss=3.1322; Val Acc=0.1607; Loss Att={'forw': '3.2586'}; Train Acc=0.176; Test Acc=0.1442; Entropy={'forw': '5.5041'}; Entropy_Test=\n",
      "\n",
      "2.6427045\n",
      "9 2.816311 2.6427045\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:07epoch=17; Loss Pred=2.7474; Val Loss=2.6427; Val Acc=0.1815; Loss Att={'forw': '6.9421'}; Train Acc=0.168; Test Acc=0.1509; Entropy={'forw': '6.9491'}; Entropy_Test=\n",
      "\n",
      "2.6602795\n",
      "0 2.6427045 2.6602795\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:12epoch=18; Loss Pred=2.6617; Val Loss=2.6603; Val Acc=0.2440; Loss Att={'forw': '3.3543'}; Train Acc=0.196; Test Acc=0.1453; Entropy={'forw': '7.0843'}; Entropy_Test=\n",
      "\n",
      "2.5914881\n",
      "1 2.6427045 2.5914881\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:18epoch=19; Loss Pred=2.3811; Val Loss=2.5915; Val Acc=0.1815; Loss Att={'forw': '6.8263'}; Train Acc=0.256; Test Acc=0.2346; Entropy={'forw': '6.3373'}; Entropy_Test=\n",
      "\n",
      "2.5052927\n",
      "0 2.5914881 2.5052927\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:23epoch=20; Loss Pred=2.2301; Val Loss=2.5053; Val Acc=0.1250; Loss Att={'forw': '3.4579'}; Train Acc=0.281; Test Acc=0.2243; Entropy={'forw': '6.2893'}; Entropy_Test=\n",
      "\n",
      "2.6113224\n",
      "0 2.5052927 2.6113224\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:29epoch=21; Loss Pred=2.4542; Val Loss=2.6113; Val Acc=0.1964; Loss Att={'forw': '8.1758'}; Train Acc=0.244; Test Acc=0.2170; Entropy={'forw': '6.7163'}; Entropy_Test=\n",
      "\n",
      "2.698525\n",
      "1 2.5052927 2.698525\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:34epoch=22; Loss Pred=2.3505; Val Loss=2.6985; Val Acc=0.2173; Loss Att={'forw': '4.1585'}; Train Acc=0.242; Test Acc=0.2170; Entropy={'forw': '6.7786'}; Entropy_Test=\n",
      "\n",
      "2.5874026\n",
      "2 2.5052927 2.5874026\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:40epoch=23; Loss Pred=2.4175; Val Loss=2.5874; Val Acc=0.2143; Loss Att={'forw': '34.4553'}; Train Acc=0.258; Test Acc=0.1864; Entropy={'forw': '5.4762'}; Entropy_Test=\n",
      "\n",
      "2.4679508\n",
      "3 2.5052927 2.4679508\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:44epoch=24; Loss Pred=2.4207; Val Loss=2.4680; Val Acc=0.2083; Loss Att={'forw': '4.6353'}; Train Acc=0.232; Test Acc=0.2112; Entropy={'forw': '6.3138'}; Entropy_Test=\n",
      "\n",
      "2.6578999\n",
      "0 2.4679508 2.6578999\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:51epoch=25; Loss Pred=2.6137; Val Loss=2.6579; Val Acc=0.1518; Loss Att={'forw': '7.7091'}; Train Acc=0.178; Test Acc=0.1721; Entropy={'forw': '5.4110'}; Entropy_Test=\n",
      "\n",
      "2.4485116\n",
      "1 2.4679508 2.4485116\n",
      "Logged Successfully: \n",
      "2018-05-22 10:31:55epoch=26; Loss Pred=2.5665; Val Loss=2.4485; Val Acc=0.2530; Loss Att={'forw': '3.8686'}; Train Acc=0.169; Test Acc=0.1998; Entropy={'forw': '5.4988'}; Entropy_Test=\n",
      "\n",
      "2.3739064\n",
      "0 2.4485116 2.3739064\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:01epoch=27; Loss Pred=2.3427; Val Loss=2.3739; Val Acc=0.3185; Loss Att={'forw': '10.2207'}; Train Acc=0.289; Test Acc=0.2223; Entropy={'forw': '6.3226'}; Entropy_Test=\n",
      "\n",
      "2.5318675\n",
      "0 2.3739064 2.5318675\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:06epoch=28; Loss Pred=2.3519; Val Loss=2.5319; Val Acc=0.2321; Loss Att={'forw': '3.6508'}; Train Acc=0.275; Test Acc=0.2080; Entropy={'forw': '6.4200'}; Entropy_Test=\n",
      "\n",
      "2.580792\n",
      "1 2.3739064 2.580792\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:12epoch=29; Loss Pred=2.3672; Val Loss=2.5808; Val Acc=0.2083; Loss Att={'forw': '9.0001'}; Train Acc=0.247; Test Acc=0.1989; Entropy={'forw': '5.8959'}; Entropy_Test=\n",
      "\n",
      "2.5868602\n",
      "2 2.3739064 2.5868602\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:17epoch=30; Loss Pred=2.4114; Val Loss=2.5869; Val Acc=0.2262; Loss Att={'forw': '5.4058'}; Train Acc=0.273; Test Acc=0.2143; Entropy={'forw': '5.8882'}; Entropy_Test=\n",
      "\n",
      "2.6807206\n",
      "3 2.3739064 2.6807206\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:23epoch=31; Loss Pred=2.5228; Val Loss=2.6807; Val Acc=0.1458; Loss Att={'forw': '18.8720'}; Train Acc=0.257; Test Acc=0.1897; Entropy={'forw': '7.0676'}; Entropy_Test=\n",
      "\n",
      "2.8474743\n",
      "4 2.3739064 2.8474743\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:28epoch=32; Loss Pred=2.4992; Val Loss=2.8475; Val Acc=0.1786; Loss Att={'forw': '5.7428'}; Train Acc=0.264; Test Acc=0.2045; Entropy={'forw': '7.4314'}; Entropy_Test=\n",
      "\n",
      "2.9056232\n",
      "5 2.3739064 2.9056232\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:34epoch=33; Loss Pred=2.6011; Val Loss=2.9056; Val Acc=0.1696; Loss Att={'forw': '8.4018'}; Train Acc=0.262; Test Acc=0.1922; Entropy={'forw': '4.9090'}; Entropy_Test=\n",
      "\n",
      "2.985577\n",
      "6 2.3739064 2.985577\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:39epoch=34; Loss Pred=2.6817; Val Loss=2.9856; Val Acc=0.1786; Loss Att={'forw': '4.5790'}; Train Acc=0.213; Test Acc=0.1853; Entropy={'forw': '4.9183'}; Entropy_Test=\n",
      "\n",
      "2.3280337\n",
      "7 2.3739064 2.3280337\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:45epoch=35; Loss Pred=2.2696; Val Loss=2.3280; Val Acc=0.2321; Loss Att={'forw': '8.1404'}; Train Acc=0.266; Test Acc=0.2123; Entropy={'forw': '6.6599'}; Entropy_Test=\n",
      "\n",
      "2.4054909\n",
      "0 2.3280337 2.4054909\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:49epoch=36; Loss Pred=2.2146; Val Loss=2.4055; Val Acc=0.2589; Loss Att={'forw': '3.6281'}; Train Acc=0.240; Test Acc=0.2067; Entropy={'forw': '6.4288'}; Entropy_Test=\n",
      "\n",
      "2.4837415\n",
      "1 2.3280337 2.4837415\n",
      "Logged Successfully: \n",
      "2018-05-22 10:32:55epoch=37; Loss Pred=2.2900; Val Loss=2.4837; Val Acc=0.2619; Loss Att={'forw': '8.6852'}; Train Acc=0.296; Test Acc=0.2259; Entropy={'forw': '6.5055'}; Entropy_Test=\n",
      "\n",
      "2.866752\n",
      "2 2.3280337 2.866752\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:00epoch=38; Loss Pred=2.2661; Val Loss=2.8668; Val Acc=0.1964; Loss Att={'forw': '4.5761'}; Train Acc=0.252; Test Acc=0.2467; Entropy={'forw': '6.5766'}; Entropy_Test=\n",
      "\n",
      "2.2312036\n",
      "3 2.3280337 2.2312036\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:06epoch=39; Loss Pred=2.2261; Val Loss=2.2312; Val Acc=0.3363; Loss Att={'forw': '9.3547'}; Train Acc=0.279; Test Acc=0.2176; Entropy={'forw': '6.8137'}; Entropy_Test=\n",
      "\n",
      "2.2621608\n",
      "0 2.2312036 2.2621608\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:11epoch=40; Loss Pred=2.1847; Val Loss=2.2622; Val Acc=0.2857; Loss Att={'forw': '5.8416'}; Train Acc=0.264; Test Acc=0.2290; Entropy={'forw': '6.7055'}; Entropy_Test=\n",
      "\n",
      "2.5427086\n",
      "1 2.2312036 2.5427086\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:17epoch=41; Loss Pred=2.2961; Val Loss=2.5427; Val Acc=0.1964; Loss Att={'forw': '7.4792'}; Train Acc=0.307; Test Acc=0.2317; Entropy={'forw': '7.1871'}; Entropy_Test=\n",
      "\n",
      "2.4784796\n",
      "2 2.2312036 2.4784796\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:21epoch=42; Loss Pred=2.2410; Val Loss=2.4785; Val Acc=0.2143; Loss Att={'forw': '3.8481'}; Train Acc=0.318; Test Acc=0.2451; Entropy={'forw': '7.0689'}; Entropy_Test=\n",
      "\n",
      "3.1056113\n",
      "3 2.2312036 3.1056113\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:28epoch=43; Loss Pred=2.4611; Val Loss=3.1056; Val Acc=0.1607; Loss Att={'forw': '11.0853'}; Train Acc=0.240; Test Acc=0.1812; Entropy={'forw': '6.8197'}; Entropy_Test=\n",
      "\n",
      "2.6948762\n",
      "4 2.2312036 2.6948762\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:32epoch=44; Loss Pred=2.5778; Val Loss=2.6949; Val Acc=0.1994; Loss Att={'forw': '5.7211'}; Train Acc=0.242; Test Acc=0.1797; Entropy={'forw': '6.7065'}; Entropy_Test=\n",
      "\n",
      "2.5299015\n",
      "5 2.2312036 2.5299015\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:38epoch=45; Loss Pred=2.6286; Val Loss=2.5299; Val Acc=0.2083; Loss Att={'forw': '11.4835'}; Train Acc=0.211; Test Acc=0.1879; Entropy={'forw': '6.0278'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8594396\n",
      "6 2.2312036 2.8594396\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:43epoch=46; Loss Pred=2.6581; Val Loss=2.8594; Val Acc=0.1696; Loss Att={'forw': '4.2289'}; Train Acc=0.211; Test Acc=0.1975; Entropy={'forw': '6.1209'}; Entropy_Test=\n",
      "\n",
      "2.8040578\n",
      "7 2.2312036 2.8040578\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:49epoch=47; Loss Pred=2.5316; Val Loss=2.8041; Val Acc=0.1786; Loss Att={'forw': '10.3984'}; Train Acc=0.268; Test Acc=0.1900; Entropy={'forw': '5.0373'}; Entropy_Test=\n",
      "\n",
      "2.9827397\n",
      "8 2.2312036 2.9827397\n",
      "Logged Successfully: \n",
      "2018-05-22 10:33:54epoch=48; Loss Pred=2.5010; Val Loss=2.9827; Val Acc=0.1429; Loss Att={'forw': '7.0638'}; Train Acc=0.262; Test Acc=0.1763; Entropy={'forw': '4.9754'}; Entropy_Test=\n",
      "\n",
      "2.5313962\n",
      "9 2.2312036 2.5313962\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:00epoch=49; Loss Pred=2.4489; Val Loss=2.5314; Val Acc=0.2262; Loss Att={'forw': '19.8110'}; Train Acc=0.236; Test Acc=0.2054; Entropy={'forw': '4.9052'}; Entropy_Test=\n",
      "\n",
      "2.5559485\n",
      "10 2.2312036 2.5559485\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:05epoch=50; Loss Pred=2.3633; Val Loss=2.5559; Val Acc=0.2470; Loss Att={'forw': '12.9952'}; Train Acc=0.251; Test Acc=0.2092; Entropy={'forw': '4.8904'}; Entropy_Test=\n",
      "\n",
      "3.158033\n",
      "11 2.2312036 3.158033\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:11epoch=51; Loss Pred=2.5443; Val Loss=3.1580; Val Acc=0.1339; Loss Att={'forw': '15.2717'}; Train Acc=0.268; Test Acc=0.2011; Entropy={'forw': '4.8136'}; Entropy_Test=\n",
      "\n",
      "2.8041604\n",
      "12 2.2312036 2.8041604\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:15epoch=52; Loss Pred=2.4970; Val Loss=2.8042; Val Acc=0.1964; Loss Att={'forw': '11.7796'}; Train Acc=0.264; Test Acc=0.1866; Entropy={'forw': '4.8315'}; Entropy_Test=\n",
      "\n",
      "2.5552135\n",
      "13 2.2312036 2.5552135\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:21epoch=53; Loss Pred=2.3166; Val Loss=2.5552; Val Acc=0.2351; Loss Att={'forw': '20.1464'}; Train Acc=0.294; Test Acc=0.2379; Entropy={'forw': '4.7441'}; Entropy_Test=\n",
      "\n",
      "2.4467251\n",
      "14 2.2312036 2.4467251\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:26epoch=54; Loss Pred=2.2512; Val Loss=2.4467; Val Acc=0.2589; Loss Att={'forw': '16.3326'}; Train Acc=0.298; Test Acc=0.2248; Entropy={'forw': '4.8135'}; Entropy_Test=\n",
      "\n",
      "2.430256\n",
      "15 2.2312036 2.430256\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:32epoch=55; Loss Pred=2.2437; Val Loss=2.4303; Val Acc=0.1964; Loss Att={'forw': '8.3486'}; Train Acc=0.268; Test Acc=0.2158; Entropy={'forw': '5.1072'}; Entropy_Test=\n",
      "\n",
      "2.7001932\n",
      "16 2.2312036 2.7001932\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:37epoch=56; Loss Pred=2.1903; Val Loss=2.7002; Val Acc=0.1696; Loss Att={'forw': '7.6170'}; Train Acc=0.268; Test Acc=0.2292; Entropy={'forw': '5.0715'}; Entropy_Test=\n",
      "\n",
      "2.4182982\n",
      "17 2.2312036 2.4182982\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:43epoch=57; Loss Pred=2.3878; Val Loss=2.4183; Val Acc=0.2292; Loss Att={'forw': '10.0122'}; Train Acc=0.227; Test Acc=0.2000; Entropy={'forw': '4.9749'}; Entropy_Test=\n",
      "\n",
      "2.7698696\n",
      "18 2.2312036 2.7698696\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:48epoch=58; Loss Pred=2.4052; Val Loss=2.7699; Val Acc=0.1696; Loss Att={'forw': '8.5630'}; Train Acc=0.225; Test Acc=0.2025; Entropy={'forw': '5.0013'}; Entropy_Test=\n",
      "\n",
      "2.7281506\n",
      "19 2.2312036 2.7281506\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:54epoch=59; Loss Pred=2.3083; Val Loss=2.7282; Val Acc=0.1815; Loss Att={'forw': '8.9908'}; Train Acc=0.267; Test Acc=0.2335; Entropy={'forw': '6.1812'}; Entropy_Test=\n",
      "\n",
      "2.458165\n",
      "20 2.2312036 2.458165\n",
      "Logged Successfully: \n",
      "2018-05-22 10:34:58epoch=60; Loss Pred=2.4374; Val Loss=2.4582; Val Acc=0.2649; Loss Att={'forw': '6.3078'}; Train Acc=0.273; Test Acc=0.2290; Entropy={'forw': '6.0746'}; Entropy_Test=\n",
      "\n",
      "2.4333444\n",
      "21 2.2312036 2.4333444\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:05epoch=61; Loss Pred=2.3517; Val Loss=2.4333; Val Acc=0.2500; Loss Att={'forw': '8.1094'}; Train Acc=0.250; Test Acc=0.2306; Entropy={'forw': '5.1272'}; Entropy_Test=\n",
      "\n",
      "2.5012434\n",
      "22 2.2312036 2.5012434\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:09epoch=62; Loss Pred=2.3116; Val Loss=2.5012; Val Acc=0.2619; Loss Att={'forw': '7.2598'}; Train Acc=0.268; Test Acc=0.2326; Entropy={'forw': '5.1167'}; Entropy_Test=\n",
      "\n",
      "2.5017705\n",
      "23 2.2312036 2.5017705\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:15epoch=63; Loss Pred=2.1758; Val Loss=2.5018; Val Acc=0.2887; Loss Att={'forw': '11.5705'}; Train Acc=0.273; Test Acc=0.2212; Entropy={'forw': '5.1668'}; Entropy_Test=\n",
      "\n",
      "2.4988434\n",
      "24 2.2312036 2.4988434\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:20epoch=64; Loss Pred=2.2206; Val Loss=2.4988; Val Acc=0.1994; Loss Att={'forw': '8.3250'}; Train Acc=0.270; Test Acc=0.2355; Entropy={'forw': '5.1606'}; Entropy_Test=\n",
      "\n",
      "2.8526542\n",
      "25 2.2312036 2.8526542\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:26epoch=65; Loss Pred=2.3949; Val Loss=2.8527; Val Acc=0.2054; Loss Att={'forw': '16.1137'}; Train Acc=0.268; Test Acc=0.2134; Entropy={'forw': '5.7566'}; Entropy_Test=\n",
      "\n",
      "2.723482\n",
      "26 2.2312036 2.723482\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:31epoch=66; Loss Pred=2.2429; Val Loss=2.7235; Val Acc=0.1875; Loss Att={'forw': '12.8990'}; Train Acc=0.321; Test Acc=0.2143; Entropy={'forw': '5.8326'}; Entropy_Test=\n",
      "\n",
      "2.6392562\n",
      "27 2.2312036 2.6392562\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:37epoch=67; Loss Pred=2.2358; Val Loss=2.6393; Val Acc=0.2054; Loss Att={'forw': '10.1892'}; Train Acc=0.284; Test Acc=0.2650; Entropy={'forw': '6.2444'}; Entropy_Test=\n",
      "\n",
      "2.6406403\n",
      "28 2.2312036 2.6406403\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:42epoch=68; Loss Pred=2.1756; Val Loss=2.6406; Val Acc=0.1518; Loss Att={'forw': '9.3436'}; Train Acc=0.338; Test Acc=0.2433; Entropy={'forw': '6.2490'}; Entropy_Test=\n",
      "\n",
      "2.5532634\n",
      "29 2.2312036 2.5532634\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:48epoch=69; Loss Pred=2.0425; Val Loss=2.5533; Val Acc=0.2321; Loss Att={'forw': '9.0855'}; Train Acc=0.358; Test Acc=0.2763; Entropy={'forw': '6.3196'}; Entropy_Test=\n",
      "\n",
      "2.5298727\n",
      "30 2.2312036 2.5298727\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:52epoch=70; Loss Pred=2.0481; Val Loss=2.5299; Val Acc=0.2798; Loss Att={'forw': '6.0227'}; Train Acc=0.328; Test Acc=0.2712; Entropy={'forw': '6.3895'}; Entropy_Test=\n",
      "\n",
      "2.3033745\n",
      "31 2.2312036 2.3033745\n",
      "Logged Successfully: \n",
      "2018-05-22 10:35:58epoch=71; Loss Pred=2.2046; Val Loss=2.3034; Val Acc=0.3095; Loss Att={'forw': '11.9390'}; Train Acc=0.311; Test Acc=0.2594; Entropy={'forw': '6.6401'}; Entropy_Test=\n",
      "\n",
      "2.6592662\n",
      "32 2.2312036 2.6592662\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:03epoch=72; Loss Pred=2.1608; Val Loss=2.6593; Val Acc=0.3244; Loss Att={'forw': '9.3764'}; Train Acc=0.314; Test Acc=0.2382; Entropy={'forw': '6.6396'}; Entropy_Test=\n",
      "\n",
      "2.3712382\n",
      "33 2.2312036 2.3712382\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:09epoch=73; Loss Pred=2.0677; Val Loss=2.3712; Val Acc=0.3006; Loss Att={'forw': '8.5823'}; Train Acc=0.347; Test Acc=0.2500; Entropy={'forw': '6.0241'}; Entropy_Test=\n",
      "\n",
      "2.6304398\n",
      "34 2.2312036 2.6304398\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:14epoch=74; Loss Pred=2.0335; Val Loss=2.6304; Val Acc=0.2321; Loss Att={'forw': '6.8018'}; Train Acc=0.314; Test Acc=0.2560; Entropy={'forw': '6.0047'}; Entropy_Test=\n",
      "\n",
      "2.3636403\n",
      "35 2.2312036 2.3636403\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:20epoch=75; Loss Pred=1.9875; Val Loss=2.3636; Val Acc=0.2500; Loss Att={'forw': '12.4444'}; Train Acc=0.349; Test Acc=0.2850; Entropy={'forw': '7.1622'}; Entropy_Test=\n",
      "\n",
      "2.471791\n",
      "36 2.2312036 2.471791\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:24epoch=76; Loss Pred=1.9252; Val Loss=2.4718; Val Acc=0.2530; Loss Att={'forw': '8.6434'}; Train Acc=0.368; Test Acc=0.2759; Entropy={'forw': '7.2369'}; Entropy_Test=\n",
      "\n",
      "2.6099718\n",
      "37 2.2312036 2.6099718\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:30epoch=77; Loss Pred=2.2560; Val Loss=2.6100; Val Acc=0.1964; Loss Att={'forw': '14.3656'}; Train Acc=0.326; Test Acc=0.2125; Entropy={'forw': '7.2202'}; Entropy_Test=\n",
      "\n",
      "2.5206733\n",
      "38 2.2312036 2.5206733\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:35epoch=78; Loss Pred=2.3845; Val Loss=2.5207; Val Acc=0.2440; Loss Att={'forw': '8.8198'}; Train Acc=0.258; Test Acc=0.1989; Entropy={'forw': '7.2032'}; Entropy_Test=\n",
      "\n",
      "2.7139652\n",
      "39 2.2312036 2.7139652\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:41epoch=79; Loss Pred=1.9398; Val Loss=2.7140; Val Acc=0.2440; Loss Att={'forw': '11.9416'}; Train Acc=0.345; Test Acc=0.2547; Entropy={'forw': '6.1531'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3196366\n",
      "40 2.2312036 2.3196366\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:46epoch=80; Loss Pred=1.9937; Val Loss=2.3196; Val Acc=0.2619; Loss Att={'forw': '8.4825'}; Train Acc=0.346; Test Acc=0.2670; Entropy={'forw': '6.1159'}; Entropy_Test=\n",
      "\n",
      "2.7130568\n",
      "41 2.2312036 2.7130568\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:52epoch=81; Loss Pred=2.1926; Val Loss=2.7131; Val Acc=0.1786; Loss Att={'forw': '13.4840'}; Train Acc=0.292; Test Acc=0.2266; Entropy={'forw': '6.2090'}; Entropy_Test=\n",
      "\n",
      "2.669405\n",
      "42 2.2312036 2.669405\n",
      "Logged Successfully: \n",
      "2018-05-22 10:36:56epoch=82; Loss Pred=2.2008; Val Loss=2.6694; Val Acc=0.1875; Loss Att={'forw': '7.0550'}; Train Acc=0.294; Test Acc=0.2304; Entropy={'forw': '6.1921'}; Entropy_Test=\n",
      "\n",
      "2.6346786\n",
      "43 2.2312036 2.6346786\n",
      "Logged Successfully: \n",
      "2018-05-22 10:37:03epoch=83; Loss Pred=2.1158; Val Loss=2.6347; Val Acc=0.1875; Loss Att={'forw': '7.5419'}; Train Acc=0.307; Test Acc=0.2449; Entropy={'forw': '5.3933'}; Entropy_Test=\n",
      "\n",
      "2.3833728\n",
      "44 2.2312036 2.3833728\n",
      "Logged Successfully: \n",
      "2018-05-22 10:37:07epoch=84; Loss Pred=2.1320; Val Loss=2.3834; Val Acc=0.3095; Loss Att={'forw': '5.9757'}; Train Acc=0.333; Test Acc=0.2346; Entropy={'forw': '5.3945'}; Entropy_Test=\n",
      "\n",
      "2.52084\n",
      "45 2.2312036 2.52084\n",
      "Logged Successfully: \n",
      "2018-05-22 10:37:13epoch=85; Loss Pred=2.2475; Val Loss=2.5208; Val Acc=0.2173; Loss Att={'forw': '17.2512'}; Train Acc=0.279; Test Acc=0.1920; Entropy={'forw': '6.6000'}; Entropy_Test=\n",
      "\n",
      "2.6762989\n",
      "46 2.2312036 2.6762989\n",
      "Logged Successfully: \n",
      "2018-05-22 10:37:17epoch=86; Loss Pred=2.2310; Val Loss=2.6763; Val Acc=0.2351; Loss Att={'forw': '10.7726'}; Train Acc=0.294; Test Acc=0.1955; Entropy={'forw': '6.5376'}; Entropy_Test=\n",
      "\n",
      "2.6055112\n",
      "47 2.2312036 2.6055112\n",
      "Logged Successfully: \n",
      "2018-05-22 10:37:23epoch=87; Loss Pred=2.3380; Val Loss=2.6055; Val Acc=0.2679; Loss Att={'forw': '7.9181'}; Train Acc=0.285; Test Acc=0.2313; Entropy={'forw': '5.3064'}; Entropy_Test=\n",
      "\n",
      "2.7624795\n",
      "48 2.2312036 2.7624795\n",
      "Logged Successfully: \n",
      "2018-05-22 10:37:28epoch=88; Loss Pred=2.2583; Val Loss=2.7625; Val Acc=0.2411; Loss Att={'forw': '5.0846'}; Train Acc=0.278; Test Acc=0.2288; Entropy={'forw': '5.3564'}; Entropy_Test=\n",
      "\n",
      "2.5343225\n",
      "49 2.2312036 2.5343225\n",
      "Logged Successfully: \n",
      "2018-05-22 10:37:34epoch=89; Loss Pred=2.2253; Val Loss=2.5343; Val Acc=0.2173; Loss Att={'forw': '7.7329'}; Train Acc=0.306; Test Acc=0.2123; Entropy={'forw': '5.3493'}; Entropy_Test=\n",
      "\n",
      "2.4741976\n",
      "50 2.2312036 2.4741976\n",
      "Logged Successfully: \n",
      "STOPPED EARLY AT 91\n",
      "Optimization Finished!\n",
      "********** replication  2  **********\n",
      "video_classification\n",
      "0.0\n",
      "(499, 40, 2048) (499, 1) (99, 40, 2048) (99, 1)\n",
      "Logged Successfully: \n",
      "\n",
      "    model_type: \t\tTANH bidir(False), task: video_classification\n",
      "    hid: \t\t\t100,\n",
      "    h_hid: \t\t\t200\n",
      "    n_attractor_iterations: \t15,\n",
      "    attractor_dynamics: \tprojection2\n",
      "    attractor_noise_level: \t0.5\n",
      "    attractor_noise_type: \tbernoilli\n",
      "    attractor_regu-n: \t\tl2_regularization(lambda:0.0)\n",
      "    word_embedding: size\t(100), train(False)\n",
      "    dropout: \t\t\t0.4\n",
      "    TRAIN/TEST_SIZE: \t499/0, SEQ_LEN: 40\n",
      "Logged Successfully: \n",
      "3.2343757\n",
      "0 10000000000.0 3.2343757\n",
      "Logged Successfully: \n",
      "2018-05-22 10:39:33epoch=0; Loss Pred=3.2280; Val Loss=3.2344; Val Acc=0.0357; Loss Att={'forw': '1.0757'}; Train Acc=0.027; Test Acc=0.0324; Entropy={'forw': '8.8985'}; Entropy_Test=\n",
      "\n",
      "3.4315162\n",
      "0 3.2343757 3.4315162\n",
      "Logged Successfully: \n",
      "2018-05-22 10:39:39epoch=1; Loss Pred=3.3134; Val Loss=3.4315; Val Acc=0.0833; Loss Att={'forw': '7.9785'}; Train Acc=0.089; Test Acc=0.0638; Entropy={'forw': '4.2736'}; Entropy_Test=\n",
      "\n",
      "3.5525165\n",
      "1 3.2343757 3.5525165\n",
      "Logged Successfully: \n",
      "2018-05-22 10:39:45epoch=2; Loss Pred=3.3760; Val Loss=3.5525; Val Acc=0.0714; Loss Att={'forw': '1.2364'}; Train Acc=0.084; Test Acc=0.0683; Entropy={'forw': '4.4392'}; Entropy_Test=\n",
      "\n",
      "3.2097008\n",
      "2 3.2343757 3.2097008\n",
      "Logged Successfully: \n",
      "2018-05-22 10:39:51epoch=3; Loss Pred=3.1457; Val Loss=3.2097; Val Acc=0.0893; Loss Att={'forw': '12.8848'}; Train Acc=0.084; Test Acc=0.0647; Entropy={'forw': '5.6126'}; Entropy_Test=\n",
      "\n",
      "3.2399437\n",
      "0 3.2097008 3.2399437\n",
      "Logged Successfully: \n",
      "2018-05-22 10:39:56epoch=4; Loss Pred=3.1406; Val Loss=3.2399; Val Acc=0.0357; Loss Att={'forw': '11.9166'}; Train Acc=0.100; Test Acc=0.0759; Entropy={'forw': '5.6780'}; Entropy_Test=\n",
      "\n",
      "3.187513\n",
      "1 3.2097008 3.187513\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:02epoch=5; Loss Pred=3.0242; Val Loss=3.1875; Val Acc=0.0982; Loss Att={'forw': '8.1469'}; Train Acc=0.098; Test Acc=0.0761; Entropy={'forw': '5.9196'}; Entropy_Test=\n",
      "\n",
      "3.0658777\n",
      "0 3.187513 3.0658777\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:09epoch=6; Loss Pred=3.1230; Val Loss=3.0659; Val Acc=0.1548; Loss Att={'forw': '1.4307'}; Train Acc=0.092; Test Acc=0.0848; Entropy={'forw': '5.7688'}; Entropy_Test=\n",
      "\n",
      "3.402385\n",
      "0 3.0658777 3.402385\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:16epoch=7; Loss Pred=3.1491; Val Loss=3.4024; Val Acc=0.0446; Loss Att={'forw': '2.1955'}; Train Acc=0.098; Test Acc=0.0971; Entropy={'forw': '4.6280'}; Entropy_Test=\n",
      "\n",
      "3.4062426\n",
      "1 3.0658777 3.4062426\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:20epoch=8; Loss Pred=3.1905; Val Loss=3.4062; Val Acc=0.0446; Loss Att={'forw': '1.5177'}; Train Acc=0.100; Test Acc=0.0772; Entropy={'forw': '4.6726'}; Entropy_Test=\n",
      "\n",
      "3.445023\n",
      "2 3.0658777 3.445023\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:26epoch=9; Loss Pred=3.2216; Val Loss=3.4450; Val Acc=0.0357; Loss Att={'forw': '3.7364'}; Train Acc=0.090; Test Acc=0.0859; Entropy={'forw': '3.2768'}; Entropy_Test=\n",
      "\n",
      "3.417601\n",
      "3 3.0658777 3.417601\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:30epoch=10; Loss Pred=3.2270; Val Loss=3.4176; Val Acc=0.0357; Loss Att={'forw': '2.3166'}; Train Acc=0.100; Test Acc=0.0792; Entropy={'forw': '3.2641'}; Entropy_Test=\n",
      "\n",
      "3.0161\n",
      "4 3.0658777 3.0161\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:36epoch=11; Loss Pred=2.9225; Val Loss=3.0161; Val Acc=0.1280; Loss Att={'forw': '5.8800'}; Train Acc=0.084; Test Acc=0.0859; Entropy={'forw': '4.1131'}; Entropy_Test=\n",
      "\n",
      "3.0362878\n",
      "0 3.0161 3.0362878\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:41epoch=12; Loss Pred=2.8985; Val Loss=3.0363; Val Acc=0.0893; Loss Att={'forw': '2.1243'}; Train Acc=0.107; Test Acc=0.0971; Entropy={'forw': '4.1445'}; Entropy_Test=\n",
      "\n",
      "2.9096582\n",
      "1 3.0161 2.9096582\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:47epoch=13; Loss Pred=2.8318; Val Loss=2.9097; Val Acc=0.0893; Loss Att={'forw': '3.8069'}; Train Acc=0.129; Test Acc=0.1228; Entropy={'forw': '4.6111'}; Entropy_Test=\n",
      "\n",
      "2.869043\n",
      "0 2.9096582 2.869043\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:51epoch=14; Loss Pred=2.8256; Val Loss=2.8690; Val Acc=0.0982; Loss Att={'forw': '1.7513'}; Train Acc=0.148; Test Acc=0.1174; Entropy={'forw': '4.6525'}; Entropy_Test=\n",
      "\n",
      "2.9242\n",
      "0 2.869043 2.9242\n",
      "Logged Successfully: \n",
      "2018-05-22 10:40:57epoch=15; Loss Pred=2.8493; Val Loss=2.9242; Val Acc=0.1280; Loss Att={'forw': '4.8184'}; Train Acc=0.143; Test Acc=0.1105; Entropy={'forw': '4.5787'}; Entropy_Test=\n",
      "\n",
      "2.8968484\n",
      "1 2.869043 2.8968484\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:02epoch=16; Loss Pred=2.8427; Val Loss=2.8968; Val Acc=0.0804; Loss Att={'forw': '2.0914'}; Train Acc=0.135; Test Acc=0.1150; Entropy={'forw': '4.5414'}; Entropy_Test=\n",
      "\n",
      "3.2816107\n",
      "2 2.869043 3.2816107\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:08epoch=17; Loss Pred=3.2617; Val Loss=3.2816; Val Acc=0.0833; Loss Att={'forw': '27.3379'}; Train Acc=0.124; Test Acc=0.0759; Entropy={'forw': '5.4661'}; Entropy_Test=\n",
      "\n",
      "3.2094846\n",
      "3 2.869043 3.2094846\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:12epoch=18; Loss Pred=3.2199; Val Loss=3.2095; Val Acc=0.0536; Loss Att={'forw': '4.8944'}; Train Acc=0.076; Test Acc=0.0837; Entropy={'forw': '5.6317'}; Entropy_Test=\n",
      "\n",
      "2.8845265\n",
      "4 2.869043 2.8845265\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:18epoch=19; Loss Pred=2.9205; Val Loss=2.8845; Val Acc=0.0714; Loss Att={'forw': '12.0009'}; Train Acc=0.109; Test Acc=0.1049; Entropy={'forw': '4.1675'}; Entropy_Test=\n",
      "\n",
      "2.8136086\n",
      "5 2.869043 2.8136086\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:23epoch=20; Loss Pred=2.9760; Val Loss=2.8136; Val Acc=0.1458; Loss Att={'forw': '3.9380'}; Train Acc=0.141; Test Acc=0.0917; Entropy={'forw': '4.1598'}; Entropy_Test=\n",
      "\n",
      "3.2339952\n",
      "0 2.8136086 3.2339952\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:29epoch=21; Loss Pred=2.9428; Val Loss=3.2340; Val Acc=0.1696; Loss Att={'forw': '5.4373'}; Train Acc=0.144; Test Acc=0.1388; Entropy={'forw': '5.2533'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0418599\n",
      "1 2.8136086 3.0418599\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:33epoch=22; Loss Pred=2.9366; Val Loss=3.0419; Val Acc=0.0982; Loss Att={'forw': '3.5877'}; Train Acc=0.148; Test Acc=0.1350; Entropy={'forw': '5.2661'}; Entropy_Test=\n",
      "\n",
      "3.2830303\n",
      "2 2.8136086 3.2830303\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:39epoch=23; Loss Pred=2.9748; Val Loss=3.2830; Val Acc=0.0893; Loss Att={'forw': '12.2897'}; Train Acc=0.143; Test Acc=0.1129; Entropy={'forw': '4.7103'}; Entropy_Test=\n",
      "\n",
      "3.244267\n",
      "3 2.8136086 3.244267\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:43epoch=24; Loss Pred=2.9630; Val Loss=3.2443; Val Acc=0.1458; Loss Att={'forw': '6.2594'}; Train Acc=0.160; Test Acc=0.1199; Entropy={'forw': '4.2923'}; Entropy_Test=\n",
      "\n",
      "2.8062603\n",
      "4 2.8136086 2.8062603\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:49epoch=25; Loss Pred=2.6401; Val Loss=2.8063; Val Acc=0.1696; Loss Att={'forw': '8.4913'}; Train Acc=0.180; Test Acc=0.1518; Entropy={'forw': '4.0309'}; Entropy_Test=\n",
      "\n",
      "2.6207623\n",
      "0 2.8062603 2.6207623\n",
      "Logged Successfully: \n",
      "2018-05-22 10:41:54epoch=26; Loss Pred=2.7323; Val Loss=2.6208; Val Acc=0.2232; Loss Att={'forw': '3.3342'}; Train Acc=0.168; Test Acc=0.1531; Entropy={'forw': '4.0199'}; Entropy_Test=\n",
      "\n",
      "2.948488\n",
      "0 2.6207623 2.948488\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:00epoch=27; Loss Pred=2.7683; Val Loss=2.9485; Val Acc=0.1161; Loss Att={'forw': '6.2054'}; Train Acc=0.145; Test Acc=0.1344; Entropy={'forw': '4.0048'}; Entropy_Test=\n",
      "\n",
      "2.7097886\n",
      "1 2.6207623 2.7097886\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:05epoch=28; Loss Pred=2.7132; Val Loss=2.7098; Val Acc=0.1548; Loss Att={'forw': '3.5717'}; Train Acc=0.159; Test Acc=0.1362; Entropy={'forw': '4.0076'}; Entropy_Test=\n",
      "\n",
      "2.946875\n",
      "2 2.6207623 2.946875\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:11epoch=29; Loss Pred=2.7024; Val Loss=2.9469; Val Acc=0.0982; Loss Att={'forw': '6.7008'}; Train Acc=0.205; Test Acc=0.1484; Entropy={'forw': '4.2528'}; Entropy_Test=\n",
      "\n",
      "2.9868467\n",
      "3 2.6207623 2.9868467\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:15epoch=30; Loss Pred=2.7053; Val Loss=2.9868; Val Acc=0.1280; Loss Att={'forw': '3.6242'}; Train Acc=0.165; Test Acc=0.1475; Entropy={'forw': '4.3021'}; Entropy_Test=\n",
      "\n",
      "3.0281787\n",
      "4 2.6207623 3.0281787\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:21epoch=31; Loss Pred=2.8207; Val Loss=3.0282; Val Acc=0.1250; Loss Att={'forw': '7.5082'}; Train Acc=0.168; Test Acc=0.1576; Entropy={'forw': '3.8391'}; Entropy_Test=\n",
      "\n",
      "2.9546869\n",
      "5 2.6207623 2.9546869\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:26epoch=32; Loss Pred=2.7036; Val Loss=2.9547; Val Acc=0.1161; Loss Att={'forw': '4.3954'}; Train Acc=0.194; Test Acc=0.1429; Entropy={'forw': '3.7991'}; Entropy_Test=\n",
      "\n",
      "3.0367393\n",
      "6 2.6207623 3.0367393\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:32epoch=33; Loss Pred=2.5624; Val Loss=3.0367; Val Acc=0.1429; Loss Att={'forw': '11.0382'}; Train Acc=0.217; Test Acc=0.1609; Entropy={'forw': '4.5494'}; Entropy_Test=\n",
      "\n",
      "2.8100834\n",
      "7 2.6207623 2.8100834\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:36epoch=34; Loss Pred=2.6359; Val Loss=2.8101; Val Acc=0.1548; Loss Att={'forw': '3.7401'}; Train Acc=0.188; Test Acc=0.1721; Entropy={'forw': '4.4900'}; Entropy_Test=\n",
      "\n",
      "2.6977437\n",
      "8 2.6207623 2.6977437\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:42epoch=35; Loss Pred=2.5351; Val Loss=2.6977; Val Acc=0.0893; Loss Att={'forw': '7.5663'}; Train Acc=0.223; Test Acc=0.1667; Entropy={'forw': '4.3263'}; Entropy_Test=\n",
      "\n",
      "2.6661217\n",
      "9 2.6207623 2.6661217\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:47epoch=36; Loss Pred=2.5099; Val Loss=2.6661; Val Acc=0.1696; Loss Att={'forw': '3.2351'}; Train Acc=0.205; Test Acc=0.1574; Entropy={'forw': '4.4655'}; Entropy_Test=\n",
      "\n",
      "2.7968805\n",
      "10 2.6207623 2.7968805\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:53epoch=37; Loss Pred=2.7053; Val Loss=2.7969; Val Acc=0.1518; Loss Att={'forw': '14.3181'}; Train Acc=0.158; Test Acc=0.1163; Entropy={'forw': '4.5667'}; Entropy_Test=\n",
      "\n",
      "2.9242494\n",
      "11 2.6207623 2.9242494\n",
      "Logged Successfully: \n",
      "2018-05-22 10:42:57epoch=38; Loss Pred=2.6796; Val Loss=2.9242; Val Acc=0.1161; Loss Att={'forw': '8.4000'}; Train Acc=0.167; Test Acc=0.1429; Entropy={'forw': '4.5724'}; Entropy_Test=\n",
      "\n",
      "2.851974\n",
      "12 2.6207623 2.851974\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:03epoch=39; Loss Pred=2.6691; Val Loss=2.8520; Val Acc=0.1429; Loss Att={'forw': '4.7772'}; Train Acc=0.170; Test Acc=0.1775; Entropy={'forw': '3.7245'}; Entropy_Test=\n",
      "\n",
      "2.9207566\n",
      "13 2.6207623 2.9207566\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:08epoch=40; Loss Pred=2.6458; Val Loss=2.9208; Val Acc=0.1518; Loss Att={'forw': '3.2998'}; Train Acc=0.184; Test Acc=0.1808; Entropy={'forw': '3.7575'}; Entropy_Test=\n",
      "\n",
      "2.8024848\n",
      "14 2.6207623 2.8024848\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:14epoch=41; Loss Pred=2.6366; Val Loss=2.8025; Val Acc=0.1071; Loss Att={'forw': '11.4035'}; Train Acc=0.180; Test Acc=0.1507; Entropy={'forw': '4.0509'}; Entropy_Test=\n",
      "\n",
      "2.8486667\n",
      "15 2.6207623 2.8486667\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:18epoch=42; Loss Pred=2.5810; Val Loss=2.8487; Val Acc=0.1875; Loss Att={'forw': '3.8150'}; Train Acc=0.191; Test Acc=0.1533; Entropy={'forw': '4.2878'}; Entropy_Test=\n",
      "\n",
      "2.7075255\n",
      "16 2.6207623 2.7075255\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:24epoch=43; Loss Pred=2.4709; Val Loss=2.7075; Val Acc=0.1518; Loss Att={'forw': '12.5469'}; Train Acc=0.193; Test Acc=0.1685; Entropy={'forw': '4.0570'}; Entropy_Test=\n",
      "\n",
      "2.6779544\n",
      "17 2.6207623 2.6779544\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:29epoch=44; Loss Pred=2.4438; Val Loss=2.6780; Val Acc=0.1518; Loss Att={'forw': '5.0338'}; Train Acc=0.237; Test Acc=0.1743; Entropy={'forw': '4.0543'}; Entropy_Test=\n",
      "\n",
      "2.8250823\n",
      "18 2.6207623 2.8250823\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:35epoch=45; Loss Pred=2.7842; Val Loss=2.8251; Val Acc=0.1815; Loss Att={'forw': '12.6810'}; Train Acc=0.146; Test Acc=0.1373; Entropy={'forw': '4.4456'}; Entropy_Test=\n",
      "\n",
      "3.0079386\n",
      "19 2.6207623 3.0079386\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:39epoch=46; Loss Pred=2.6795; Val Loss=3.0079; Val Acc=0.1458; Loss Att={'forw': '4.1620'}; Train Acc=0.163; Test Acc=0.1386; Entropy={'forw': '4.2851'}; Entropy_Test=\n",
      "\n",
      "2.968445\n",
      "20 2.6207623 2.968445\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:45epoch=47; Loss Pred=2.7822; Val Loss=2.9684; Val Acc=0.1250; Loss Att={'forw': '14.8641'}; Train Acc=0.146; Test Acc=0.1395; Entropy={'forw': '4.8811'}; Entropy_Test=\n",
      "\n",
      "2.855329\n",
      "21 2.6207623 2.855329\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:49epoch=48; Loss Pred=2.6898; Val Loss=2.8553; Val Acc=0.1518; Loss Att={'forw': '10.5273'}; Train Acc=0.177; Test Acc=0.1373; Entropy={'forw': '4.7517'}; Entropy_Test=\n",
      "\n",
      "2.676621\n",
      "22 2.6207623 2.676621\n",
      "Logged Successfully: \n",
      "2018-05-22 10:43:55epoch=49; Loss Pred=2.4882; Val Loss=2.6766; Val Acc=0.1161; Loss Att={'forw': '5.8756'}; Train Acc=0.217; Test Acc=0.1609; Entropy={'forw': '4.6458'}; Entropy_Test=\n",
      "\n",
      "2.6896245\n",
      "23 2.6207623 2.6896245\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:00epoch=50; Loss Pred=2.5038; Val Loss=2.6896; Val Acc=0.1696; Loss Att={'forw': '4.9554'}; Train Acc=0.194; Test Acc=0.1574; Entropy={'forw': '4.6294'}; Entropy_Test=\n",
      "\n",
      "2.6712368\n",
      "24 2.6207623 2.6712368\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:06epoch=51; Loss Pred=2.7462; Val Loss=2.6712; Val Acc=0.0804; Loss Att={'forw': '13.1233'}; Train Acc=0.156; Test Acc=0.1453; Entropy={'forw': '4.4991'}; Entropy_Test=\n",
      "\n",
      "2.758078\n",
      "25 2.6207623 2.758078\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:10epoch=52; Loss Pred=2.6723; Val Loss=2.7581; Val Acc=0.1518; Loss Att={'forw': '4.8489'}; Train Acc=0.172; Test Acc=0.1676; Entropy={'forw': '4.5504'}; Entropy_Test=\n",
      "\n",
      "2.9860063\n",
      "26 2.6207623 2.9860063\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:16epoch=53; Loss Pred=2.7056; Val Loss=2.9860; Val Acc=0.0893; Loss Att={'forw': '26.7388'}; Train Acc=0.143; Test Acc=0.1484; Entropy={'forw': '3.9191'}; Entropy_Test=\n",
      "\n",
      "2.946835\n",
      "27 2.6207623 2.946835\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:21epoch=54; Loss Pred=2.6427; Val Loss=2.9468; Val Acc=0.1518; Loss Att={'forw': '23.7037'}; Train Acc=0.200; Test Acc=0.1690; Entropy={'forw': '3.9091'}; Entropy_Test=\n",
      "\n",
      "3.0470126\n",
      "28 2.6207623 3.0470126\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:27epoch=55; Loss Pred=2.5399; Val Loss=3.0470; Val Acc=0.0893; Loss Att={'forw': '17.0095'}; Train Acc=0.193; Test Acc=0.1652; Entropy={'forw': '4.3160'}; Entropy_Test=\n",
      "\n",
      "2.8007832\n",
      "29 2.6207623 2.8007832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Successfully: \n",
      "2018-05-22 10:44:31epoch=56; Loss Pred=2.5205; Val Loss=2.8008; Val Acc=0.0893; Loss Att={'forw': '8.0776'}; Train Acc=0.176; Test Acc=0.1712; Entropy={'forw': '4.1776'}; Entropy_Test=\n",
      "\n",
      "2.6295476\n",
      "30 2.6207623 2.6295476\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:37epoch=57; Loss Pred=2.6309; Val Loss=2.6295; Val Acc=0.1905; Loss Att={'forw': '24.2249'}; Train Acc=0.164; Test Acc=0.1857; Entropy={'forw': '3.8183'}; Entropy_Test=\n",
      "\n",
      "2.7891371\n",
      "31 2.6207623 2.7891371\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:42epoch=58; Loss Pred=2.5367; Val Loss=2.7891; Val Acc=0.2083; Loss Att={'forw': '5.7624'}; Train Acc=0.196; Test Acc=0.1797; Entropy={'forw': '3.9852'}; Entropy_Test=\n",
      "\n",
      "2.7526934\n",
      "32 2.6207623 2.7526934\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:48epoch=59; Loss Pred=2.3754; Val Loss=2.7527; Val Acc=0.2440; Loss Att={'forw': '6.6187'}; Train Acc=0.231; Test Acc=0.1853; Entropy={'forw': '4.3026'}; Entropy_Test=\n",
      "\n",
      "2.977359\n",
      "33 2.6207623 2.977359\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:52epoch=60; Loss Pred=2.4257; Val Loss=2.9774; Val Acc=0.1518; Loss Att={'forw': '5.8596'}; Train Acc=0.221; Test Acc=0.1908; Entropy={'forw': '4.2827'}; Entropy_Test=\n",
      "\n",
      "2.7305286\n",
      "34 2.6207623 2.7305286\n",
      "Logged Successfully: \n",
      "2018-05-22 10:44:58epoch=61; Loss Pred=2.6588; Val Loss=2.7305; Val Acc=0.1429; Loss Att={'forw': '15.0537'}; Train Acc=0.193; Test Acc=0.1598; Entropy={'forw': '4.1883'}; Entropy_Test=\n",
      "\n",
      "2.806802\n",
      "35 2.6207623 2.806802\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:03epoch=62; Loss Pred=2.6176; Val Loss=2.8068; Val Acc=0.1905; Loss Att={'forw': '8.9032'}; Train Acc=0.186; Test Acc=0.1730; Entropy={'forw': '4.2639'}; Entropy_Test=\n",
      "\n",
      "2.5798473\n",
      "36 2.6207623 2.5798473\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:09epoch=63; Loss Pred=2.4784; Val Loss=2.5798; Val Acc=0.1726; Loss Att={'forw': '27.2906'}; Train Acc=0.214; Test Acc=0.1585; Entropy={'forw': '4.9098'}; Entropy_Test=\n",
      "\n",
      "2.3386672\n",
      "0 2.5798473 2.3386672\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:13epoch=64; Loss Pred=2.5095; Val Loss=2.3387; Val Acc=0.2083; Loss Att={'forw': '8.2085'}; Train Acc=0.188; Test Acc=0.1674; Entropy={'forw': '5.0712'}; Entropy_Test=\n",
      "\n",
      "2.7206473\n",
      "0 2.3386672 2.7206473\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:19epoch=65; Loss Pred=2.3512; Val Loss=2.7206; Val Acc=0.1161; Loss Att={'forw': '5.8766'}; Train Acc=0.203; Test Acc=0.1853; Entropy={'forw': '4.7801'}; Entropy_Test=\n",
      "\n",
      "2.719338\n",
      "1 2.3386672 2.719338\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:23epoch=66; Loss Pred=2.2629; Val Loss=2.7193; Val Acc=0.1607; Loss Att={'forw': '4.7943'}; Train Acc=0.226; Test Acc=0.1902; Entropy={'forw': '4.7573'}; Entropy_Test=\n",
      "\n",
      "2.6646266\n",
      "2 2.3386672 2.6646266\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:30epoch=67; Loss Pred=2.5114; Val Loss=2.6646; Val Acc=0.2173; Loss Att={'forw': '9.4190'}; Train Acc=0.193; Test Acc=0.1877; Entropy={'forw': '4.8613'}; Entropy_Test=\n",
      "\n",
      "2.6130805\n",
      "3 2.3386672 2.6130805\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:34epoch=68; Loss Pred=2.3357; Val Loss=2.6131; Val Acc=0.2083; Loss Att={'forw': '6.6200'}; Train Acc=0.211; Test Acc=0.1908; Entropy={'forw': '4.8922'}; Entropy_Test=\n",
      "\n",
      "3.2266502\n",
      "4 2.3386672 3.2266502\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:40epoch=69; Loss Pred=2.6191; Val Loss=3.2267; Val Acc=0.1607; Loss Att={'forw': '14.3104'}; Train Acc=0.211; Test Acc=0.1821; Entropy={'forw': '3.6641'}; Entropy_Test=\n",
      "\n",
      "2.8670115\n",
      "5 2.3386672 2.8670115\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:44epoch=70; Loss Pred=2.5095; Val Loss=2.8670; Val Acc=0.0893; Loss Att={'forw': '5.0663'}; Train Acc=0.203; Test Acc=0.1565; Entropy={'forw': '3.7051'}; Entropy_Test=\n",
      "\n",
      "2.535973\n",
      "6 2.3386672 2.535973\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:51epoch=71; Loss Pred=2.2827; Val Loss=2.5360; Val Acc=0.2083; Loss Att={'forw': '8.1565'}; Train Acc=0.243; Test Acc=0.1877; Entropy={'forw': '3.5450'}; Entropy_Test=\n",
      "\n",
      "2.6741884\n",
      "7 2.3386672 2.6741884\n",
      "Logged Successfully: \n",
      "2018-05-22 10:45:55epoch=72; Loss Pred=2.3582; Val Loss=2.6742; Val Acc=0.1339; Loss Att={'forw': '5.3782'}; Train Acc=0.207; Test Acc=0.1788; Entropy={'forw': '3.5077'}; Entropy_Test=\n",
      "\n",
      "2.5860221\n",
      "8 2.3386672 2.5860221\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:01epoch=73; Loss Pred=2.4238; Val Loss=2.5860; Val Acc=0.2619; Loss Att={'forw': '21.4641'}; Train Acc=0.212; Test Acc=0.1652; Entropy={'forw': '3.8803'}; Entropy_Test=\n",
      "\n",
      "2.4510953\n",
      "9 2.3386672 2.4510953\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:06epoch=74; Loss Pred=2.4775; Val Loss=2.4511; Val Acc=0.1994; Loss Att={'forw': '16.1339'}; Train Acc=0.211; Test Acc=0.1621; Entropy={'forw': '3.8631'}; Entropy_Test=\n",
      "\n",
      "2.6330874\n",
      "10 2.3386672 2.6330874\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:12epoch=75; Loss Pred=2.3346; Val Loss=2.6331; Val Acc=0.1429; Loss Att={'forw': '12.9951'}; Train Acc=0.212; Test Acc=0.1752; Entropy={'forw': '3.7313'}; Entropy_Test=\n",
      "\n",
      "2.6294267\n",
      "11 2.3386672 2.6294267\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:16epoch=76; Loss Pred=2.2781; Val Loss=2.6294; Val Acc=0.1994; Loss Att={'forw': '14.6949'}; Train Acc=0.253; Test Acc=0.2011; Entropy={'forw': '3.6951'}; Entropy_Test=\n",
      "\n",
      "2.8189023\n",
      "12 2.3386672 2.8189023\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:22epoch=77; Loss Pred=2.4692; Val Loss=2.8189; Val Acc=0.2054; Loss Att={'forw': '9.9094'}; Train Acc=0.237; Test Acc=0.2020; Entropy={'forw': '3.9975'}; Entropy_Test=\n",
      "\n",
      "2.8797855\n",
      "13 2.3386672 2.8797855\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:27epoch=78; Loss Pred=2.4282; Val Loss=2.8798; Val Acc=0.1786; Loss Att={'forw': '7.0459'}; Train Acc=0.237; Test Acc=0.2022; Entropy={'forw': '3.9971'}; Entropy_Test=\n",
      "\n",
      "2.8434198\n",
      "14 2.3386672 2.8434198\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:33epoch=79; Loss Pred=2.6290; Val Loss=2.8434; Val Acc=0.1250; Loss Att={'forw': '41.5466'}; Train Acc=0.188; Test Acc=0.1730; Entropy={'forw': '3.3791'}; Entropy_Test=\n",
      "\n",
      "2.8229403\n",
      "15 2.3386672 2.8229403\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:37epoch=80; Loss Pred=2.5435; Val Loss=2.8229; Val Acc=0.1339; Loss Att={'forw': '32.4934'}; Train Acc=0.215; Test Acc=0.1824; Entropy={'forw': '3.2327'}; Entropy_Test=\n",
      "\n",
      "2.9404945\n",
      "16 2.3386672 2.9404945\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:43epoch=81; Loss Pred=2.4851; Val Loss=2.9405; Val Acc=0.1696; Loss Att={'forw': '24.2160'}; Train Acc=0.212; Test Acc=0.2047; Entropy={'forw': '3.7330'}; Entropy_Test=\n",
      "\n",
      "2.8026364\n",
      "17 2.3386672 2.8026364\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:48epoch=82; Loss Pred=2.4560; Val Loss=2.8026; Val Acc=0.1875; Loss Att={'forw': '17.1801'}; Train Acc=0.238; Test Acc=0.1877; Entropy={'forw': '3.7958'}; Entropy_Test=\n",
      "\n",
      "2.6377296\n",
      "18 2.3386672 2.6377296\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:54epoch=83; Loss Pred=2.3970; Val Loss=2.6377; Val Acc=0.1548; Loss Att={'forw': '8.3746'}; Train Acc=0.205; Test Acc=0.2143; Entropy={'forw': '3.6807'}; Entropy_Test=\n",
      "\n",
      "2.5136964\n",
      "19 2.3386672 2.5136964\n",
      "Logged Successfully: \n",
      "2018-05-22 10:46:58epoch=84; Loss Pred=2.3669; Val Loss=2.5137; Val Acc=0.2054; Loss Att={'forw': '5.1195'}; Train Acc=0.195; Test Acc=0.2179; Entropy={'forw': '3.7101'}; Entropy_Test=\n",
      "\n",
      "2.4437153\n",
      "20 2.3386672 2.4437153\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:04epoch=85; Loss Pred=2.3297; Val Loss=2.4437; Val Acc=0.1875; Loss Att={'forw': '10.2950'}; Train Acc=0.195; Test Acc=0.1931; Entropy={'forw': '3.9577'}; Entropy_Test=\n",
      "\n",
      "2.4372249\n",
      "21 2.3386672 2.4372249\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:09epoch=86; Loss Pred=2.2952; Val Loss=2.4372; Val Acc=0.2649; Loss Att={'forw': '5.8299'}; Train Acc=0.231; Test Acc=0.1710; Entropy={'forw': '3.9989'}; Entropy_Test=\n",
      "\n",
      "3.025808\n",
      "22 2.3386672 3.025808\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:15epoch=87; Loss Pred=2.7178; Val Loss=3.0258; Val Acc=0.1071; Loss Att={'forw': '27.8317'}; Train Acc=0.190; Test Acc=0.1362; Entropy={'forw': '5.0856'}; Entropy_Test=\n",
      "\n",
      "2.555244\n",
      "23 2.3386672 2.555244\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:19epoch=88; Loss Pred=2.6921; Val Loss=2.5552; Val Acc=0.1726; Loss Att={'forw': '15.8619'}; Train Acc=0.208; Test Acc=0.1317; Entropy={'forw': '5.0562'}; Entropy_Test=\n",
      "\n",
      "2.360212\n",
      "24 2.3386672 2.360212\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:25epoch=89; Loss Pred=2.3302; Val Loss=2.3602; Val Acc=0.2024; Loss Att={'forw': '27.0642'}; Train Acc=0.243; Test Acc=0.1688; Entropy={'forw': '3.9220'}; Entropy_Test=\n",
      "\n",
      "2.8135552\n",
      "25 2.3386672 2.8135552\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:30epoch=90; Loss Pred=2.4305; Val Loss=2.8136; Val Acc=0.1161; Loss Att={'forw': '14.3633'}; Train Acc=0.205; Test Acc=0.2179; Entropy={'forw': '3.9493'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6411662\n",
      "26 2.3386672 2.6411662\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:36epoch=91; Loss Pred=2.2913; Val Loss=2.6412; Val Acc=0.1696; Loss Att={'forw': '11.6305'}; Train Acc=0.242; Test Acc=0.1819; Entropy={'forw': '3.9877'}; Entropy_Test=\n",
      "\n",
      "2.5056176\n",
      "27 2.3386672 2.5056176\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:40epoch=92; Loss Pred=2.2351; Val Loss=2.5056; Val Acc=0.1905; Loss Att={'forw': '5.6221'}; Train Acc=0.277; Test Acc=0.1868; Entropy={'forw': '3.9754'}; Entropy_Test=\n",
      "\n",
      "2.878205\n",
      "28 2.3386672 2.878205\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:46epoch=93; Loss Pred=2.3584; Val Loss=2.8782; Val Acc=0.1815; Loss Att={'forw': '24.6073'}; Train Acc=0.191; Test Acc=0.1607; Entropy={'forw': '3.6215'}; Entropy_Test=\n",
      "\n",
      "2.914959\n",
      "29 2.3386672 2.914959\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:51epoch=94; Loss Pred=2.3665; Val Loss=2.9150; Val Acc=0.1161; Loss Att={'forw': '9.7746'}; Train Acc=0.212; Test Acc=0.1688; Entropy={'forw': '3.5609'}; Entropy_Test=\n",
      "\n",
      "2.7673\n",
      "30 2.3386672 2.7673\n",
      "Logged Successfully: \n",
      "2018-05-22 10:47:57epoch=95; Loss Pred=2.4642; Val Loss=2.7673; Val Acc=0.1548; Loss Att={'forw': '8.8470'}; Train Acc=0.174; Test Acc=0.1888; Entropy={'forw': '4.0466'}; Entropy_Test=\n",
      "\n",
      "2.8811288\n",
      "31 2.3386672 2.8811288\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:01epoch=96; Loss Pred=2.4369; Val Loss=2.8811; Val Acc=0.0714; Loss Att={'forw': '6.0565'}; Train Acc=0.168; Test Acc=0.1842; Entropy={'forw': '4.0517'}; Entropy_Test=\n",
      "\n",
      "2.7956681\n",
      "32 2.3386672 2.7956681\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:07epoch=97; Loss Pred=2.3477; Val Loss=2.7957; Val Acc=0.1280; Loss Att={'forw': '16.7526'}; Train Acc=0.195; Test Acc=0.1837; Entropy={'forw': '3.4115'}; Entropy_Test=\n",
      "\n",
      "2.7501714\n",
      "33 2.3386672 2.7501714\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:12epoch=98; Loss Pred=2.4354; Val Loss=2.7502; Val Acc=0.1161; Loss Att={'forw': '8.5289'}; Train Acc=0.194; Test Acc=0.1897; Entropy={'forw': '3.5745'}; Entropy_Test=\n",
      "\n",
      "2.5926938\n",
      "34 2.3386672 2.5926938\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:18epoch=99; Loss Pred=2.3871; Val Loss=2.5927; Val Acc=0.1250; Loss Att={'forw': '12.5428'}; Train Acc=0.215; Test Acc=0.2033; Entropy={'forw': '4.0115'}; Entropy_Test=\n",
      "\n",
      "2.7784536\n",
      "35 2.3386672 2.7784536\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:22epoch=100; Loss Pred=2.3496; Val Loss=2.7785; Val Acc=0.1280; Loss Att={'forw': '9.2877'}; Train Acc=0.201; Test Acc=0.1763; Entropy={'forw': '4.0934'}; Entropy_Test=\n",
      "\n",
      "2.6373641\n",
      "36 2.3386672 2.6373641\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:28epoch=101; Loss Pred=2.4310; Val Loss=2.6374; Val Acc=0.1548; Loss Att={'forw': '8.2055'}; Train Acc=0.219; Test Acc=0.1942; Entropy={'forw': '3.2265'}; Entropy_Test=\n",
      "\n",
      "2.6182408\n",
      "37 2.3386672 2.6182408\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:33epoch=102; Loss Pred=2.4812; Val Loss=2.6182; Val Acc=0.1726; Loss Att={'forw': '6.4802'}; Train Acc=0.184; Test Acc=0.1913; Entropy={'forw': '3.2114'}; Entropy_Test=\n",
      "\n",
      "2.720108\n",
      "38 2.3386672 2.720108\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:39epoch=103; Loss Pred=2.3548; Val Loss=2.7201; Val Acc=0.1250; Loss Att={'forw': '11.9213'}; Train Acc=0.207; Test Acc=0.1609; Entropy={'forw': '3.2373'}; Entropy_Test=\n",
      "\n",
      "2.5821407\n",
      "39 2.3386672 2.5821407\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:43epoch=104; Loss Pred=2.3129; Val Loss=2.5821; Val Acc=0.1518; Loss Att={'forw': '9.0829'}; Train Acc=0.239; Test Acc=0.2121; Entropy={'forw': '3.2434'}; Entropy_Test=\n",
      "\n",
      "2.7183907\n",
      "40 2.3386672 2.7183907\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:49epoch=105; Loss Pred=2.5713; Val Loss=2.7184; Val Acc=0.1607; Loss Att={'forw': '20.9533'}; Train Acc=0.201; Test Acc=0.1576; Entropy={'forw': '4.0824'}; Entropy_Test=\n",
      "\n",
      "2.7883756\n",
      "41 2.3386672 2.7883756\n",
      "Logged Successfully: \n",
      "2018-05-22 10:48:54epoch=106; Loss Pred=2.5233; Val Loss=2.7884; Val Acc=0.1161; Loss Att={'forw': '7.8014'}; Train Acc=0.223; Test Acc=0.1587; Entropy={'forw': '4.1014'}; Entropy_Test=\n",
      "\n",
      "2.8897424\n",
      "42 2.3386672 2.8897424\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:00epoch=107; Loss Pred=2.4404; Val Loss=2.8897; Val Acc=0.0804; Loss Att={'forw': '21.7298'}; Train Acc=0.214; Test Acc=0.1723; Entropy={'forw': '3.2306'}; Entropy_Test=\n",
      "\n",
      "2.8546958\n",
      "43 2.3386672 2.8546958\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:04epoch=108; Loss Pred=2.4001; Val Loss=2.8547; Val Acc=0.0982; Loss Att={'forw': '8.9584'}; Train Acc=0.205; Test Acc=0.1777; Entropy={'forw': '3.2172'}; Entropy_Test=\n",
      "\n",
      "2.2618122\n",
      "44 2.3386672 2.2618122\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:10epoch=109; Loss Pred=2.2686; Val Loss=2.2618; Val Acc=0.3095; Loss Att={'forw': '9.4434'}; Train Acc=0.237; Test Acc=0.1824; Entropy={'forw': '3.4894'}; Entropy_Test=\n",
      "\n",
      "2.3760035\n",
      "0 2.2618122 2.3760035\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:15epoch=110; Loss Pred=2.3426; Val Loss=2.3760; Val Acc=0.2887; Loss Att={'forw': '7.8925'}; Train Acc=0.227; Test Acc=0.1989; Entropy={'forw': '3.5016'}; Entropy_Test=\n",
      "\n",
      "2.5136852\n",
      "1 2.2618122 2.5136852\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:21epoch=111; Loss Pred=2.4140; Val Loss=2.5137; Val Acc=0.2054; Loss Att={'forw': '16.8616'}; Train Acc=0.219; Test Acc=0.1801; Entropy={'forw': '4.5938'}; Entropy_Test=\n",
      "\n",
      "2.3947294\n",
      "2 2.2618122 2.3947294\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:25epoch=112; Loss Pred=2.3873; Val Loss=2.3947; Val Acc=0.2917; Loss Att={'forw': '11.0479'}; Train Acc=0.191; Test Acc=0.1752; Entropy={'forw': '4.5282'}; Entropy_Test=\n",
      "\n",
      "2.5653636\n",
      "3 2.2618122 2.5653636\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:32epoch=113; Loss Pred=2.4182; Val Loss=2.5654; Val Acc=0.1429; Loss Att={'forw': '13.2763'}; Train Acc=0.189; Test Acc=0.2176; Entropy={'forw': '3.2952'}; Entropy_Test=\n",
      "\n",
      "2.538667\n",
      "4 2.2618122 2.538667\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:36epoch=114; Loss Pred=2.2782; Val Loss=2.5387; Val Acc=0.2083; Loss Att={'forw': '18.7431'}; Train Acc=0.213; Test Acc=0.1618; Entropy={'forw': '3.3924'}; Entropy_Test=\n",
      "\n",
      "2.8242073\n",
      "5 2.2618122 2.8242073\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:42epoch=115; Loss Pred=2.6677; Val Loss=2.8242; Val Acc=0.1607; Loss Att={'forw': '33.4431'}; Train Acc=0.192; Test Acc=0.1518; Entropy={'forw': '4.1441'}; Entropy_Test=\n",
      "\n",
      "3.0165448\n",
      "6 2.2618122 3.0165448\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:47epoch=116; Loss Pred=2.6520; Val Loss=3.0165; Val Acc=0.1339; Loss Att={'forw': '29.2452'}; Train Acc=0.174; Test Acc=0.1529; Entropy={'forw': '4.1318'}; Entropy_Test=\n",
      "\n",
      "2.811896\n",
      "7 2.2618122 2.811896\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:53epoch=117; Loss Pred=2.4901; Val Loss=2.8119; Val Acc=0.1429; Loss Att={'forw': '28.2277'}; Train Acc=0.192; Test Acc=0.1618; Entropy={'forw': '3.5256'}; Entropy_Test=\n",
      "\n",
      "2.7586205\n",
      "8 2.2618122 2.7586205\n",
      "Logged Successfully: \n",
      "2018-05-22 10:49:58epoch=118; Loss Pred=2.4458; Val Loss=2.7586; Val Acc=0.1607; Loss Att={'forw': '13.0884'}; Train Acc=0.210; Test Acc=0.1935; Entropy={'forw': '3.4733'}; Entropy_Test=\n",
      "\n",
      "3.0617688\n",
      "9 2.2618122 3.0617688\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:04epoch=119; Loss Pred=2.3592; Val Loss=3.0618; Val Acc=0.1607; Loss Att={'forw': '48.1447'}; Train Acc=0.211; Test Acc=0.1866; Entropy={'forw': '3.5980'}; Entropy_Test=\n",
      "\n",
      "2.5460217\n",
      "10 2.2618122 2.5460217\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:08epoch=120; Loss Pred=2.3838; Val Loss=2.5460; Val Acc=0.2024; Loss Att={'forw': '37.8207'}; Train Acc=0.205; Test Acc=0.1777; Entropy={'forw': '3.6239'}; Entropy_Test=\n",
      "\n",
      "2.611529\n",
      "11 2.2618122 2.611529\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:15epoch=121; Loss Pred=2.2874; Val Loss=2.6115; Val Acc=0.1875; Loss Att={'forw': '32.7035'}; Train Acc=0.211; Test Acc=0.1975; Entropy={'forw': '3.6178'}; Entropy_Test=\n",
      "\n",
      "2.4357083\n",
      "12 2.2618122 2.4357083\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:19epoch=122; Loss Pred=2.2375; Val Loss=2.4357; Val Acc=0.1786; Loss Att={'forw': '26.3776'}; Train Acc=0.264; Test Acc=0.2143; Entropy={'forw': '3.6180'}; Entropy_Test=\n",
      "\n",
      "2.8534987\n",
      "13 2.2618122 2.8534987\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:25epoch=123; Loss Pred=2.3731; Val Loss=2.8535; Val Acc=0.1548; Loss Att={'forw': '12.6406'}; Train Acc=0.212; Test Acc=0.1763; Entropy={'forw': '3.8005'}; Entropy_Test=\n",
      "\n",
      "3.0027587\n",
      "14 2.2618122 3.0027587\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:30epoch=124; Loss Pred=2.3261; Val Loss=3.0028; Val Acc=0.1250; Loss Att={'forw': '8.5560'}; Train Acc=0.209; Test Acc=0.2094; Entropy={'forw': '3.7904'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0162797\n",
      "15 2.2618122 3.0162797\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:36epoch=125; Loss Pred=2.7172; Val Loss=3.0163; Val Acc=0.0893; Loss Att={'forw': '14.6824'}; Train Acc=0.174; Test Acc=0.1406; Entropy={'forw': '4.1228'}; Entropy_Test=\n",
      "\n",
      "3.216184\n",
      "16 2.2618122 3.216184\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:41epoch=126; Loss Pred=2.6746; Val Loss=3.2162; Val Acc=0.1250; Loss Att={'forw': '10.2810'}; Train Acc=0.181; Test Acc=0.1551; Entropy={'forw': '4.1468'}; Entropy_Test=\n",
      "\n",
      "2.6030335\n",
      "17 2.2618122 2.6030335\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:47epoch=127; Loss Pred=2.3133; Val Loss=2.6030; Val Acc=0.2143; Loss Att={'forw': '7.9532'}; Train Acc=0.213; Test Acc=0.2067; Entropy={'forw': '3.8534'}; Entropy_Test=\n",
      "\n",
      "2.5190985\n",
      "18 2.2618122 2.5190985\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:51epoch=128; Loss Pred=2.2234; Val Loss=2.5191; Val Acc=0.2143; Loss Att={'forw': '9.3359'}; Train Acc=0.257; Test Acc=0.2022; Entropy={'forw': '3.9447'}; Entropy_Test=\n",
      "\n",
      "2.7952101\n",
      "19 2.2618122 2.7952101\n",
      "Logged Successfully: \n",
      "2018-05-22 10:50:58epoch=129; Loss Pred=2.4817; Val Loss=2.7952; Val Acc=0.1369; Loss Att={'forw': '14.1064'}; Train Acc=0.215; Test Acc=0.1719; Entropy={'forw': '4.4369'}; Entropy_Test=\n",
      "\n",
      "2.7101865\n",
      "20 2.2618122 2.7101865\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:02epoch=130; Loss Pred=2.4967; Val Loss=2.7102; Val Acc=0.2054; Loss Att={'forw': '10.9943'}; Train Acc=0.176; Test Acc=0.1688; Entropy={'forw': '4.3976'}; Entropy_Test=\n",
      "\n",
      "2.8677084\n",
      "21 2.2618122 2.8677084\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:08epoch=131; Loss Pred=2.4632; Val Loss=2.8677; Val Acc=0.1607; Loss Att={'forw': '12.2865'}; Train Acc=0.178; Test Acc=0.1429; Entropy={'forw': '3.7264'}; Entropy_Test=\n",
      "\n",
      "2.4956443\n",
      "22 2.2618122 2.4956443\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:13epoch=132; Loss Pred=2.5115; Val Loss=2.4956; Val Acc=0.2173; Loss Att={'forw': '7.5411'}; Train Acc=0.195; Test Acc=0.1844; Entropy={'forw': '3.7450'}; Entropy_Test=\n",
      "\n",
      "2.5369942\n",
      "23 2.2618122 2.5369942\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:19epoch=133; Loss Pred=2.4181; Val Loss=2.5370; Val Acc=0.2083; Loss Att={'forw': '15.7569'}; Train Acc=0.184; Test Acc=0.1931; Entropy={'forw': '3.7084'}; Entropy_Test=\n",
      "\n",
      "2.491512\n",
      "24 2.2618122 2.491512\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:23epoch=134; Loss Pred=2.4514; Val Loss=2.4915; Val Acc=0.2262; Loss Att={'forw': '8.3138'}; Train Acc=0.201; Test Acc=0.1830; Entropy={'forw': '3.6333'}; Entropy_Test=\n",
      "\n",
      "2.6881137\n",
      "25 2.2618122 2.6881137\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:29epoch=135; Loss Pred=2.2685; Val Loss=2.6881; Val Acc=0.1964; Loss Att={'forw': '12.0068'}; Train Acc=0.231; Test Acc=0.2011; Entropy={'forw': '3.8084'}; Entropy_Test=\n",
      "\n",
      "2.798178\n",
      "26 2.2618122 2.798178\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:34epoch=136; Loss Pred=2.1702; Val Loss=2.7982; Val Acc=0.1607; Loss Att={'forw': '6.5546'}; Train Acc=0.247; Test Acc=0.2114; Entropy={'forw': '3.8284'}; Entropy_Test=\n",
      "\n",
      "2.4730268\n",
      "27 2.2618122 2.4730268\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:40epoch=137; Loss Pred=2.2191; Val Loss=2.4730; Val Acc=0.2708; Loss Att={'forw': '26.0078'}; Train Acc=0.195; Test Acc=0.2009; Entropy={'forw': '4.5489'}; Entropy_Test=\n",
      "\n",
      "2.580562\n",
      "28 2.2618122 2.580562\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:45epoch=138; Loss Pred=2.3386; Val Loss=2.5806; Val Acc=0.1905; Loss Att={'forw': '19.0209'}; Train Acc=0.223; Test Acc=0.2031; Entropy={'forw': '4.5531'}; Entropy_Test=\n",
      "\n",
      "2.8942053\n",
      "29 2.2618122 2.8942053\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:51epoch=139; Loss Pred=2.2815; Val Loss=2.8942; Val Acc=0.1250; Loss Att={'forw': '17.4431'}; Train Acc=0.229; Test Acc=0.1964; Entropy={'forw': '4.2227'}; Entropy_Test=\n",
      "\n",
      "2.523997\n",
      "30 2.2618122 2.523997\n",
      "Logged Successfully: \n",
      "2018-05-22 10:51:55epoch=140; Loss Pred=2.2740; Val Loss=2.5240; Val Acc=0.1964; Loss Att={'forw': '12.7065'}; Train Acc=0.247; Test Acc=0.1830; Entropy={'forw': '4.2273'}; Entropy_Test=\n",
      "\n",
      "2.795895\n",
      "31 2.2618122 2.795895\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:01epoch=141; Loss Pred=2.2201; Val Loss=2.7959; Val Acc=0.1964; Loss Att={'forw': '16.2318'}; Train Acc=0.249; Test Acc=0.2125; Entropy={'forw': '4.0406'}; Entropy_Test=\n",
      "\n",
      "2.1583915\n",
      "32 2.2618122 2.1583915\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:06epoch=142; Loss Pred=2.1926; Val Loss=2.1584; Val Acc=0.3006; Loss Att={'forw': '7.4070'}; Train Acc=0.256; Test Acc=0.2167; Entropy={'forw': '4.0105'}; Entropy_Test=\n",
      "\n",
      "2.7207437\n",
      "0 2.1583915 2.7207437\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:12epoch=143; Loss Pred=2.2510; Val Loss=2.7207; Val Acc=0.0893; Loss Att={'forw': '13.5895'}; Train Acc=0.213; Test Acc=0.1775; Entropy={'forw': '4.2167'}; Entropy_Test=\n",
      "\n",
      "2.508497\n",
      "1 2.1583915 2.508497\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:17epoch=144; Loss Pred=2.2403; Val Loss=2.5085; Val Acc=0.1429; Loss Att={'forw': '8.5301'}; Train Acc=0.262; Test Acc=0.2116; Entropy={'forw': '4.2201'}; Entropy_Test=\n",
      "\n",
      "2.3350904\n",
      "2 2.1583915 2.3350904\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:23epoch=145; Loss Pred=2.3239; Val Loss=2.3351; Val Acc=0.3065; Loss Att={'forw': '27.7036'}; Train Acc=0.198; Test Acc=0.1864; Entropy={'forw': '4.0806'}; Entropy_Test=\n",
      "\n",
      "2.807633\n",
      "3 2.1583915 2.807633\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:27epoch=146; Loss Pred=2.2150; Val Loss=2.8076; Val Acc=0.1815; Loss Att={'forw': '6.0234'}; Train Acc=0.227; Test Acc=0.1819; Entropy={'forw': '4.1242'}; Entropy_Test=\n",
      "\n",
      "2.7195945\n",
      "4 2.1583915 2.7195945\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:33epoch=147; Loss Pred=2.1791; Val Loss=2.7196; Val Acc=0.1786; Loss Att={'forw': '9.8129'}; Train Acc=0.199; Test Acc=0.1790; Entropy={'forw': '3.7165'}; Entropy_Test=\n",
      "\n",
      "2.6414\n",
      "5 2.1583915 2.6414\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:38epoch=148; Loss Pred=2.2741; Val Loss=2.6414; Val Acc=0.2113; Loss Att={'forw': '7.5853'}; Train Acc=0.209; Test Acc=0.1920; Entropy={'forw': '3.7218'}; Entropy_Test=\n",
      "\n",
      "2.7299702\n",
      "6 2.1583915 2.7299702\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:44epoch=149; Loss Pred=2.3383; Val Loss=2.7300; Val Acc=0.2113; Loss Att={'forw': '11.0409'}; Train Acc=0.227; Test Acc=0.1788; Entropy={'forw': '3.2717'}; Entropy_Test=\n",
      "\n",
      "2.7707543\n",
      "7 2.1583915 2.7707543\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:49epoch=150; Loss Pred=2.4601; Val Loss=2.7708; Val Acc=0.1905; Loss Att={'forw': '8.4057'}; Train Acc=0.197; Test Acc=0.2022; Entropy={'forw': '3.2665'}; Entropy_Test=\n",
      "\n",
      "2.705357\n",
      "8 2.1583915 2.705357\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:55epoch=151; Loss Pred=2.3088; Val Loss=2.7054; Val Acc=0.1518; Loss Att={'forw': '9.6475'}; Train Acc=0.242; Test Acc=0.2100; Entropy={'forw': '4.0508'}; Entropy_Test=\n",
      "\n",
      "2.6435463\n",
      "9 2.1583915 2.6435463\n",
      "Logged Successfully: \n",
      "2018-05-22 10:52:59epoch=152; Loss Pred=2.2615; Val Loss=2.6435; Val Acc=0.1339; Loss Att={'forw': '8.6368'}; Train Acc=0.237; Test Acc=0.2179; Entropy={'forw': '4.0424'}; Entropy_Test=\n",
      "\n",
      "2.4824939\n",
      "10 2.1583915 2.4824939\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:05epoch=153; Loss Pred=2.1791; Val Loss=2.4825; Val Acc=0.2262; Loss Att={'forw': '9.7583'}; Train Acc=0.257; Test Acc=0.2011; Entropy={'forw': '3.7336'}; Entropy_Test=\n",
      "\n",
      "2.68512\n",
      "11 2.1583915 2.68512\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:10epoch=154; Loss Pred=2.1545; Val Loss=2.6851; Val Acc=0.1696; Loss Att={'forw': '6.7074'}; Train Acc=0.278; Test Acc=0.2038; Entropy={'forw': '3.7305'}; Entropy_Test=\n",
      "\n",
      "2.5763066\n",
      "12 2.1583915 2.5763066\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:16epoch=155; Loss Pred=2.1384; Val Loss=2.5763; Val Acc=0.2619; Loss Att={'forw': '8.5814'}; Train Acc=0.248; Test Acc=0.2201; Entropy={'forw': '4.0461'}; Entropy_Test=\n",
      "\n",
      "2.3984263\n",
      "13 2.1583915 2.3984263\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:20epoch=156; Loss Pred=2.0989; Val Loss=2.3984; Val Acc=0.2976; Loss Att={'forw': '7.1515'}; Train Acc=0.275; Test Acc=0.1991; Entropy={'forw': '4.0310'}; Entropy_Test=\n",
      "\n",
      "2.972841\n",
      "14 2.1583915 2.972841\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:26epoch=157; Loss Pred=2.2941; Val Loss=2.9728; Val Acc=0.1786; Loss Att={'forw': '11.9664'}; Train Acc=0.232; Test Acc=0.1922; Entropy={'forw': '3.8678'}; Entropy_Test=\n",
      "\n",
      "2.3689086\n",
      "15 2.1583915 2.3689086\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:31epoch=158; Loss Pred=2.1956; Val Loss=2.3689; Val Acc=0.2619; Loss Att={'forw': '7.5938'}; Train Acc=0.252; Test Acc=0.1866; Entropy={'forw': '3.8807'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.560572\n",
      "16 2.1583915 2.560572\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:37epoch=159; Loss Pred=2.0635; Val Loss=2.5606; Val Acc=0.2232; Loss Att={'forw': '13.8246'}; Train Acc=0.292; Test Acc=0.2335; Entropy={'forw': '4.1107'}; Entropy_Test=\n",
      "\n",
      "2.1360111\n",
      "17 2.1583915 2.1360111\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:42epoch=160; Loss Pred=2.0217; Val Loss=2.1360; Val Acc=0.3065; Loss Att={'forw': '7.6772'}; Train Acc=0.273; Test Acc=0.2462; Entropy={'forw': '4.1056'}; Entropy_Test=\n",
      "\n",
      "2.6254294\n",
      "0 2.1360111 2.6254294\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:48epoch=161; Loss Pred=2.2043; Val Loss=2.6254; Val Acc=0.1429; Loss Att={'forw': '12.3134'}; Train Acc=0.266; Test Acc=0.2103; Entropy={'forw': '4.4319'}; Entropy_Test=\n",
      "\n",
      "2.5786893\n",
      "1 2.1360111 2.5786893\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:52epoch=162; Loss Pred=2.1452; Val Loss=2.5787; Val Acc=0.1429; Loss Att={'forw': '7.6997'}; Train Acc=0.250; Test Acc=0.2221; Entropy={'forw': '4.4344'}; Entropy_Test=\n",
      "\n",
      "2.72464\n",
      "2 2.1360111 2.72464\n",
      "Logged Successfully: \n",
      "2018-05-22 10:53:58epoch=163; Loss Pred=2.2696; Val Loss=2.7246; Val Acc=0.1429; Loss Att={'forw': '13.1082'}; Train Acc=0.238; Test Acc=0.2042; Entropy={'forw': '4.2584'}; Entropy_Test=\n",
      "\n",
      "2.5995498\n",
      "3 2.1360111 2.5995498\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:03epoch=164; Loss Pred=2.2810; Val Loss=2.5995; Val Acc=0.2530; Loss Att={'forw': '7.1126'}; Train Acc=0.239; Test Acc=0.1879; Entropy={'forw': '4.2742'}; Entropy_Test=\n",
      "\n",
      "2.5003765\n",
      "4 2.1360111 2.5003765\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:09epoch=165; Loss Pred=2.1059; Val Loss=2.5004; Val Acc=0.2262; Loss Att={'forw': '13.1788'}; Train Acc=0.248; Test Acc=0.2183; Entropy={'forw': '3.9462'}; Entropy_Test=\n",
      "\n",
      "2.5389826\n",
      "5 2.1360111 2.5389826\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:13epoch=166; Loss Pred=2.1015; Val Loss=2.5390; Val Acc=0.2708; Loss Att={'forw': '8.9845'}; Train Acc=0.247; Test Acc=0.2103; Entropy={'forw': '3.9289'}; Entropy_Test=\n",
      "\n",
      "2.4867246\n",
      "6 2.1360111 2.4867246\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:19epoch=167; Loss Pred=2.2069; Val Loss=2.4867; Val Acc=0.2173; Loss Att={'forw': '23.2624'}; Train Acc=0.282; Test Acc=0.1958; Entropy={'forw': '3.7885'}; Entropy_Test=\n",
      "\n",
      "2.6687915\n",
      "7 2.1360111 2.6687915\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:24epoch=168; Loss Pred=2.1825; Val Loss=2.6688; Val Acc=0.2232; Loss Att={'forw': '12.3276'}; Train Acc=0.246; Test Acc=0.2147; Entropy={'forw': '3.8131'}; Entropy_Test=\n",
      "\n",
      "2.8055403\n",
      "8 2.1360111 2.8055403\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:30epoch=169; Loss Pred=2.1280; Val Loss=2.8055; Val Acc=0.1518; Loss Att={'forw': '9.0936'}; Train Acc=0.268; Test Acc=0.2154; Entropy={'forw': '3.9935'}; Entropy_Test=\n",
      "\n",
      "2.6387432\n",
      "9 2.1360111 2.6387432\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:35epoch=170; Loss Pred=2.1070; Val Loss=2.6387; Val Acc=0.1994; Loss Att={'forw': '9.2638'}; Train Acc=0.301; Test Acc=0.2290; Entropy={'forw': '3.9943'}; Entropy_Test=\n",
      "\n",
      "2.7258499\n",
      "10 2.1360111 2.7258499\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:41epoch=171; Loss Pred=2.4190; Val Loss=2.7258; Val Acc=0.1964; Loss Att={'forw': '17.7464'}; Train Acc=0.228; Test Acc=0.1808; Entropy={'forw': '3.5717'}; Entropy_Test=\n",
      "\n",
      "2.7501347\n",
      "11 2.1360111 2.7501347\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:45epoch=172; Loss Pred=2.3347; Val Loss=2.7501; Val Acc=0.1875; Loss Att={'forw': '11.1647'}; Train Acc=0.249; Test Acc=0.1732; Entropy={'forw': '3.5908'}; Entropy_Test=\n",
      "\n",
      "2.2887158\n",
      "12 2.1360111 2.2887158\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:52epoch=173; Loss Pred=2.0091; Val Loss=2.2887; Val Acc=0.2262; Loss Att={'forw': '15.5575'}; Train Acc=0.283; Test Acc=0.2281; Entropy={'forw': '3.5317'}; Entropy_Test=\n",
      "\n",
      "2.2492518\n",
      "13 2.1360111 2.2492518\n",
      "Logged Successfully: \n",
      "2018-05-22 10:54:56epoch=174; Loss Pred=1.9989; Val Loss=2.2493; Val Acc=0.2887; Loss Att={'forw': '10.6007'}; Train Acc=0.275; Test Acc=0.2112; Entropy={'forw': '3.5079'}; Entropy_Test=\n",
      "\n",
      "2.452893\n",
      "14 2.1360111 2.452893\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:02epoch=175; Loss Pred=2.1830; Val Loss=2.4529; Val Acc=0.2262; Loss Att={'forw': '23.5902'}; Train Acc=0.265; Test Acc=0.2248; Entropy={'forw': '4.3028'}; Entropy_Test=\n",
      "\n",
      "2.6005511\n",
      "15 2.1360111 2.6005511\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:07epoch=176; Loss Pred=2.2168; Val Loss=2.6006; Val Acc=0.2232; Loss Att={'forw': '18.6870'}; Train Acc=0.236; Test Acc=0.2279; Entropy={'forw': '4.3440'}; Entropy_Test=\n",
      "\n",
      "2.435774\n",
      "16 2.1360111 2.435774\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:13epoch=177; Loss Pred=2.0902; Val Loss=2.4358; Val Acc=0.2143; Loss Att={'forw': '10.9929'}; Train Acc=0.273; Test Acc=0.2402; Entropy={'forw': '3.9262'}; Entropy_Test=\n",
      "\n",
      "2.4287775\n",
      "17 2.1360111 2.4287775\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:18epoch=178; Loss Pred=2.1148; Val Loss=2.4288; Val Acc=0.2143; Loss Att={'forw': '7.0289'}; Train Acc=0.282; Test Acc=0.2223; Entropy={'forw': '3.8867'}; Entropy_Test=\n",
      "\n",
      "2.6471639\n",
      "18 2.1360111 2.6471639\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:24epoch=179; Loss Pred=2.0943; Val Loss=2.6472; Val Acc=0.2619; Loss Att={'forw': '15.9479'}; Train Acc=0.292; Test Acc=0.2252; Entropy={'forw': '3.7723'}; Entropy_Test=\n",
      "\n",
      "2.4850652\n",
      "19 2.1360111 2.4850652\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:29epoch=180; Loss Pred=2.0792; Val Loss=2.4851; Val Acc=0.2262; Loss Att={'forw': '8.8048'}; Train Acc=0.290; Test Acc=0.1980; Entropy={'forw': '3.8283'}; Entropy_Test=\n",
      "\n",
      "2.7185528\n",
      "20 2.1360111 2.7185528\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:35epoch=181; Loss Pred=2.3027; Val Loss=2.7186; Val Acc=0.1875; Loss Att={'forw': '17.5342'}; Train Acc=0.285; Test Acc=0.2212; Entropy={'forw': '3.8196'}; Entropy_Test=\n",
      "\n",
      "2.880194\n",
      "21 2.1360111 2.880194\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:40epoch=182; Loss Pred=2.2759; Val Loss=2.8802; Val Acc=0.1964; Loss Att={'forw': '13.3248'}; Train Acc=0.246; Test Acc=0.2388; Entropy={'forw': '3.7913'}; Entropy_Test=\n",
      "\n",
      "2.625115\n",
      "22 2.1360111 2.625115\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:46epoch=183; Loss Pred=2.0698; Val Loss=2.6251; Val Acc=0.2054; Loss Att={'forw': '14.2865'}; Train Acc=0.307; Test Acc=0.2397; Entropy={'forw': '4.1785'}; Entropy_Test=\n",
      "\n",
      "2.4607298\n",
      "23 2.1360111 2.4607298\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:50epoch=184; Loss Pred=2.1020; Val Loss=2.4607; Val Acc=0.2708; Loss Att={'forw': '7.3444'}; Train Acc=0.331; Test Acc=0.2513; Entropy={'forw': '4.1217'}; Entropy_Test=\n",
      "\n",
      "2.3368452\n",
      "24 2.1360111 2.3368452\n",
      "Logged Successfully: \n",
      "2018-05-22 10:55:57epoch=185; Loss Pred=2.0287; Val Loss=2.3368; Val Acc=0.2708; Loss Att={'forw': '15.0961'}; Train Acc=0.288; Test Acc=0.2310; Entropy={'forw': '3.7689'}; Entropy_Test=\n",
      "\n",
      "2.3804576\n",
      "25 2.1360111 2.3804576\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:01epoch=186; Loss Pred=2.0425; Val Loss=2.3805; Val Acc=0.2411; Loss Att={'forw': '9.8740'}; Train Acc=0.307; Test Acc=0.2337; Entropy={'forw': '3.7895'}; Entropy_Test=\n",
      "\n",
      "2.4765098\n",
      "26 2.1360111 2.4765098\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:07epoch=187; Loss Pred=2.1382; Val Loss=2.4765; Val Acc=0.2619; Loss Att={'forw': '23.2613'}; Train Acc=0.284; Test Acc=0.2313; Entropy={'forw': '4.1004'}; Entropy_Test=\n",
      "\n",
      "2.515567\n",
      "27 2.1360111 2.515567\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:12epoch=188; Loss Pred=2.1699; Val Loss=2.5156; Val Acc=0.3185; Loss Att={'forw': '15.2767'}; Train Acc=0.258; Test Acc=0.2304; Entropy={'forw': '4.1010'}; Entropy_Test=\n",
      "\n",
      "2.4487784\n",
      "28 2.1360111 2.4487784\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:19epoch=189; Loss Pred=2.1816; Val Loss=2.4488; Val Acc=0.2530; Loss Att={'forw': '11.3185'}; Train Acc=0.264; Test Acc=0.2109; Entropy={'forw': '4.0222'}; Entropy_Test=\n",
      "\n",
      "2.9264805\n",
      "29 2.1360111 2.9264805\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:23epoch=190; Loss Pred=2.2276; Val Loss=2.9265; Val Acc=0.2530; Loss Att={'forw': '9.6627'}; Train Acc=0.264; Test Acc=0.2087; Entropy={'forw': '4.0483'}; Entropy_Test=\n",
      "\n",
      "2.7732036\n",
      "30 2.1360111 2.7732036\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:30epoch=191; Loss Pred=2.1076; Val Loss=2.7732; Val Acc=0.1607; Loss Att={'forw': '12.9958'}; Train Acc=0.260; Test Acc=0.2395; Entropy={'forw': '3.8727'}; Entropy_Test=\n",
      "\n",
      "2.7044451\n",
      "31 2.1360111 2.7044451\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:34epoch=192; Loss Pred=2.0355; Val Loss=2.7044; Val Acc=0.2143; Loss Att={'forw': '9.7834'}; Train Acc=0.277; Test Acc=0.2083; Entropy={'forw': '3.8856'}; Entropy_Test=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4132192\n",
      "32 2.1360111 2.4132192\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:41epoch=193; Loss Pred=2.3549; Val Loss=2.4132; Val Acc=0.1964; Loss Att={'forw': '14.0309'}; Train Acc=0.237; Test Acc=0.2002; Entropy={'forw': '4.1163'}; Entropy_Test=\n",
      "\n",
      "2.7816603\n",
      "33 2.1360111 2.7816603\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:45epoch=194; Loss Pred=2.4857; Val Loss=2.7817; Val Acc=0.2262; Loss Att={'forw': '10.1661'}; Train Acc=0.240; Test Acc=0.1893; Entropy={'forw': '4.0899'}; Entropy_Test=\n",
      "\n",
      "2.334757\n",
      "34 2.1360111 2.334757\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:52epoch=195; Loss Pred=1.9748; Val Loss=2.3348; Val Acc=0.3065; Loss Att={'forw': '15.2874'}; Train Acc=0.283; Test Acc=0.2513; Entropy={'forw': '3.8213'}; Entropy_Test=\n",
      "\n",
      "2.3434367\n",
      "35 2.1360111 2.3434367\n",
      "Logged Successfully: \n",
      "2018-05-22 10:56:56epoch=196; Loss Pred=1.9113; Val Loss=2.3434; Val Acc=0.2798; Loss Att={'forw': '12.2761'}; Train Acc=0.346; Test Acc=0.2301; Entropy={'forw': '3.8331'}; Entropy_Test=\n",
      "\n",
      "2.2026205\n",
      "36 2.1360111 2.2026205\n",
      "Logged Successfully: \n",
      "2018-05-22 10:57:03epoch=197; Loss Pred=2.0320; Val Loss=2.2026; Val Acc=0.2411; Loss Att={'forw': '12.7708'}; Train Acc=0.327; Test Acc=0.2366; Entropy={'forw': '4.1617'}; Entropy_Test=\n",
      "\n",
      "2.1591556\n",
      "37 2.1360111 2.1591556\n",
      "Logged Successfully: \n",
      "2018-05-22 10:57:07epoch=198; Loss Pred=2.0462; Val Loss=2.1592; Val Acc=0.2976; Loss Att={'forw': '11.5930'}; Train Acc=0.288; Test Acc=0.2469; Entropy={'forw': '4.1735'}; Entropy_Test=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "# This version of the code trains the attractor connections with a separate\n",
    "# objective function than the objective function used to train all other weights\n",
    "# in the network (on the prediction task).\n",
    "\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload\n",
    "\n",
    "from tensorflow_helpers import *\n",
    "from data_generator import generate_examples, pick_task\n",
    "\n",
    "from helper_functions import get_batches, load_pretrained_embeddings, \\\n",
    "    get_model_type_str, translate_ids_to_words, \\\n",
    "    save_results, print_into_log, print_some_translated_sentences, \\\n",
    "    get_training_progress_comment\n",
    "from graph_init import GRU_attractor, TANH_attractor\n",
    "from information_trackers import compute_entropy_fullvec\n",
    "\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, patience_max, disp_epoch, min_delta = 0.00):\n",
    "        self.best = 1e10\n",
    "        self.patience = 0  # our patience\n",
    "        self.patience_max = patience_max\n",
    "        self.display_epoch = disp_epoch\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def update(self, current):\n",
    "        if self.best > current:\n",
    "            self.best = current\n",
    "            self.patience = 0\n",
    "        elif abs(self.best - current) > self.min_delta:\n",
    "            self.patience += 1\n",
    "\n",
    "    def patience_ran_out(self):\n",
    "        if self.patience*self.display_epoch > self.patience_max:\n",
    "            return True\n",
    "        else:\n",
    "            False\n",
    "            \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "ops = {\n",
    "    'model_type': \"TANH\",  # OPTIONS: vanilla, LSTM_raw, LSTM_tensorflow, LSTM_attractor\n",
    "    'hid': 100,\n",
    "    'in': None,  # TBD\n",
    "    'out': 1,\n",
    "    #         'batch_size':n_examples, #since the sequences are 1-dimensional it's easier to just run them all at once\n",
    "    'n_attractor_iterations': 15,\n",
    "    'attractor_dynamics': \"projection2\",  # OPTIONS:  \"\" (for no attractor dynamics),\n",
    "    #           \"direct\" (simple attractor weights applied to hidden states directly, trained with noise addition)\n",
    "    #           \"projection\" (project the hidden state into a separate space via weights, do attraction, project back)\n",
    "    #           \"helper_hidden\" (hidden-hidden neurons) - IMPORTANT: don't forget to add h_hid number\n",
    "    'h_hid': 200,  # helper hidden for \"helper hidden\" \"attractory_dynamics\" mode\n",
    "    'attractor_noise_level': 0.5,\n",
    "    'attractor_noise_type': \"bernoilli\",  # OPTIONS: \"gaussian\", \"dropout\", \"random_drop\"\n",
    "\n",
    "    'training_mode': \"attractor_on_both\",  # 'attractor_on_both',\n",
    "\n",
    "    'attractor_regularization': \"l2_regularization\",  # OPTIONS: \"l2_regularization\", \"l2_norm\"\n",
    "    'attractor_regularization_lambda': 0.0,\n",
    "\n",
    "    'record_mutual_information': True,\n",
    "    'problem_type': \"video_classification\",  # OPTIONS: parity, parity_length, majority, reber, kazakov, pos_brown, ner_german, sentiment_imdb, topic_classification, video_classification\n",
    "    'masking': False,#\"seq\", \"final\"\n",
    "    'prediction_type': 'final_class', #'seq', 'final', 'final_class'\n",
    "    'seq_len': None,\n",
    "\n",
    "    'save_best_model': True,\n",
    "    'reshuffle_data_each_replication': False,  # relevant for POS datasets (since they are loaded from files)\n",
    "    'test_partition': 0.3,\n",
    "    'lrate': 1e-5,  # was 0.008\n",
    "\n",
    "    # NLP related (pos_brown task)\n",
    "    'bidirectional': False,\n",
    "    'embedding_size': 100,\n",
    "    'load_word_embeddings': False,\n",
    "    'train_word_embeddings': False,\n",
    "    'trainable_logic_symbols': 0, #make first *N* embeddings trainable(Pad, unknown, start symbols make it a separate matrix and trainable)\n",
    "    'input_type': \"embed\",  # embed&prior, embed, prior\n",
    "    'dropout': 0.4  # in range(0,1)\n",
    "}\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# SEQ_LEN = 12 # number of bits in input sequence\n",
    "INPUT_NOISE_LEVEL = 0.1\n",
    "\n",
    "\n",
    "TRAIN_ATTR_WEIGHTS_ON_PREDICTION = False\n",
    "# True: train attractor weights on attractor net _and_ prediction\n",
    "REPORT_BEST_TRAIN_PERFORMANCE = True\n",
    "# True: save the train/test perf on the epoch for which train perf was best\n",
    "LOSS_SWITCH_FREQ = 1\n",
    "ops, SEQ_LEN, N_INPUT, N_CLASSES, N_TRAIN, N_TEST = pick_task(ops['problem_type'],\n",
    "                                                              ops)  # task (parity, majority, reber, kazakov)\n",
    "\n",
    "# Training Parameters\n",
    "\n",
    "TRAINING_EPOCHS = 500\n",
    "N_REPLICATIONS = 10\n",
    "BATCH_SIZE = 16\n",
    "DISPLAY_EPOCH = 1\n",
    "EARLY_STOPPING_THRESH = 0.0 # 1e-3 for POS, 0.03 for Sentiment\n",
    "EARLY_STOPPING_PATIENCE = 50  # in epochs\n",
    "EARLY_STOPPING_MINIMUM_EPOCH = 0\n",
    "\n",
    "# NOTEBOOK CODE\n",
    "\n",
    "######### MAIN CODE #############################################################\n",
    "#0.02, 0.05, 0.1, 0.2, 0.35, 0.5, \n",
    "for dataset_part in [0.2244165, 0.448833, 0.67324955, 0.897666]:\n",
    "    for attractor_steps in [15, 0]:\n",
    "#     for att_reg in [0.0]:\n",
    "#         ops['attractor_regularization_lambda'] = att_reg\n",
    "        # the tf seed needs to be within the context of the graph.\n",
    "        # TODO: legacy dependency (to remove)\n",
    "        N_HIDDEN = ops['hid']  # number of hidden units\n",
    "        N_H_HIDDEN = ops['h_hid']\n",
    "        TASK = ops['problem_type']\n",
    "        ARCH = ops['model_type']  # hidden layer type: 'GRU' or 'tanh'\n",
    "        NOISE_LEVEL = ops['attractor_noise_level']\n",
    "        ATTRACTOR_TYPE = ops['attractor_dynamics']\n",
    "        N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "        # TODO: legacy dependency done\n",
    "    \n",
    "    \n",
    "        tf.reset_default_graph()\n",
    "        np.random.seed(11)\n",
    "        tf.set_random_seed(11)\n",
    "        ops['n_attractor_iterations'] = attractor_steps\n",
    "        N_ATTRACTOR_STEPS = ops['n_attractor_iterations']\n",
    "\n",
    "        #\n",
    "        # PLACEHOLDERS\n",
    "        #\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            # last dimension is left singular, tensorflow will expect it to be an id number, not 1-hot embed\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN], name='Y')\n",
    "        elif ops['problem_type'] == 'sentiment_imdb':\n",
    "             # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            Y = tf.placeholder(\"int64\", [None, N_CLASSES], name='Y')\n",
    "        elif ops['problem_type'] == 'topic_classification':\n",
    "             # X will be looked up in the embedding table, so the last dimension is just a number\n",
    "            X = tf.placeholder(\"int64\", [None, SEQ_LEN], name='X')\n",
    "            Y = tf.placeholder(\"int64\", [None, 1], name='Y')\n",
    "        elif ops['problem_type'] == 'ner_german':\n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, SEQ_LEN])\n",
    "        else:  # single output \n",
    "            X = tf.placeholder(\"float\", [None, SEQ_LEN, N_INPUT])\n",
    "            Y = tf.placeholder(\"int64\", [None, 1])\n",
    "        attractor_tgt_net = tf.placeholder(\"float\", [None, N_HIDDEN], name='attractor_tgt')\n",
    "\n",
    "        # Embedding matrix initialization\n",
    "        if 'pos' in ops['problem_type'] or 'sentiment' in ops['problem_type'] or ops['problem_type'] == \"topic_classification\":\n",
    "            [_, _, _, _, _, _, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                         INPUT_NOISE_LEVEL, TASK, ops)\n",
    "\n",
    "            if ops['load_word_embeddings']:\n",
    "                embeddings_loaded, _ = load_pretrained_embeddings('data/glove.6B.{}d.txt'.format(ops['embedding_size']),\n",
    "                                                               maps, ops)\n",
    "                if ops['trainable_logic_symbols'] > 0:\n",
    "                    with tf.variable_scope(\"TASK_WEIGHTS\"):\n",
    "                        symbols_embedding = tf.get_variable(\"symb_embedding\",\n",
    "                                                initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                                shape=[ops['trainable_logic_symbols'], ops['embedding_size']],\n",
    "                                                dtype=tf.float32,\n",
    "                                                trainable=True)\n",
    "                    \n",
    "                word_embedding = tf.get_variable(\"embedding\",\n",
    "                                            initializer=embeddings_loaded,\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=ops['train_word_embeddings'])\n",
    "                if ops['trainable_logic_symbols'] > 0:\n",
    "                    embedding = tf.concat([symbols_embedding, word_embedding], axis=0)\n",
    "                else:\n",
    "                    embedding = word_embedding\n",
    "            else:  # initialize randomly\n",
    "                embedding = tf.get_variable(\"embedding\",\n",
    "                                            initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                            shape=[ops['vocab_size'], ops['embedding_size']],\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=ops['train_word_embeddings'])\n",
    "            embed_lookup = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "            # load priors information\n",
    "            if ops['input_type'] == 'prior' or ops['input_type'] == 'embed&prior':\n",
    "                id2prior = maps['id2prior']\n",
    "                word2id = maps['word2id']\n",
    "                priors = np.zeros([len(id2prior), len(id2prior[0])]).astype(\"float32\")\n",
    "                for id, prior in id2prior.items():\n",
    "                    priors[id] = prior\n",
    "                priors_op = tf.get_variable(\"priors\",\n",
    "                                            initializer=priors,\n",
    "                                            dtype=tf.float32,\n",
    "                                            trainable=False)\n",
    "                prior_lookup = tf.nn.embedding_lookup(priors_op, X)\n",
    "\n",
    "            if ops['input_type'] == 'embed':\n",
    "                embed = embed_lookup\n",
    "            elif ops['input_type'] == 'prior':\n",
    "                embed = prior_lookup\n",
    "            elif ops['input_type'] == 'embed&prior':\n",
    "                embed = tf.concat([embed_lookup, prior_lookup], axis=2)\n",
    "\n",
    "        # Graph + all the training variables\n",
    "        if 'pos' in ops['problem_type']:\n",
    "            net_inputs = {'X': embed, 'mask': Y, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        elif ops['problem_type'] == 'sentiment_imdb' or ops['problem_type'] == 'topic_classification':\n",
    "            net_inputs = {'X': embed, 'mask': X, 'attractor_tgt_net': attractor_tgt_net}\n",
    "        else:\n",
    "            net_inputs = {'X': X, 'mask': X, 'attractor_tgt_net': attractor_tgt_net}\n",
    "\n",
    "        if ops['model_type'] == \"TANH\":\n",
    "            cell = TANH_attractor\n",
    "        elif ops['model_type'] == \"GRU\":\n",
    "            cell = GRU_attractor\n",
    "        if ops['bidirectional']:\n",
    "            G_attractors = {'forw': [], 'back': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = cell(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            # Backward:\n",
    "            G_back = cell(ops, inputs=net_inputs, direction='backward', suffix=names[1])\n",
    "            attr_loss_op_back = G_back.attr_loss_op\n",
    "            attr_train_op_back = G_back.attr_train_op\n",
    "            h_clean_seq_flat_back = G_back.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_back = G_back.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['back'] = {'attr_loss_op': attr_loss_op_back, \"attr_train_op\": attr_train_op_back,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_back, 'h_net_seq_flat': h_net_seq_flat_back}\n",
    "            G_back_output = G_back.output\n",
    "\n",
    "            \n",
    "            \n",
    "            # Merge: [seq_len, batch_size, n_hid*2]\n",
    "            # Note that we reverse the backward cell's output to align with original direction\n",
    "            # note in \"final\" only prediction, one less dimension\n",
    "            if 'final' in ops['prediction_type']:\n",
    "                merge_index = 1\n",
    "            else:\n",
    "                merge_index = 2\n",
    "            output = tf.concat([G_forw_output, tf.reverse(G_back_output, axis=[0])], axis=merge_index)\n",
    "    \n",
    "            if ops['dropout'] > 0.0:\n",
    "                # note keep_prob = 1.0 - drop_probability (not sure why they implemented it this way)\n",
    "                # tensorflow implementation scales by 1/keep_prob automatically\n",
    "                output = tf.nn.dropout(output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output = output\n",
    "\n",
    "            input_size_final_projection = 2 * ops['hid']\n",
    "            Y_ =  project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "            \n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "        else:\n",
    "            G_attractors = {'forw': []}\n",
    "            names = G_attractors.keys()\n",
    "            # Forward:\n",
    "            G_forw = cell(ops, inputs=net_inputs, direction='forward', suffix=names[0])\n",
    "            attr_loss_op_forw = G_forw.attr_loss_op\n",
    "            attr_train_op_forw = G_forw.attr_train_op\n",
    "            h_clean_seq_flat_forw = G_forw.h_clean_seq_flat  # for computing entropy of states\n",
    "            h_net_seq_flat_forw = G_forw.h_net_seq_flat  # -> attractor_tgt_net placeholder\n",
    "            G_attractors['forw'] = {'attr_loss_op': attr_loss_op_forw, \"attr_train_op\": attr_train_op_forw,\n",
    "                                    'h_clean_seq_flat': h_clean_seq_flat_forw, 'h_net_seq_flat': h_net_seq_flat_forw}\n",
    "            G_forw_output = G_forw.output\n",
    "\n",
    "            input_size_final_projection = ops['hid']\n",
    "            \n",
    "            if ops['dropout'] > 0.0:\n",
    "                output = tf.nn.dropout(G_forw_output, keep_prob=1.0 - ops['dropout'])\n",
    "            else:\n",
    "                output = G_forw_output\n",
    "            \n",
    "            Y_ = project_into_output(Y, output, input_size_final_projection, ops['out'], ops)\n",
    "\n",
    "            # LOSS, ACC, & TRAIN OPS\n",
    "            pred_loss_op = task_loss(Y, Y_, ops)\n",
    "            optimizer_pred = tf.train.AdamOptimizer(learning_rate=0.008)\n",
    "            prediction_parameters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TASK_WEIGHTS\")\n",
    "            pred_train_op = optimizer_pred.minimize(pred_loss_op, var_list=prediction_parameters)\n",
    "            accuracy = task_accuracy(Y, Y_, ops)\n",
    "\n",
    "            \n",
    "        mask_op = tf.cast(tf.sign(Y), dtype=tf.float32)\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            # TODO: make a class for all \"best\" quantities (a lot of space)\n",
    "            saved_train_acc = []\n",
    "            saved_test_acc = []\n",
    "            saved_epoch = []\n",
    "            saved_att_loss = []\n",
    "            saved_entropy_final = []\n",
    "            saved_val_acc = []\n",
    "            saved_val_loss = []\n",
    "            saved_traini_loss = []\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # Start training\n",
    "            for replication in range(N_REPLICATIONS):\n",
    "                print(\"********** replication \", replication, \" **********\")\n",
    "                early_stopper = EarlyStopper(EARLY_STOPPING_PATIENCE, DISPLAY_EPOCH)\n",
    "                [X_full_train, Y_full_train, X_test, Y_test, X_val, Y_val, maps] = generate_examples(SEQ_LEN, N_TRAIN, N_TEST,\n",
    "                                                                                           INPUT_NOISE_LEVEL, TASK, ops)\n",
    "                # Take Only part of dataset:\n",
    "                all_ids = range(len(X_full_train))\n",
    "                np.random.shuffle(all_ids)\n",
    "                train_part = int(dataset_part * len(X_full_train))\n",
    "                ids_to_take = all_ids[0:train_part]\n",
    "                ids_for_val = all_ids[train_part:int(train_part + 0.2*train_part)]\n",
    "                if len(ids_to_take) > X_full_train.shape[0]:\n",
    "                    ids_to_take = range(X_full_train.shape[0])\n",
    "                X_train = X_full_train[ids_to_take, :]\n",
    "                Y_train = Y_full_train[ids_to_take, :]\n",
    "                \n",
    "                if BATCH_SIZE < len(X_train):\n",
    "                    ops['attractor_regularization_lambda'] = ops['attractor_regularization_lambda']/(len(X_train)*1.0/BATCH_SIZE)\n",
    "                    print(ops['attractor_regularization_lambda'])\n",
    "                \n",
    "                X_val, Y_val = X_full_train[ids_for_val,:], Y_full_train[ids_for_val,:]\n",
    "                \n",
    "                N_TRAIN = len(X_train)\n",
    "                print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)\n",
    "\n",
    "                # Log Path init-n:\n",
    "                COMMENT = 'dataset_starvation_experiment'\n",
    "                MODEL_NAME_FILE = '{}_(att_iter{}__bidir{}__drop{})_{}.txt'.format(ops['problem_type'],\n",
    "                                                                                   ops['n_attractor_iterations'],\n",
    "                                                                                   ops['bidirectional'],\n",
    "                                                                                   ops['dropout'],\n",
    "                                                                                   COMMENT)\n",
    "                LOG_DIRECTORY = 'experiments/logs/{}'.format(MODEL_NAME_FILE)\n",
    "                MODEL_DIRECTORY = 'experiments/logs/{}_{}'.format(datetime.date.today(), MODEL_NAME_FILE)\n",
    "                print_into_log(LOG_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN))\n",
    "                print_into_log(MODEL_DIRECTORY, get_model_type_str(ops, N_TRAIN, N_TEST, SEQ_LEN), supress=True)\n",
    "\n",
    "                sess.run(init)  # Run the initializer\n",
    "\n",
    "                train_prediction_loss = True\n",
    "                best_train_acc = -1000.\n",
    "                best_test_acc = 0\n",
    "                best_entropy = 0.0\n",
    "                best_att_loss = 0\n",
    "                best_train_loss = 0\n",
    "                best_val_loss = 0.0\n",
    "                best_val_acc = 0.0\n",
    "                best_epoch = 0\n",
    "                for epoch in range(1, TRAINING_EPOCHS + 2):\n",
    "                    if (epoch - 1) % DISPLAY_EPOCH == 0:\n",
    "                        # TRAIN set:\n",
    "                        ploss, train_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        # TEST set:\n",
    "                        test_acc = batch_tensor_collect(sess, [accuracy], X, Y, X_test, Y_test, BATCH_SIZE)[0]\n",
    "                        \n",
    "                        # Validation set & Early stopping:\n",
    "                        ploss_val, val_acc = batch_tensor_collect(sess, [pred_loss_op, accuracy],\n",
    "                                                                  X, Y, X_val, Y_val, BATCH_SIZE)\n",
    "                        print(ploss_val)\n",
    "                       \n",
    "                        print(early_stopper.patience, early_stopper.best, ploss_val)\n",
    "                        early_stopper.update(ploss_val)\n",
    "                        if early_stopper.patience_ran_out():\n",
    "                            print_into_log(LOG_DIRECTORY, \"STOPPED EARLY AT {}\".format(epoch))\n",
    "                            break\n",
    "\n",
    "                        # ATTRACTOR(s) LOSS\n",
    "                        aloss = {}\n",
    "                        entropy = {}\n",
    "                        hid_vals_arr = batch_tensor_collect(sess, [A['h_net_seq_flat'] for att_name, A in\n",
    "                                                                   G_attractors.items()],\n",
    "                                                            X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        h_clean_val_arr = batch_tensor_collect(sess, [A['h_clean_seq_flat'] for att_name, A in\n",
    "                                                                      G_attractors.items()],\n",
    "                                                               X, Y, X_train, Y_train, BATCH_SIZE)\n",
    "                        for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                            A = G_attractors[attractor_name]\n",
    "                            a_loss_val = []\n",
    "                            n_splits = np.max([1, int(len(X_train) / BATCH_SIZE)])\n",
    "                            for batch_hid_vals in np.array_split(hid_vals_arr[i], n_splits):\n",
    "                                a_loss_val.append(\n",
    "                                    sess.run(A['attr_loss_op'], feed_dict={attractor_tgt_net: batch_hid_vals}))\n",
    "                            aloss[attractor_name] = \"{:.4f}\".format(np.mean(a_loss_val))\n",
    "\n",
    "                            entropy[attractor_name] = \"{:.4f}\".format(\n",
    "                                compute_entropy_fullvec(h_clean_val_arr[i], ops, n_bins=8))\n",
    "\n",
    "                        # Print training information:\n",
    "                        print_into_log(LOG_DIRECTORY, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + get_training_progress_comment(epoch, ploss, aloss, ploss_val, val_acc, train_acc,\n",
    "                                                                     test_acc, entropy))\n",
    "                        # Update the logs:\n",
    "                       \n",
    "                        #                 if ops['record_mutual_information']:\n",
    "                        # #                     h_attractor_val, h_clean_val = sess.run([h_attractor_collection, h_clean_seq_flat],\n",
    "                        # #                                                                    feed_dict={X: X_train, Y: Y_train})\n",
    "                        #                     # TODO: h_attractor_collection reshapeing masking.\n",
    "                        #                     h_attractor_val = None\n",
    "                        #                     h_clean_val = batch_tensor_collect(sess, [h_clean_seq_flat],\n",
    "                        #                                                                         X, Y, X_train, Y_train, BATCH_SIZE)[0]\n",
    "                        #                     MIS.update(ploss, aloss, train_acc, test_acc, np.tanh(hid_vals), h_attractor_val, h_clean_val)\n",
    "\n",
    "                        if (val_acc > best_val_acc):\n",
    "                            best_train_acc = train_acc\n",
    "                            best_test_acc = test_acc\n",
    "                            best_att_loss = aloss\n",
    "                            best_epoch = epoch\n",
    "                            best_val_acc = val_acc\n",
    "\n",
    "                            best_val_loss = ploss_val\n",
    "                            best_train_loss = ploss\n",
    "                            if ops['save_best_model']:\n",
    "                                save_path = saver.save(sess, MODEL_DIRECTORY)\n",
    "                            best_entropy = entropy\n",
    "                        if (1.0 - 1e-15 < 0.0):\n",
    "                            print('reached_peak')\n",
    "                            break\n",
    "\n",
    "                    if epoch > 1 and LOSS_SWITCH_FREQ > 0 \\\n",
    "                            and (epoch - 1) % LOSS_SWITCH_FREQ == 0:\n",
    "                        train_prediction_loss = not train_prediction_loss\n",
    "\n",
    "                    # MODEL TRAINING\n",
    "                    batches = get_batches(BATCH_SIZE, X_train, Y_train)\n",
    "                    for (batch_x, batch_y) in batches:\n",
    "                        if (LOSS_SWITCH_FREQ == 0 or train_prediction_loss):\n",
    "                            # Optimize all parameters except for attractor weights\n",
    "                            _ = sess.run([pred_train_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y})\n",
    "                        # Attractor:\n",
    "                        if (N_ATTRACTOR_STEPS > 0):\n",
    "                            batch_hid_vals = sess.run([A['h_net_seq_flat'] for att_name, A in G_attractors.items()],\n",
    "                                                      feed_dict={X:batch_x,  Y:batch_y})\n",
    "\n",
    "                            for i, attractor_name in enumerate(G_attractors.keys()):\n",
    "                                A = G_attractors[attractor_name]\n",
    "                                _ = sess.run(A['attr_train_op'], feed_dict={attractor_tgt_net: batch_hid_vals[i]})\n",
    "                print(\"Optimization Finished!\")\n",
    "\n",
    "                if (REPORT_BEST_TRAIN_PERFORMANCE):\n",
    "                    saved_train_acc.append(best_train_acc)\n",
    "                    saved_test_acc.append(best_test_acc)\n",
    "                    saved_att_loss.append(best_att_loss)\n",
    "                    saved_entropy_final.append(best_entropy)\n",
    "                    saved_epoch.append(best_epoch)\n",
    "\n",
    "                    saved_val_acc.append(best_val_acc)\n",
    "                    saved_val_loss.append(best_val_loss)\n",
    "                    saved_traini_loss.append(best_train_loss)\n",
    "                else:\n",
    "                    saved_train_acc.append(train_acc)\n",
    "                    saved_test_acc.append(test_acc)\n",
    "                    #             saved_att_loss.append(aloss)\n",
    "\n",
    "            save_results(ops, saved_epoch, saved_train_acc, saved_test_acc, saved_att_loss, saved_entropy_final, saved_val_acc,\n",
    "                 saved_val_loss, saved_traini_loss, N_TRAIN, N_TEST, SEQ_LEN, comment=COMMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_cut = int(len(Y)*0.30)\n",
    "# train_cut = int(len(Y) - (test_cut))\n",
    "# X_train = X_bow[0:train_cut, :]\n",
    "# Y_train = Y[0:train_cut]\n",
    "\n",
    "# X_test = X[train_cut:, :]\n",
    "# Y_test = Y[train_cut:]\n",
    "    \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# clf = GaussianNB()\n",
    "clf = SVC()\n",
    "# clf.fit(X_train[:,0,:], Y_train[:,0]) \n",
    "clf.fit(np.max(X_train, axis=1), Y_train[:,0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3725"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred = clf.predict(X_test[:,0,:])\n",
    "Y_pred = clf.predict(np.max(X_test, axis=1))\n",
    "np.sum(np.equal(Y_pred, Y_test[:,0]))*1.0/2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 2048)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.99, 0.5, 0.25, 0.75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
